{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import rasterio , os , glob  , geojson\n",
    "from pathlib import PureWindowsPath\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "import rioxarray\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot as rplt\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import shapes\n",
    "from rasterio import plot\n",
    "from rasterstats import zonal_stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from shapely.geometry import LineString, Point\n",
    "import math\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as mtext\n",
    "from collections import defaultdict\n",
    "from matplotlib import colormaps\n",
    "\n",
    "from h3 import h3\n",
    "import h3pandas\n",
    "import contextily as cx\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from IPython.display import Image , display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# plt.rcParams['figure.figsize']=9,4\n",
    "import xarray as xr\n",
    "from shapely.geometry import mapping, Polygon\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from geopandas.tools import overlay\n",
    "from pyproj import CRS\n",
    "from os.path import exists\n",
    "import time\n",
    "from pathlib import Path\n",
    "import fiona\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import shapefile\n",
    "from shapely.geometry import shape, Point\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "# Stats heavy-lifting\n",
    "from libpysal.weights import KNN\n",
    "from sklearn.preprocessing import robust_scale\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import osmnx as ox\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, TransformedBbox, BboxPatch, BboxConnector)\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "width=8\n",
    "height=7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# socio_econmic=False\n",
    "# cckp=False\n",
    "socio_econmic=True\n",
    "cckp=True\n",
    "\n",
    "# base_dir= '/Users/ipshitakarmakar/Dropbox (MIT)/WB/Future city scan' # Project directory\n",
    "base_dir= 'C:/Users/Aziz/Dropbox/CRP/FCS' # Project directory\n",
    "# output= f'{base_dir}/output' # csv and figure output directory\n",
    "# data= f'{base_dir}/data'  # Input data directory\n",
    "# shp = f'{data}/test_cities/Buffered_Chittagong_Final.shp' # City shapefile\n",
    "# shapefiles = f'{data}/test_cities' # City shapefiles output dir\n",
    "country = 'BGD'\n",
    "city= 'Chittagong'\n",
    "#Mapping \n",
    "WGS84=4326\n",
    "EPSG_str= 'EPSG:4326'\n",
    "hexbin_res=8\n",
    "\n",
    "def set_paths(base_dir):\n",
    "    data = os.path.join(base_dir, 'data') \n",
    "    output = os.path.join(base_dir, 'output') \n",
    "    shapefiles = os.path.join(output, 'shapefiles') \n",
    "    maps = os.path.join(output, 'maps') \n",
    "    rasters = os.path.join(output, 'rasters') \n",
    "    tables = os.path.join(output, 'tables') \n",
    "    \n",
    "    dirs_list= [data, output, base_dir, shapefiles , maps , rasters , tables]\n",
    "    for dir in dirs_list:\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "    return data, shapefiles , maps , rasters , output ,tables\n",
    "\n",
    "data, shapefiles , maps , rasters , output ,tables = set_paths(base_dir)\n",
    "\n",
    "# [8:54 PM] Ipshita Karmakar\n",
    "# variablename_ssp_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City and supra and subcity admin shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = f'{shapefiles}/Buffered_Chittagong_Final.shp' # City shapefile\n",
    "country_shp = f'{shapefiles}/bgd_admbnda_adm0_bbs_20201113.shp' # City shapefile\n",
    "lowest_admin_shp = f'{shapefiles}/bgd_admbnda_adm4_bbs_20201113.shp' # City shapefile\n",
    "second_lowest_admin_shp = f'{shapefiles}/bgd_admbnda_adm3_bbs_20201113.shp' # City shapefile\n",
    "vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "country_vector_file = gpd.read_file(country_shp).to_crs(WGS84)\n",
    "lowest_admin_vector_file = gpd.read_file(lowest_admin_shp).to_crs(WGS84)\n",
    "second_lowest_admin_vector_file = gpd.read_file(second_lowest_admin_shp).to_crs(WGS84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_items(source):\n",
    "    print(\"/n\")\n",
    "    for ele in enumerate(source): \n",
    "        print(ele)\n",
    "\n",
    "def list_df_columns(df):\n",
    "    field_list = list(df)\n",
    "    enumerate_items(field_list)\n",
    "    return field_list\n",
    "\n",
    "def percentage_formatter(x, pos):\n",
    "    return f'{x * 100 :,.0f}'\n",
    "\n",
    "def millions_formatter(x, pos):\n",
    "    return f'{x / 1000000 :,.0f}'\n",
    "\n",
    "\n",
    "def hundred_thousand_formatter(x, pos):\n",
    "    return f'{x / 100000 :,.0f}'\n",
    "\n",
    "def billions_formatter(x, pos):\n",
    "    return f'{x / 1000000000 :,.0f}'\n",
    "\n",
    "# For loading shapefiles into geopandas dataframe\n",
    "def load_shapefile(shp):\n",
    "    data = gpd.read_file(shp).to_crs(WGS84)\n",
    "    return data\n",
    "\n",
    "# For loading shapefiles as a geopandas dataframe and buffering it.\n",
    "def load_shapefile_buffer(shp):\n",
    "    data = gpd.read_file(shp).to_crs(WGS84).buffer(0.016) #remember to reproject =2 kilomers\n",
    "    return data\n",
    "\n",
    "# For re-projecting input vector layer to raster projection \n",
    "def reproject_gpdf(input_gpdf, raster):\n",
    "    proj = raster.crs.to_proj4()\n",
    "    reproj = input_gpdf.to_crs(proj)\n",
    "    return reproj\n",
    "\n",
    "# clipping the raster to the buffered AOI and exporting it. \n",
    "def clip_and_export_raster_fathom(input_gpdf, raster , out_raster):\n",
    "    Vector=input_gpdf\n",
    "    with rasterio.open(raster) as src:\n",
    "        Vector=Vector.to_crs(src.crs)\n",
    "        # print(Vector.crs)\n",
    "        out_image, out_transform=mask(src,Vector.geometry,crop=True)\n",
    "        no_data_value=0\n",
    "        out_image = np.where(out_image<0,no_data_value,out_image) #replaced negative values with \n",
    "        # out_image = np.where(((out_image<0) & (out_image>99)),no_data_value,out_image) #replaced negative values with \n",
    "        # out_image = np.where(out_image>99,no_data_value,out_image) #replaced negative values with \n",
    "        out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "        \n",
    "    out_meta.update({\n",
    "        \"driver\":\"Gtiff\",\n",
    "        \"height\":out_image.shape[1], # height starts with shape[1]\n",
    "        \"width\":out_image.shape[2], # width starts with shape[2]\n",
    "        \"transform\":out_transform\n",
    "    })\n",
    "                \n",
    "    with rasterio.open(out_raster,'w',**out_meta) as dst:\n",
    "        dst.write(out_image)\n",
    "\n",
    "\n",
    "# clipping the raster to the buffered AOI and exporting it. \n",
    "def clip_and_export_raster(input_gpdf, raster , out_raster):\n",
    "    Vector=input_gpdf\n",
    "    with rasterio.open(raster) as src:\n",
    "        Vector=Vector.to_crs(src.crs)\n",
    "        # print(Vector.crs)\n",
    "        out_image, out_transform=mask(src,Vector.geometry,crop=True)\n",
    "        no_data_value=0\n",
    "        out_image = np.where(out_image<0,no_data_value,out_image) #replaced negative values with \n",
    "        out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "        \n",
    "    out_meta.update({\n",
    "        \"driver\":\"Gtiff\",\n",
    "        \"height\":out_image.shape[1], # height starts with shape[1]\n",
    "        \"width\":out_image.shape[2], # width starts with shape[2]\n",
    "        \"transform\":out_transform\n",
    "    })\n",
    "                \n",
    "    with rasterio.open(out_raster,'w',**out_meta) as dst:\n",
    "        dst.write(out_image)\n",
    "\n",
    "\n",
    "\n",
    "def reproject_rasters(crs , unprojected_raster, projected_raster):\n",
    "    if not exists(projected_raster):\n",
    "        with rasterio.open(unprojected_raster) as src:\n",
    "            dst_crs = 'EPSG:' + str(crs)\n",
    "            transform, width, height = calculate_default_transform(\n",
    "                src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "            kwargs = src.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': dst_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "            with rasterio.open(projected_raster, 'w', **kwargs) as dst:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=dst_crs,\n",
    "                        resampling=Resampling.nearest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For selecting which raster statistics to calculate\n",
    "def list_statistics(stat):\n",
    "    out_stats = stat\n",
    "    return out_stats\n",
    "\n",
    "# For selecting which raster statistics to calculate\n",
    "def list_statistics_urbanland():\n",
    "    out_stats = ['mean']\n",
    "    return out_stats\n",
    "\n",
    "# For calculating zonal statistics\n",
    "def calculate_zonal_stats(vector, raster, stats):\n",
    "    # Run zonal statistics, store result in geopandas dataframe\n",
    "    result = zonal_stats(vector, raster, stats=stats, geojson_out=True)\n",
    "    geostats = gpd.GeoDataFrame.from_features(result)\n",
    "    for c in stats:\n",
    "        geostats[c] = np.round(geostats[c] , decimals= 2)\n",
    "    return geostats\n",
    "\n",
    "# out_image = np.where(out_image<0,no_data_value,out_image) #replaced negative values with \n",
    "\n",
    "\n",
    "# For generating raster from zonal statistics result.\n",
    "def export_zonal_stats_raster(zdf, raster, stat, out_raster, no_data='y'):\n",
    "    meta = raster.meta.copy()\n",
    "    out_shape = raster.shape\n",
    "    transform = raster.transform\n",
    "    dtype = raster.dtypes[0]\n",
    "    field_list = list_df_columns(stats)\n",
    "    index = 0 #first statistic for now\n",
    "    zone = zdf[field_list[index]]\n",
    "    stats = list_statistics(stat)\n",
    "    shapes = ((geom,value) for geom, value in zip(zdf.geometry, zone))\n",
    "    burned = rasterize(shapes=shapes, fill=0, out_shape=out_shape, transform=transform)\n",
    "    # show(burned)\n",
    "    meta.update(dtype=rasterio.float32, nodata=0)\n",
    "    # Optional to set nodata values to min of stat\n",
    "    if no_data == 'y':\n",
    "        # cutoff = min(zone.values)\n",
    "        cutoff < 0\n",
    "        # print(\"Setting nodata cutoff to: \", cutoff)\n",
    "        burned[burned < cutoff] = 0 \n",
    "    with rasterio.open(out_raster, 'w', **meta) as out:\n",
    "        no_data_value=0\n",
    "        out.write_band(1, burned)\n",
    "    # print(\"Zonal Statistics Raster generated\")\n",
    "\n",
    "def visualize_popdynamics_figure(df_for_heatmap, country, city, subdir):\n",
    "    # Create a mask for the NaN values\n",
    "    mask = df_for_heatmap.isnull()\n",
    "    # Create a heatmap using seaborn\n",
    "    fig, ax = plt.subplots(figsize=(14,5))\n",
    "    stats = list_statistics(stat)\n",
    "    for c in stats:\n",
    "        if c==\"sum\":\n",
    "            df_formatted =df_for_heatmap.applymap( lambda val: f'{val / 100000:,.0f}')\n",
    "            fmt = ''\n",
    "        else:\n",
    "            fmt = '.0f'\n",
    "        sns.heatmap(df_for_heatmap,  annot=df_formatted, cmap='YlGnBu', cbar=False, fmt=fmt,\n",
    "                    square=True, annot_kws={'size': 10},\n",
    "                    linewidths=1, linecolor='gray',\n",
    "                    cbar_kws={'label': 'Value'}, ax=ax)\n",
    "\n",
    "        ax.patch.set_edgecolor('black')  \n",
    "        ax.patch.set_linewidth(1)  \n",
    "        plt.title(f\"Projected Population(100k) \\n {city}\")\n",
    "        plt.savefig(f\"{maps}/{country}_{city}_{subdir}_{c}.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_popdynamics_graph(df_for_graph, country, city, subdir):\n",
    "    df_for_graph['Year'] = pd.to_datetime(\n",
    "                            df_for_graph['Year'],\n",
    "                            format='%Y'\n",
    "                            )\n",
    "    # plot data\n",
    "    g = sns.lineplot(data=df_for_graph, x=\"Year\", y=\"value\", \n",
    "                    hue=\"Scenario\"  , linewidth=2.5                     \n",
    "                     )\n",
    "     \n",
    "    g.set(xlabel='Year', ylabel='Population (100k)')\n",
    "    # g.axes.yaxis.set_major_formatter(ticker.FuncFormatter(lambda y, p: f'{y/1000000:.0f}'))\n",
    "    g.axes.yaxis.set_major_formatter(ticker.FuncFormatter(hundred_thousand_formatter))\n",
    "    # Tell matplotlib to interpret the x-axis values as dates\n",
    "    g.xaxis_date()\n",
    "    g.get_figure().savefig(f\"{maps}/{country}_{city}_{subdir}_graph.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "\n",
    "def visualize_gdp_figure(df_for_heatmap, country, city, subdir):\n",
    "    # Create a mask for the NaN values\n",
    "    mask = df_for_heatmap.isnull()\n",
    "    # Create a heatmap using seaborn\n",
    "    fig, ax = plt.subplots(figsize=(14,5))\n",
    "    stats = list_statistics(stat)\n",
    "    for c in stats:\n",
    "        if c==\"sum\":\n",
    "            df_formatted =df_for_heatmap.applymap( lambda val: f'{val / 1000000000:,.0f}' )\n",
    "            fmt = ''\n",
    "        else:\n",
    "            fmt = '.0f'\n",
    "        sns.heatmap(df_for_heatmap,  annot=df_formatted, cmap='YlGnBu', cbar=False, fmt=fmt,\n",
    "                    square=True, annot_kws={'size': 10},\n",
    "                    linewidths=1, linecolor='gray',\n",
    "                    cbar_kws={'label': 'Value'}, ax=ax)\n",
    "\n",
    "        ax.patch.set_edgecolor('black')  \n",
    "        ax.patch.set_linewidth(1)  \n",
    "        plt.title(f\"Projected GDP of {city}\\n in Billions USD\")\n",
    "        plt.savefig(f\"{maps}/{country}_{city}_{subdir}_heatmap.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_gdp_graph(df_for_graph, country, city, subdir):\n",
    "    df_for_graph['Year'] = pd.to_datetime(\n",
    "                            df_for_graph['Year'],\n",
    "                            format='%Y'\n",
    "                            )\n",
    "    # plot data\n",
    "    g = sns.lineplot(data=df_for_graph, x=\"Year\", y=\"value\", \n",
    "                     hue=\"Scenario\"  , linewidth=2.5\n",
    "                     )\n",
    "    g.set(xlabel='Year', ylabel='GDP (Billions USD)')\n",
    "    g.axes.yaxis.set_major_formatter(ticker.FuncFormatter(billions_formatter))\n",
    "    # Tell matplotlib to interpret the x-axis values as dates\n",
    "    g.xaxis_date()\n",
    "    g.get_figure().savefig(f\"{maps}/{country}_{city}_{subdir}_graph.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "\n",
    "def visualize_urbanland_figure(df_for_heatmap, country, city, subdir):\n",
    "    # Create a mask for the NaN values\n",
    "    mask = df_for_heatmap.isnull()\n",
    "    # Create a heatmap using seaborn\n",
    "    fig, ax = plt.subplots(figsize=(14,5))\n",
    "    df_formatted =df_for_heatmap.applymap( lambda val: f'{val*100 :,.0f}%')\n",
    "    fmt = ''\n",
    "    sns.heatmap(df_for_heatmap,  annot=df_formatted, cmap='YlGnBu', cbar=False, fmt=fmt,\n",
    "                square=True, annot_kws={'size': 10},\n",
    "                linewidths=1, linecolor='gray',\n",
    "                cbar_kws={'label': 'Value'}, ax=ax)\n",
    "\n",
    "    ax.patch.set_edgecolor('black')  \n",
    "    ax.patch.set_linewidth(1)  \n",
    "    plt.title(f\"Projected % Urban Land of \\n {city} \")\n",
    "    plt.savefig(f\"{maps}/{country}_{city}_{subdir}_heatmap.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_urbanland_graph(df_for_graph, country, city, subdir):\n",
    "    df_for_graph['Year'] = pd.to_datetime(\n",
    "                            df_for_graph['Year'],\n",
    "                            format='%Y'\n",
    "                            )\n",
    "    # plot data\n",
    "    g = sns.lineplot(data=df_for_graph, x=\"Year\", y=\"value\", \n",
    "                    hue=\"Scenario\"  , linewidth=2.5                     \n",
    "                    )\n",
    "    g.set(xlabel='Year', ylabel='% Urban Land')\n",
    "    g.axes.yaxis.set_major_formatter(ticker.FuncFormatter(percentage_formatter))\n",
    "    # Tell matplotlib to interpret the x-axis values as dates\n",
    "    g.xaxis_date()\n",
    "    g.get_figure().savefig(f\"{maps}/{country}_{city}_{subdir}_graph.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "def visualize_image(dir):\n",
    "    for file in os.listdir(dir):\n",
    "        file_path=os.path.join(dir, file)\n",
    "        print(file_path)\n",
    "        display(Image(filename=file_path))\n",
    "\n",
    "def overlay_sub_city_tier(shapefiles, shp, shp1, crs):\n",
    "    city = gpd.read_file(shp).to_crs(crs)\n",
    "    sub_city = gpd.read_file(shp1).to_crs(crs)\n",
    "    ax = city.to_crs(crs).plot(figsize=(8, 8) , \\\n",
    "        facecolor='none', edgecolor='black'\n",
    "        )\n",
    "    sub_city.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='red',alpha=0.2)\n",
    "    # sub_city.plot()\n",
    "    # return sub_city\n",
    "\n",
    "def create_hexbins(shapefiles, shp, hexbin_res, WGS84):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    poly = vector_file.geometry.unary_union\n",
    "    gdf_boundary = gpd.GeoDataFrame(geometry=[poly],crs=vector_file.crs)\n",
    "    # gdf_h3 = gdf_boundary.h3.polyfill(9,explode=True)\n",
    "    gdf_h3 = gdf_boundary.h3.polyfill(hexbin_res,explode=True)\n",
    "    gdf_h3 = gdf_h3.set_index('h3_polyfill').h3.h3_to_geo_boundary()\n",
    "    output_shapefile= f'{shapefiles}/h3_grid.shp'\n",
    "    gdf_h3.to_file(output_shapefile)\n",
    "    hexbins_projected = gpd.read_file(output_shapefile).to_crs(WGS84)\n",
    "    # gdf_h3.plot()\n",
    "    return hexbins_projected\n",
    "\n",
    "\n",
    "def hexbin_zonal_stats(shp, hexbins_projected , crs, raster , out_raster, statistic):\n",
    "    input_gpdf= gpd.read_file(shp).to_crs(crs)\n",
    "    Vector=input_gpdf\n",
    "    with rasterio.open(raster) as src:\n",
    "        Vector=Vector.to_crs(src.crs)\n",
    "        # print(f'Gdf srs: {Vector.crs}')\n",
    "        out_image, out_transform=mask(src,Vector.geometry,crop=True)\n",
    "        no_data_value=0\n",
    "        out_image = np.where(out_image<0,no_data_value,out_image) #replaced negative values with 0\n",
    "        out_meta=src.meta.copy() # copy the metadata of the source DEM\n",
    "        \n",
    "    out_meta.update({\n",
    "        \"driver\":\"Gtiff\",\n",
    "        \"height\":out_image.shape[1], # height starts with shape[1]\n",
    "        \"width\":out_image.shape[2], # width starts with shape[2]\n",
    "        \"transform\":out_transform\n",
    "    })\n",
    "                \n",
    "    with rasterio.open(out_raster,'w',**out_meta) as dst:\n",
    "        dst.write(out_image)\n",
    "\n",
    "    # add zonal stats to the hexbin gpd dataframe and replace nans with 0 \n",
    "    hexbins_projected[statistic] = zonal_stats(hexbins_projected, out_raster ,stats=statistic)\n",
    "    hexbins_projected[statistic] = [item[statistic] for item in hexbins_projected[statistic]]\n",
    "    hexbins_projected[statistic].fillna(0,inplace=True)\n",
    "    max=int(hexbins_projected.loc[hexbins_projected[statistic].idxmax()][statistic])\n",
    "    min=int(hexbins_projected.loc[hexbins_projected[statistic].idxmin()][statistic])\n",
    "    \n",
    "    return hexbins_projected , max , min\n",
    "\n",
    "# Change colormap by truncating\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# ,vmin=0,  vmax=int(max) , scheme='quantiles'\n",
    "def export_map_categorical(vector_file,  hexagons, legend_title,legends_format, crs ,cmap,  visualize_column, title, map_output):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax = hexagons.to_crs(crs).plot(ax=ax, \n",
    "                                   column= visualize_column, \\\n",
    "                                   linewidth=0 , \\\n",
    "                                   alpha=0.6, categorical=True, cmap=cmap, \\\n",
    "                                   legend=True,  # Add legend \n",
    "                                   legend_kwds={'loc':'upper right', \n",
    "                                                'bbox_to_anchor':(1.01, 1.01), \n",
    "                                                'fmt':legends_format,\n",
    "                                                'markerscale':1.01, \n",
    "                                                'title_fontsize':'small', \n",
    "                                                'fontsize':'x-small',\n",
    "                                                # 'fontweight': 'bold'\n",
    "                                                # title_fontsizeint or {'xx-small', 'x-small', 'small'\n",
    "                                                }  \n",
    "                                        )\n",
    "\n",
    "    vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "    cx.add_basemap(ax, \n",
    "                source=cx.providers.Esri.WorldImagery,  \n",
    "                crs=crs,\n",
    "                attribution=False, #No citation\n",
    "                ) \n",
    "    # plt.title(f'{title}',fontsize=16)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "    leg1 = ax.get_legend()\n",
    "    # Set markers to square shape\n",
    "    for ea in leg1.legendHandles:\n",
    "        ea.set_marker('s')\n",
    "    leg1.set_title(f'{legend_title}')\n",
    "    # ax.title.set_text(f'{title}') \n",
    "    ax.set_title(title,  fontweight='bold', wrap=True)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n",
    "    \n",
    "def export_map(map_dir, vector_file,  hexagons, legend_title,legends_format, crs ,cmap,  visualize_column, title, map_output):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax = hexagons.to_crs(crs).plot(ax=ax, \n",
    "                                   column= visualize_column, \\\n",
    "                                   linewidth=0 , \\\n",
    "                                   alpha=0.6, scheme=\"quantiles\", cmap=cmap, \\\n",
    "                                   legend=True,  # Add legend \n",
    "                                   legend_kwds={'loc':'upper right', \n",
    "                                                'bbox_to_anchor':(1.01, 1.01), \n",
    "                                                'fmt':legends_format,\n",
    "                                                'markerscale':1.01, \n",
    "                                                'title_fontsize':'small', \n",
    "                                                'fontsize':'x-small',\n",
    "                                                # 'fontweight': 'bold'\n",
    "                                                # title_fontsizeint or {'xx-small', 'x-small', 'small'\n",
    "                                                }  \n",
    "                                        )\n",
    "\n",
    "    vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "    cx.add_basemap(ax, \n",
    "                source=cx.providers.Esri.WorldImagery,  \n",
    "                crs=crs,\n",
    "                attribution=False, #No citation\n",
    "                ) \n",
    "    # plt.title(f'{title}',fontsize=16)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "    leg1 = ax.get_legend()\n",
    "    # Set markers to square shape\n",
    "    for ea in leg1.legendHandles:\n",
    "        ea.set_marker('s')\n",
    "    leg1.set_title(f'{legend_title}')\n",
    "    # ax.title.set_text(f'{title}')\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold', wrap=True)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n",
    "    \n",
    "def export_map_osm(map_dir, vector_file,  hexagons, legend_title, legends_format , crs ,cmap,  visualize_column, title, map_output):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax = hexagons.to_crs(crs).plot(ax=ax, \\\n",
    "                                   column= visualize_column, \n",
    "                                   linewidth=0 , \n",
    "                                   alpha=0.6, scheme=\"quantiles\", cmap=cmap, \n",
    "                                   legend=True,  # Add legend \\\n",
    "                                   legend_kwds={'loc':'upper right', \n",
    "                                                'bbox_to_anchor':(1.01, 1.01), \n",
    "                                                'fmt':legends_format,\n",
    "                                                'markerscale':1.01, \n",
    "                                                'title_fontsize':'small', \n",
    "                                                'fontsize':'x-small',\n",
    "                                                # 'fontweight': 'bold'\n",
    "                                                # title_fontsizeint or {'xx-small', 'x-small', 'small'\n",
    "                                                }  \n",
    "                                        )\n",
    "    \n",
    "    vector_file.to_crs(crs).plot(ax=ax,facecolor='none', edgecolor='k',alpha=0.1)\n",
    "    # cx.add_basemap(ax, source=cx.providers.Stamen.TonerLite,  crs=crs , zoom=12) \n",
    "    cx.add_basemap(ax,\n",
    "                    zoom=\"auto\",\n",
    "                    # zoom=10,\n",
    "                    crs=vector_file.crs.to_string(),\n",
    "                    # source=cx.providers.CartoDB.Voyager,\n",
    "                    source=cx.providers.OpenStreetMap.Mapnik,\n",
    "                    attribution=False, #No citation\n",
    "                    # attribution_size=1\n",
    "                    )\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    leg1 = ax.get_legend()\n",
    "    # Set markers to square shape\n",
    "    for ea in leg1.legendHandles:\n",
    "        ea.set_marker('s')\n",
    "    leg1.set_title(f'{legend_title}')\n",
    "    ax.set_title(title,  fontweight='bold', wrap=True)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n",
    "    \n",
    "def export_map_rasters(map_dir, shp, input_raster, crs , cmap, title, max_for_sublots, Year, ssp):\n",
    "\n",
    "    # dir_list = os.listdir(input_rasters_path)\n",
    "    # print(\"Files and directories in '\", input_rasters_path, \"' :\") \n",
    "    # # print the list\n",
    "    # print(dir_list)\n",
    "\n",
    "    # Read raster\n",
    "    b1 = rasterio.open(input_raster)\n",
    "    b2 = rasterio.open(input_raster)\n",
    "    b3 = rasterio.open(input_raster)\n",
    "    b4 = rasterio.open(input_raster)\n",
    "\n",
    "    # Read geometry\n",
    "    shapefile = gpd.read_file(shp).to_crs(crs) \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(6,6), constrained_layout=True) #\n",
    "    p1  = rasterio.plot.show(b1, interpolation='bilinear',  ax=ax[0,0], cmap=cmap, title=f'Year:{Year} SSP:{ssp}')\n",
    "    p2  = rasterio.plot.show(b2, interpolation='bilinear',  ax=ax[0,1], cmap=cmap, title=f'Year:{Year} SSP:{ssp}')\n",
    "    p3  = rasterio.plot.show(b3, interpolation='bilinear',  ax=ax[1,0], cmap=cmap, title=f'Year:{Year} SSP:{ssp}')\n",
    "    p4  = rasterio.plot.show(b4, interpolation='bilinear',  ax=ax[1,1], cmap=cmap, title=f'Year:{Year} SSP:{ssp}')\n",
    "\n",
    "    shapefile.plot(ax=p1,  facecolor='none', edgecolor='black',alpha=0.2 )\n",
    "    shapefile.plot(ax=p2,  facecolor='none', edgecolor='black',alpha=0.2)\n",
    "    shapefile.plot(ax=p3,  facecolor='none', edgecolor='black',alpha=0.2)\n",
    "    shapefile.plot(ax=p4,  facecolor='none', edgecolor='black',alpha=0.2 ,  lw=0.7)\n",
    "    \n",
    "    normalizer=Normalize(vmin=0,vmax=max_for_sublots)\n",
    "    im=cm.ScalarMappable(cmap=cmap,norm=normalizer)\n",
    "    # remove the x and y ticks\n",
    "    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "    # plt.tight_layout()\n",
    "    fig.colorbar(im, ax=ax.ravel().tolist(), location='bottom', shrink=0.7)\n",
    "    plt.show()\n",
    "    # ax.figure.savefig(f\"{map_dir}/{city}_{visualize_column}_truncated_cmap_max_div_4_2.png\")\n",
    "\n",
    "def export_figure_with_heading( document_output, mappath, title):  \n",
    "    document = Document(document_output)\n",
    "    p = document.add_paragraph()\n",
    "    r = p.add_run()\n",
    "    r.add_text(title)\n",
    "    r.add_picture(mappath)\n",
    "    r.add_text('Notes here')\n",
    "    document.save(document_output)\n",
    "    # return print('doc saved')\n",
    "\n",
    "def polygonize(raster_file ,crs, output_shapefile,   driver, mask_value, raster_val, ssp, Year):\n",
    "    # mask = None\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(raster_file) as src:\n",
    "            image = src.read(1) # first band\n",
    "            no_data_value=0\n",
    "            image = np.where(image<0,no_data_value,image) #replaced negative values with 0\n",
    "            results = (\n",
    "            {'properties': {raster_val : v}, 'geometry': s}\n",
    "            for i, (s, v) \n",
    "            in enumerate(\n",
    "                shapes(image, mask=mask_value, transform=src.transform)))\n",
    "\n",
    "    # The result is a generator of GeoJSON features\n",
    "    geoms = list(results)\n",
    "    # first feature\n",
    "    # print(geoms[0])\n",
    "    gpd_polygonized_raster  = gpd.GeoDataFrame.from_features(geoms)\n",
    "    gpd_polygonized_raster['Scenario'] = ssp\n",
    "    gpd_polygonized_raster['Year'] = Year\n",
    "    crs_utm = CRS.from_user_input(crs)\n",
    "    driver= 'ESRI Shapefile'\n",
    "    gpd_polygonized_raster.to_file(output_shapefile, driver, crs=crs_utm)\n",
    "    return gpd_polygonized_raster\n",
    "\n",
    "\n",
    "def polygonize_with_negatives(raster_file ,crs, output_shapefile,   driver, mask_value, raster_val, ssp, Year):\n",
    "    # mask = None\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(raster_file) as src:\n",
    "            image = src.read(1) # first band\n",
    "            # no_data_value=0\n",
    "            # image = np.where(image<0,no_data_value,image) #replaced negative values with 0\n",
    "            results = (\n",
    "            {'properties': {raster_val : v}, 'geometry': s}\n",
    "            for i, (s, v) \n",
    "            in enumerate(\n",
    "                shapes(image, mask=mask_value, transform=src.transform)))\n",
    "\n",
    "    # The result is a generator of GeoJSON features\n",
    "    geoms = list(results)\n",
    "    # first feature\n",
    "    # print(geoms[0])\n",
    "    gpd_polygonized_raster  = gpd.GeoDataFrame.from_features(geoms)\n",
    "    gpd_polygonized_raster['Scenario'] = ssp\n",
    "    gpd_polygonized_raster['Year'] = Year\n",
    "    crs_utm = CRS.from_user_input(crs)\n",
    "    driver= 'ESRI Shapefile'\n",
    "    gpd_polygonized_raster.to_file(output_shapefile, driver, crs=crs_utm)\n",
    "    return gpd_polygonized_raster\n",
    "\n",
    "\n",
    "\n",
    "def clip_raster_shapefiles(shp, output_shapefile,crs,output_geojson, output_csv, clipped_output_shapefile):\n",
    "    city_shapefile=   gpd.read_file(shp).to_crs(crs)\n",
    "    # city_shapefile.plot(aspect=1)\n",
    "    raster_shapefile=   gpd.read_file(output_shapefile).to_crs(crs)\n",
    "    # raster_shapefile.plot(aspect=1)\n",
    "    newdf = overlay(raster_shapefile, city_shapefile, how=\"intersection\")\n",
    "    # newdf.plot(aspect=1)\n",
    "    driver= 'ESRI Shapefile'\n",
    "    newdf.to_file(clipped_output_shapefile, driver)\n",
    "    driver='GeoJSON'\n",
    "    newdf.to_file(output_geojson, driver=driver)\n",
    "    newdf[\"centroid\"] = newdf[\"geometry\"].centroid\n",
    "    newdf['lon'] = newdf['centroid'].x\n",
    "    newdf['lat'] = newdf['centroid'].y\n",
    "    newdf[\"center\"] = newdf.dissolve().centroid #center\n",
    "    newdf['center_lon'] = newdf['center'].x\n",
    "    newdf['center_lat'] = newdf['center'].y\n",
    "    newdf = newdf.drop(columns=['geometry', 'center', 'centroid'])\n",
    "    newdf.to_csv(output_csv)\n",
    "    return newdf\n",
    "\n",
    "\n",
    "def merge_JsonFiles(dir, section):\n",
    "    folder = Path(dir)\n",
    "    shapefiles = folder.glob(f\"polygonized_clipped_{section}*.shp\")\n",
    "    gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles]).pipe(gpd.GeoDataFrame)\n",
    "    output_geojson= f'{dir}/panel_{section}.geojson'\n",
    "    driver='GeoJSON'\n",
    "    gdf.to_file(output_geojson, driver=driver)\n",
    "\n",
    "    # json_pattern = os.path.join(dir, f'geojson_{section}*.geojson')\n",
    "    # file_list = glob.glob(json_pattern)\n",
    "    # collection = []\n",
    "    # for file in file_list:\n",
    "    #     with open(file) as f:\n",
    "    #         layer = geojson.load(f)\n",
    "    #         collection.append(layer)\n",
    "    #         print(f\"{layer} appended\")\n",
    "    # geo_collection = geojson.GeometryCollection(collection)\n",
    "    # # geo_collection = collection #geojson.GeometryCollection(collection)\n",
    "    # with open(f'{dir}/panel_{section}.geojson', 'w') as f:\n",
    "    #     geojson.dump(geo_collection, f)\n",
    "    #     print(f\"dumped {dir}\")\n",
    "   \n",
    "\n",
    "def export_cckp_data(section):\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    appended_data = []\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.nc'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    variable_name = file_name.split(\"_\")[0] #.upper()\n",
    "                    Year = file_name.split(\"_\")[-1].upper()\n",
    "                    ssp = file_name.split(\"-\")[-3].upper() \n",
    "                    # ssp = subdir.upper()  #Already created \n",
    "                    # print(file_name,  file_name_path, subdir, Year, ssp, variable_name)\n",
    "                    # dataset = xr.open_dataset(file_name_path, decode_coords=\"all\")\n",
    "                    dataset = xr.open_dataset(file_name_path)\n",
    "                    clean_var_name= variable_name.replace('-', '_')\n",
    "                    clean_var_name= clean_var_name.replace(':', '')\n",
    "                    dataset=dataset.rename(name_dict={f'{variable_name}':f'{clean_var_name}'})\n",
    "                    var_long_label= dataset.variables[clean_var_name].attrs['long_name'].capitalize()\n",
    "                    var_long_label= var_long_label.replace(':', '')\n",
    "                    # print(variable_name ,\"----\", clean_var_name, \"----\", var_long_label)\n",
    "                    # start_date = \"2040-01-01\"\n",
    "                    # end_date = \"2040-12-31\"\n",
    "                    gpdf = gpd.read_file(shp) #\n",
    "                    gpdf['centroid'] = gpdf['geometry'].centroid\n",
    "                    gpdf['lat'] = gpdf['centroid'].y\n",
    "                    gpdf['lon'] = gpdf['centroid'].x\n",
    "                    lat= gpdf['centroid'].y\n",
    "                    lon= gpdf['centroid'].x\n",
    "                    # df = dataset[clean_var_name].sel(time=slice(start_date, end_date), lat=slice(int(lat) , int(lat)),lon=slice(int(lon) , int(lon)))\n",
    "                    df = dataset[clean_var_name].sel( lat=slice(int(lat) , int(lat)),lon=slice(int(lon) , int(lon)))\n",
    "                    df=df.to_dataframe( )\n",
    "                    df=df.dropna().reset_index()\n",
    "                    df[subdir]=df[clean_var_name]\n",
    "                    df[\"Variable\"]=var_long_label \n",
    "                    df[\"SSP\"]=ssp\n",
    "                    try:\n",
    "                        df_date = df[df[[subdir]].apply(pd.to_datetime, errors='coerce').isna().all(axis=1)]\n",
    "                        df_date[subdir] = df_date[subdir] / pd.to_timedelta(1, unit='D')\n",
    "                        appended_data.append(df_date)\n",
    "                    except:\n",
    "                        pass\n",
    "                        # print(f\"Passing {var_long_label}\")\n",
    "                        appended_data.append(df)\n",
    "        appended_data = pd.concat(appended_data, axis=0, join='inner').sort_index()\n",
    "        # appended_data = pd.concat(appended_data, axis=0).sort_index()\n",
    "        appended_data.to_csv(f\"{tables}/{country}_{city}_{subdir}_panel.csv\")\n",
    "    return appended_data\n",
    "\n",
    "def export_cckp_graph(df , x_var_name, y_var_names, subdir, city):\n",
    "    df[\"date\"] = pd.to_datetime(df[f\"{x_var_name}\"]) #.month\n",
    "    # df['month_year'] = df['date_column'].dt.to_period('M')\n",
    "    # y_var_names=df[y_var_names].unique()\n",
    "    for ssp in df[\"SSP\"].unique():\n",
    "        for y_var in df[y_var_names].unique():\n",
    "            sub_df = df[(df[\"SSP\"]==ssp) & (df[y_var_names]==y_var)]\n",
    "            sub_df=sub_df.sort_values([y_var_names, \"SSP\"], ascending=[True, True])\n",
    "            # Set a Style\n",
    "            sns.set_style('whitegrid')\n",
    "            # Create plot\n",
    "            ax = sns.lineplot(data=sub_df, x=x_var_name, \n",
    "                            y=subdir , errorbar=None, \n",
    "                            hue=y_var_names, style=y_var_names, markers=True\n",
    "                            )\n",
    "            # Set Title and Labels\n",
    "            ax.set_title(f'{subdir} in {city} for {ssp}', \n",
    "            fontdict={'size': 13, 'weight': 'bold'})\n",
    "            ax.set_xlabel(f'Month')\n",
    "            # ax.set_ylabel(f'{y_var}')\n",
    "            # Adjust the Legend\n",
    "            plt.legend(frameon=False, loc='lower left', bbox_to_anchor=(0, -.3), ncol=1)\n",
    "            # Format the date axis to be prettier.\n",
    "            # ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%y'))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "            # ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "            ax.xaxis.set_major_locator(mdates.AutoDateLocator(interval_multiples=False))\n",
    "            # # Increase size and change color of axes ticks\n",
    "            GREY91 = \"#e8e8e8\"\n",
    "            ax.tick_params(axis=\"x\", length=12, color=GREY91)\n",
    "            ax.tick_params(axis=\"y\", length=8, color=GREY91)\n",
    "            # Remove the Spines\n",
    "            sns.despine()\n",
    "            plt.show()\n",
    "            ax.figure.savefig(f\"{maps}/{country}_{city}_{subdir}_graph.png\",  bbox_inches='tight', dpi=300) #, transparent=True  \n",
    "\n",
    "    return df #print(f\"{subdir} graph exported {df}\")\n",
    "\n",
    "def get_lat_lon(s):\n",
    "    # split by comma\n",
    "    l = s[1:-1].split(',')\n",
    "    # get lat lon without direction\n",
    "    lat = int(l[0]) + ( float(l[1][:-1]) / 60) # -1 for skipping direction letter\n",
    "    lon = int(l[2]) + ( float(l[3][:-1]) / 60)\n",
    "    # check cardinal direction letter\n",
    "    if l[1][-1] in ['S', 's']: lat = -lat        \n",
    "    if l[3][-1] in ['W', 'w']: lon = -lon \n",
    "    return lat, lon\n",
    "\n",
    "# s = \"(38,40.1365N, 75,4.23933W)\"\n",
    "# # lat, lon = get_lat_lon(s)\n",
    "# # lat, lon\n",
    "\n",
    "def get_city_poolygon(shp):\n",
    "    # get the shapes\n",
    "    r = shapefile.Reader(shp)\n",
    "    shapes = r.shapes()\n",
    "    # build a shapely polygon from your shape\n",
    "    polygon = shape(shapes[0])   \n",
    "    # read your shapefile\n",
    "    return polygon\n",
    "\n",
    "def check(lon, lat):\n",
    "    # build a shapely point from your geopoint\n",
    "    point = Point(lon, lat)\n",
    "    # the contains function does exactly what you want\n",
    "    return polygon.contains(point)\n",
    "\n",
    "def create_storm_csvs(data, section):\n",
    "    for subdir in section:      \n",
    "        file_list = glob.glob(os.path.join( os.path.join(data, subdir), \"STORM_DATA_*.txt\"))\n",
    "        corpus = []\n",
    "        files = []\n",
    "        for file_path in file_list:\n",
    "            # file = open(file_path, 'r')\n",
    "            # for line in file.readlines():\n",
    "            #     fname = line.rstrip().split(',') #using rstrip to remove the \\n\n",
    "            #     corpus.append(fname)\n",
    "            #     files.append(''.join([n for n in os.path.basename(file_path) if n.isdigit()]))\n",
    "            file_name= os.path.basename(file_path)\n",
    "            df = pd.read_csv(file_path, sep=',')\n",
    "            df.to_csv(f\"{tables}/{subdir}/converted_storm_csv_{file_name}.csv\")\n",
    "\n",
    "def export_storms():\n",
    "    section=['tropicalstorms']\n",
    "    subdir='tropicalstorms'\n",
    "    column_names = ['Year','Month' , 'TC number', 'Time step', 'Basin ID', 'Latitude', 'Longitude', 'Minimum pressure', 'Maximum wind speed', 'Radius to maximum winds', 'Category', 'Landfall', 'Distance to land']\n",
    "    for subdir in section:\n",
    "        file_list = glob.glob(os.path.join( os.path.join(data, subdir), \"converted_storm_csv_STORM_DATA_*.txt.csv\"))\n",
    "        files = []\n",
    "        appended_data=[]\n",
    "        for file_path in file_list:\n",
    "            file_name= os.path.basename(file_path)\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            df_points= pd.read_csv(file_path,  names=column_names, header=None)\n",
    "            # print(\"Read file:\" , file_path)\n",
    "            for lat,lon in zip(df_points.Latitude, df_points.Longitude):\n",
    "                if check(lon, lat):\n",
    "                    files.append(file_path)\n",
    "                    print(check(lon, lat))\n",
    "                    sub_df = df_points[(df_points[\"Latitude\"]==lat) & (df_points[\"Longitude\"]==lon)]\n",
    "                    appended_data.append(sub_df)\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    # write DataFrame to an excel sheet \n",
    "    appended_data.to_csv(f\"{tables}/{country}_{city}_{subdir}_storms.csv\")\n",
    "    return appended_data\n",
    "\n",
    "def map_storms(appended_data):\n",
    "    lon =appended_data[\"Longitude\"]\n",
    "    lat =appended_data[\"Latitude\"]\n",
    "    # lat = [28.6877899169922, 28.663863, 28.648287, 28.5429172515869]\n",
    "    geometry = [Point(xy) for xy in zip(lon,lat)]\n",
    "    wardlink = shp\n",
    "    ward = gpd.read_file(wardlink, bbox=None, mask=None, rows=None)\n",
    "    geo_df = gpd.GeoDataFrame(geometry = geometry)\n",
    "\n",
    "    ward.crs = {'init':\"epsg:4326\"}\n",
    "    geo_df.crs = {'init':\"epsg:4326\"}\n",
    "\n",
    "    # plot the polygon\n",
    "    ax = ward.plot(alpha=0.35, color='#d66058', zorder=1)\n",
    "    # plot the boundary only (without fill), just uncomment\n",
    "    #ax = gpd.GeoSeries(ward.to_crs(epsg=3857)['geometry'].unary_union).boundary.plot(ax=ax, alpha=0.5, color=\"#ed2518\",zorder=2)\n",
    "    ax = gpd.GeoSeries(ward['geometry'].unary_union).boundary.plot(ax=ax, alpha=0.5, color=\"#ed2518\",zorder=2)\n",
    "\n",
    "    # plot the marker\n",
    "    ax = geo_df.plot(ax = ax, markersize = 20, color = 'red',\\\n",
    "                    marker = '*', zorder=3,\n",
    "                    legend=True,  # Add legend \n",
    "                    legend_kwds={'loc':'upper right', \n",
    "                                    'bbox_to_anchor':(1, 1), \n",
    "                                                    'markerscale':1.01, \n",
    "                                                    'title_fontsize':'small', \n",
    "                                                    'fontsize':'x-small'\n",
    "                                                    } ,\n",
    "                                                    )\n",
    "    ctx.add_basemap(ax, crs=geo_df.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "        \n",
    "    leg1 = ax.get_legend()\n",
    "    # Set markers to square shape\n",
    "    # for ea in leg1.legendHandles:\n",
    "    #     ea.set_marker('s')\n",
    "    # leg1.set_title(f'{legend_title}')\n",
    "    # ax.title.set_text(f'{title}')\n",
    "    # plt.tight_layout()\n",
    "    # ax.figure.savefig(map_output)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_gdp_iiasa(country):\n",
    "    # section='tropicalstorms'\n",
    "    for subdir in section:\n",
    "        file_list = glob.glob(os.path.join( os.path.join(data, subdir), \"iamc_db_GDP.xlsx\"))\n",
    "        for file_path in file_list:\n",
    "            file_name= os.path.basename(file_path)\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            iiasa_gdp_df = pd.read_excel(file_path, sheet_name=\"data\")\n",
    "            iiasa_gdp_df = iiasa_gdp_df[iiasa_gdp_df[\"Region\"]==country]\n",
    "    return iiasa_gdp_df\n",
    "\n",
    "def process_pop_iiasa(country):\n",
    "    # section\n",
    "    for subdir in section:\n",
    "        file_list = glob.glob(os.path.join( os.path.join(data, subdir), \"iamc_db.xlsx\"))\n",
    "        for file_path in file_list:\n",
    "            file_name= os.path.basename(file_path)\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            iiasa_pop_df = pd.read_excel(file_path, sheet_name=\"data\")\n",
    "            iiasa_pop_df = iiasa_pop_df[iiasa_pop_df[\"Region\"]==country]\n",
    "    return iiasa_pop_df\n",
    "\n",
    "\n",
    "def visualize_gdp_graph(df_for_graph, country, city, subdir):\n",
    "    df_for_graph['Year'] = pd.to_datetime(\n",
    "                            df_for_graph['Year'],\n",
    "                            format='%Y'\n",
    "                            )\n",
    "    # plot data\n",
    "    g = sns.lineplot(data=df_for_graph, x=\"Year\", y=\"value\", \n",
    "                     hue=\"Scenario\"  , linewidth=2.5\n",
    "                     )\n",
    "    g.set(xlabel='Year', ylabel='GDP (Billions USD)')\n",
    "    g.axes.yaxis.set_major_formatter(ticker.FuncFormatter(billions_formatter))\n",
    "    # Tell matplotlib to interpret the x-axis values as dates\n",
    "    g.xaxis_date()\n",
    "    g.get_figure().savefig(f\"{maps}/{country}_{city}_{subdir}_graph.jpeg\" , dpi=300, bbox_inches='tight')\n",
    "\n",
    "# iiasa_pop_df=process_pop_iiasa(country=country)\n",
    "# # df_tranformed= iiasa_pop_df.pivot(index=['Scenario'], columns='Year', values=\"sum\")\n",
    "# # df=iiasa_pop_df.drop([], axis=1)\n",
    "# df = pd.melt(iiasa_pop_df, id_vars=[\"Model\", \"Scenario\"  ], value_vars='year')\n",
    "# # df_graph = df.melt(ignore_index=False).reset_index()\n",
    "# # iiasa_pop_df\n",
    "# iiasa_gdp_df=process_gdp_iiasa(country=country)\n",
    "# iiasa_gdp_df\n",
    "\n",
    "\n",
    "def process_erosion():\n",
    "    # section\n",
    "    column_names = ['Latitude','Longitude' , 'percentile_1', 'percentile_5', 'percentile_17', 'percentile_50','percentile_83', 'percentile_95', 'percentile_99']\n",
    "    for subdir in section:      \n",
    "        file_list = glob.glob(os.path.join( os.path.join(data, subdir), \"globalErosionProjections_Long_Term_Change*.csv\"))\n",
    "        files = []\n",
    "        appended_data=[]\n",
    "        for file_path in file_list:\n",
    "            file_name= os.path.basename(file_path)\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            # df_points= pd.read_csv(file_path)\n",
    "            df_points= pd.read_csv(file_path,  names=column_names, header=None)\n",
    "            # print(\"Read file:\" , file_path)\n",
    "            for lat,lon in zip(df_points.Latitude, df_points.Longitude):\n",
    "                if check(lon, lat):\n",
    "                    files.append(file_path)\n",
    "                    # print(check(lon, lat))\n",
    "                    sub_df = df_points[(df_points[\"Latitude\"]==lat) & (df_points[\"Longitude\"]==lon)]\n",
    "                    appended_data.append(sub_df)\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    # write DataFrame to an excel sheet \n",
    "    appended_data.to_csv(f\"{tables}/{country}_{city}_{subdir}_erosion.csv\")\n",
    "    erosion_df=appended_data\n",
    "    return erosion_df\n",
    "    # return df_points\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "\n",
    "def difference_rasters(base_year_raster, end_year_raster, diff_out_path):\n",
    "    r1=rioxarray.open_rasterio(base_year_raster, masked=True).squeeze()\n",
    "    r2=rioxarray.open_rasterio(end_year_raster, masked=True).squeeze()\n",
    "    diff_raster = r2 - r1\n",
    "    diff_raster.rio.to_raster(diff_out_path)\n",
    "    return diff_out_path\n",
    "\n",
    "def difference_rasters_float_32(base_year_raster, end_year_raster, diff_out_path):\n",
    "    r1=rioxarray.open_rasterio(base_year_raster, masked=True).squeeze()\n",
    "    r2=rioxarray.open_rasterio(end_year_raster, masked=True).squeeze()\n",
    "    diff_raster = r2 - r1\n",
    "    diff_raster =diff_raster.astype(np.float32)\n",
    "    diff_raster.rio.to_raster(diff_out_path)\n",
    "    return diff_out_path\n",
    "\n",
    "\n",
    "def convert_raster_to_float_32(input_raster, out_raster):\n",
    "    input_raster=rioxarray.open_rasterio(input_raster, masked=True).squeeze()\n",
    "    input_raster =input_raster.astype(np.float32)\n",
    "    input_raster.rio.to_raster(out_raster)\n",
    "    return out_raster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_country_iso_code_from_country_name(country_name):\n",
    "#     # get country iso_code from country name\n",
    "#     gc_city_folder = os.path.join(data, f'{section[0]}/GC_countries/')\n",
    "#     # Step 0.2: Get city's historical demographic change\n",
    "#     df = pd.read_csv(gc_city_folder+'GC_{}.csv'.format(country_iso3))\n",
    "\n",
    "#     if len(ssp_country)==0:\n",
    "#         print('Country not found in SSP database. Check if ISO3 name of the country is correct, and whether the country is actually available in the SSP database.')\n",
    "#         return 0,0\n",
    "#     else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssp2_2013_dataset(country_iso3, section, dataset,var):\n",
    "    gc_city_folder = os.path.join(data, f'{section[0]}/GC_countries/')\n",
    "    ssp_master_fn = os.path.join(data, f'{section[0]}/{dataset}')\n",
    "    # Step 0.1: Get country-level projections\n",
    "    if \".csv\" in dataset:\n",
    "        ssp_master = pd.read_csv(ssp_master_fn)\n",
    "    print(ssp_master.columns)\n",
    "    # ssp_master[ssp_master.columns] = ssp_master[ssp_master.columns].astype(str).apply(lambda col: col.str.upper())\n",
    "    ssp_master.columns = [col.upper() for col in ssp_master if col in ssp_master.columns]\n",
    "    # ssp_master['REGION'] = ssp_master['REGION'].str[:4]\n",
    "    ssp_master['SCENARIO'] = ssp_master['SCENARIO'].str[:4]\n",
    "    print(\"capital---\", ssp_master.columns)\n",
    "    ssp_country = ssp_master.loc[ssp_master['REGION']==country_iso3]\n",
    "    print(f\"variables----->>>> {ssp_country['MODEL'].unique()}\")\n",
    "\n",
    "    ssp_country = ssp_country.loc[ssp_country['MODEL']==var]\n",
    "    ssp_country.reset_index(inplace=True, drop=True)\n",
    "    if len(ssp_country)==0:\n",
    "        print('Country not found in SSP database. Check if ISO3 name of the country is correct, and whether the country is actually available in the SSP database.')\n",
    "        return 0,0\n",
    "    else:\n",
    "        del ssp_master\n",
    "    \n",
    "    \n",
    "    # Step 0.2: Get city's historical demographic change\n",
    "    global_cities = pd.read_csv(gc_city_folder+'GC_{}.csv'.format(country_iso3))\n",
    "    gc_city = global_cities.loc[global_cities['Location']==city]\n",
    "    if len(gc_city)==0:\n",
    "        print('City not found in the Global Cities database. Manually check if the city name spelling is correct, and whether the city is actually available in the Global Cities database.')\n",
    "        return 0,0\n",
    "    else:\n",
    "        del global_cities\n",
    "    iso3=gc_city['iso3'].unique()[0]\n",
    "    country_name=gc_city['Country'].unique()[0]\n",
    "    print(iso3, country_name.title())\n",
    "    ssp_country['iso3']=iso3.title()\n",
    "    ssp_country['REGION']=country_name.title()\n",
    "    \n",
    "    return ssp_country, country_name, country_iso3\n",
    "\n",
    "# 2023 ssp3\n",
    "def get_ssp3_2023_dataset(country, section, dataset,var):\n",
    "    ssp_master_fn = os.path.join(data, f'{section[0]}/{dataset}')\n",
    "    # Step 0.1: Get country-level projections\n",
    "    if \".csv\" in dataset:\n",
    "        ssp_master = pd.read_csv(ssp_master_fn)\n",
    "    print(ssp_master.columns)\n",
    "    ssp_master.columns = [col.upper() for col in ssp_master if col in ssp_master.columns]\n",
    "    print(\"capital---\", ssp_master.columns)\n",
    "    ssp_country = ssp_master.loc[ssp_master['REGION']==country]\n",
    "    ssp_country = ssp_country.loc[ssp_country['MODEL']==var]\n",
    "    ssp_country.reset_index(inplace=True, drop=True)\n",
    "    if len(ssp_country)==0:\n",
    "        print('Country not found in SSP database. Check if ISO3 name of the country is correct, and whether the country is actually available in the SSP database.')\n",
    "        return 0,0\n",
    "    else:\n",
    "        del ssp_master\n",
    "    return ssp_country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "country_iso3 = 'BGD'\n",
    "var= 'IIASA-WiC POP' # 'IIASA GDP 2023', \n",
    "var_proper=\"Population\"\n",
    "section= ['demographic']\n",
    "dataset=r\"SspDb_country_data_2013-06-12.csv\"\n",
    "ssp_country_2013, country_name, country_iso3=get_ssp2_2013_dataset(country_iso3, section, dataset,var) \n",
    "ssp_country_2013=ssp_country_2013[ssp_country_2013['VARIABLE']==var_proper]\n",
    "\n",
    "country_iso3 = 'BGD'\n",
    "country_name=\"Bangladesh\"\n",
    "var= 'IIASA-WiC POP 2023' # 'IIASA GDP 2023', \n",
    "# var= 'IIASA GDP 2023'\n",
    "section= ['demographic']\n",
    "dataset=r\"1706548837040-ssp_basic_drivers_release_3.0_full.csv\"\n",
    "ssp_country=get_ssp3_2023_dataset(country=country_name, \n",
    "                                                  section=section, dataset=dataset, var=var) \n",
    "ssp_country=ssp_country[ssp_country['VARIABLE']==var_proper]\n",
    "df_merged=ssp_country_2013.merge(ssp_country, how='outer', \n",
    "                                 on=['SCENARIO','REGION','VARIABLE'], \n",
    "                                 suffixes=('_2013', '_2023'),\n",
    "                                 indicator= True)\n",
    "cols = df_merged.columns.tolist()\n",
    "for j, s in enumerate(cols):  \n",
    "    for i in range(2025,2105,5):\n",
    "        try: \n",
    "            a = int(s) \n",
    "            y1=f\"{i}_2013\"\n",
    "            y2=f\"{i}_2023\"\n",
    "            # print(f\"y1={y1}, y2={y2}, s={s}\")\n",
    "            df_merged[f\"rescaling_factor_{y2}_div_{y1}\"] = df_merged[y2]/df_merged[y1]\n",
    "            df_merged[f\"updated_{i}\"] = df_merged[y1]*df_merged[f\"rescaling_factor_{y2}_div_{y1}\"]\n",
    "            df_merged.to_csv()\n",
    "        except ValueError: \n",
    "                    print(f'{s} was not an integer')                                 \n",
    "df_merged.to_csv(f\"{tables}/{country}_{city}_{var}_rescaled.csv\")\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine by SCENARIO\tREGION\tVARIABLE the two dataset. Name the columns with left and right. This will give us the scaling facor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "country_iso3 = 'BGD'\n",
    "var= 'IIASA GDP' #'IIASA-WiC POP' # 'IIASA GDP 2023', \n",
    "var_proper=\"GDP|PPP\" #\"Population\"\n",
    "section= ['demographic']\n",
    "dataset=r\"SspDb_country_data_2013-06-12.csv\"\n",
    "ssp_country_2013, country_name, country_iso3=get_ssp2_2013_dataset(country_iso3, section, dataset,var) \n",
    "ssp_country_2013=ssp_country_2013[ssp_country_2013['VARIABLE']==var_proper]\n",
    "\n",
    "# Get gdp defkator and inflate the gdp to 2017 USD \n",
    "dataset=r'gdp_deflator.csv'\n",
    "deflator= os.path.join(data, f'{section[0]}/{dataset}')\n",
    "deflator = pd.read_csv(deflator)\n",
    "deflator=deflator[deflator[\"Country Code\"]==country_iso3]\n",
    "deflator_multiplier=(deflator['2017']/deflator['2005']).item()\n",
    "print(deflator_multiplier)\n",
    "\n",
    "# Multiply all columns by the defaltor multiplier\n",
    "cols = ssp_country_2013.columns.tolist()\n",
    "for j, s in enumerate(cols):  \n",
    "    for i in range(2025,2105,5):\n",
    "        try: \n",
    "            a = int(s) \n",
    "            y1=f\"{i}\"\n",
    "            # print(f\"y1={y1}, y2={y2}, s={s}\")\n",
    "            ssp_country_2013[f\"{i}\"] = ssp_country_2013[y1]*deflator_multiplier\n",
    "        except ValueError: \n",
    "                    print(f'{s} was not an integer') \n",
    "country_name=\"Bangladesh\"\n",
    "var= 'IIASA GDP 2023'\n",
    "section= ['demographic']\n",
    "dataset=r\"1706548837040-ssp_basic_drivers_release_3.0_full.csv\"\n",
    "ssp_country=get_ssp3_2023_dataset(country=country_name, \n",
    "                                                  section=section, dataset=dataset, var=var) \n",
    "\n",
    "df_merged=ssp_country_2013.merge(ssp_country, how='outer', \n",
    "                                 on=['SCENARIO','REGION','VARIABLE'], \n",
    "                                 suffixes=('_2013', '_2023'),\n",
    "                                 indicator= True)\n",
    "\n",
    "cols = df_merged.columns.tolist()\n",
    "for j, s in enumerate(cols):  \n",
    "    for i in range(2025,2105,5):\n",
    "        try: \n",
    "            a = int(s) \n",
    "            y1=f\"{i}_2013\"\n",
    "            y2=f\"{i}_2023\"\n",
    "            # print(f\"y1={y1}, y2={y2}, s={s}\")\n",
    "            df_merged[f\"rescaling_factor_{y2}_div_{y1}\"] = df_merged[y2]/df_merged[y1]\n",
    "            df_merged[f\"updated_{i}\"] = df_merged[y1]*df_merged[f\"rescaling_factor_{y2}_div_{y1}\"]\n",
    "        except ValueError: \n",
    "                    print(f'{s} was not an integer')                                 \n",
    "df_merged.to_csv(f\"{tables}/{country}_{city}_{var}_rescaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year--> 2025  ....ssp_value-->> SSP1...rescaling_factor--->> 2.7084350327164718e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.7084350327164718e-17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rescaling_factor(input_csv, year, ssp_value):\n",
    "    df= pd.read_csv(input_csv)\n",
    "    df=df[[f\"rescaling_factor_{year}_2023_div_{year}_2013\", \"SCENARIO\"]] \n",
    "    rescaling_factor=df[df['SCENARIO']==ssp_value][f\"rescaling_factor_{year}_2023_div_{year}_2013\"].item()\n",
    "    print(f\"year--> {year}  ....ssp_value-->> {ssp_value}...rescaling_factor--->> {rescaling_factor}\")\n",
    "    return rescaling_factor\n",
    "\n",
    "var='IIASA GDP 2023'\n",
    "input_csv=f\"{tables}/{country}_{city}_{var}_rescaled.csv\"\n",
    "year=2025\n",
    "ssp_value=\"SSP1\"\n",
    "rescaling_factor=get_rescaling_factor(input_csv, year, ssp_value)\n",
    "rescaling_factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year--> 2025  ....ssp_value-->> SSP1...rescaling_factor--->> 2.7084350327164718e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.7084350327164718e-17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var='IIASA GDP 2023'\n",
    "input_csv=f\"{tables}/{country}_{city}_{var}_rescaled.csv\"\n",
    "year=2025\n",
    "ssp_value=\"SSP1\"\n",
    "rescaling_factor=get_rescaling_factor(input_csv, year, ssp_value)\n",
    "rescaling_factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year--> 2025  ....ssp_value-->> SSP1...rescaling_factor--->> 1.031974297504538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.031974297504538"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var= 'IIASA-WiC POP 2023' # 'IIASA GDP 2023', \n",
    "input_csv=f\"{tables}/{country}_{city}_{var}_rescaled.csv\"\n",
    "year=2025\n",
    "ssp_value=\"SSP1\"\n",
    "rescaling_factor=get_rescaling_factor(input_csv, year, ssp_value)\n",
    "rescaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical map function with same legend breaks across maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_popdynamics():\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name.split('_')[1].upper() \n",
    "                    ssp = file_name.split('_')[0].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "                    combined_results.append(df)\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==\"SSP2\" or ssp==\"SSP5\") and (Year==\"2050\" or Year==\"2100\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Population in {Year} under {ssp}\"\n",
    "                        legend_title=\"Population (x1000)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "                        \n",
    "                        var= 'IIASA-WiC POP 2023' # 'IIASA GDP 2023', \n",
    "                        input_csv=f\"{tables}/{country}_{city}_{var}_rescaled.csv\"\n",
    "                        year=Year #2025\n",
    "                        ssp_value= ssp # \"SSP1\"\n",
    "                        rescaling_factor=get_rescaling_factor(input_csv, year, ssp_value)\n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].multiply(rescaling_factor)\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        polygonized_shapefile.to_file(clipped_output_shapefile, driver)\n",
    "\n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(1000).round(0)\n",
    "                        # Function for getting quantile ranges. \n",
    "                        polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 )\n",
    "                        a = pd.qcut(polygonized_shapefile[raster_val], 5).cat.categories.right\n",
    "                        b = pd.qcut(polygonized_shapefile[raster_val], 5).cat.categories.left\n",
    "                        binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "                        \n",
    "                        # binned = pd.cut(polygonized_shapefile[raster_val], bins=bins, labels=labels)\n",
    "                        # print(f\"a:{a}---df1:{df1} df2:{df2} , bins: {bins}, binned: {binned}, binned_df {binned_df}\" )\n",
    "\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles.png\"\n",
    "                        quantile_range= \"quantile_range\"\n",
    "                        export_the_map= export_map_categorical(vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                                crs=crs, cmap=cmap , visualize_column=quantile_range,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "\n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                                crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "\n",
    "\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "        # Wide\n",
    "        [stats_to_get] = stats_to_get\n",
    "        df_tranformed= df.pivot(index=['Scenario'], columns='Year', values=stats_to_get)\n",
    "        # Long\n",
    "        df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "        # export csv of wide\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "        #run the visualize function to export the heatmap\n",
    "        visualize_popdynamics_figure(df_tranformed, country, city, subdir)\n",
    "        # Visualize pop dynamics graph\n",
    "        visualize_popdynamics_graph(df_graph, country, city, subdir)\n",
    "        # Merge json files for all ssps \n",
    "        merge_JsonFiles_=merge_JsonFiles(dir=shapefiles, section=subdir)\n",
    "        return clipped_output_shapefile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_popdynamics_change(input_ssp, start_year, end_year):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        extension = '.tif'\n",
    "        two_raster_list=[]\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name.split('_')[1].upper() \n",
    "                    ssp = file_name.split('_')[0].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==f\"{input_ssp}\" and Year==f\"{start_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = out_rst #f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Difference in Population in {Year} under {ssp} from 2050\"\n",
    "                        legend_title=\"Population (x1000)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        \n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "                    elif (ssp==f\"{input_ssp}\" ) and (  Year==f\"{end_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}_1.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 =out_rst # f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        # map_title= f\"Population in {Year} under {ssp}\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}_1.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "        # for delta in len(two_raster_list):\n",
    "        diff_out_path = f'{rasters}/projected_clipped_difference_{subdir}_{file_name}.tif'\n",
    "        base_year_raster=two_raster_list[0]\n",
    "        end_year_raster=two_raster_list[1]\n",
    "        print(f\"base_year_raster:: {base_year_raster}-----end_year_raster::{end_year_raster}\")\n",
    "        diff_out_path=difference_rasters(base_year_raster, end_year_raster, diff_out_path)\n",
    "                                \n",
    "        legend_title=\"Population(x1000)\"\n",
    "        map_title= f\"Population change during {start_year}-{end_year} ({input_ssp})\"\n",
    "        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}_diff.shp'\n",
    "        output_geojson= f'{shapefiles}/geojson_{subdir}_diff.geojson'\n",
    "        output_csv= f'{tables}/csv_{subdir}_{file_name}_diff.csv'\n",
    "        driver= 'ESRI Shapefile'\n",
    "        mask_value=None\n",
    "        raster_val= subdir[0:9]\n",
    "        name = polygonize_with_negatives(raster_file=diff_out_path ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                            driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_difference_{subdir}_{file_name}.shp'\n",
    "        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                    output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(1000).round(0)\n",
    "        \n",
    "        # Function for getting quantile ranges. \n",
    "        polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 ,duplicates='drop')\n",
    "        a = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.right\n",
    "        b = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.left\n",
    "        binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "        \n",
    "        map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles_diff_{input_ssp}_{start_year}_{end_year}.png\"\n",
    "        quantile_range= \"quantile_range\"\n",
    "        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                title=map_title, map_output=map_output)\n",
    "       \n",
    "        return polygonized_shapefile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_gdp():\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    Year = file_name[3:7].upper() \n",
    "                    ssp = file_name[8:14].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "                    combined_results.append(df)\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==\"SSP2\" or ssp==\"SSP5\") and (Year==\"2050\" or Year==\"2100\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"GDP(Millions USD) in {Year} under {ssp}\"\n",
    "                        legend_title=\"GDP(Millions USD)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "                        \n",
    "                        var='IIASA GDP 2023'\n",
    "                        input_csv=f\"{tables}/{country}_{city}_{var}_rescaled.csv\"\n",
    "                        year=Year #2025\n",
    "                        ssp_value= ssp # \"SSP1\"\n",
    "                        rescaling_factor=get_rescaling_factor(input_csv, year, ssp_value)\n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].multiply(rescaling_factor)\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        polygonized_shapefile.to_file(clipped_output_shapefile, driver)\n",
    "\n",
    "\n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(1000000).round(2)\n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "\n",
    "\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "       # Wide\n",
    "        [stats_to_get] = stats_to_get\n",
    "        df_tranformed= df.pivot(index=['Scenario'], columns='Year', values=stats_to_get)        \n",
    "        # Long\n",
    "        df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "        # export csv of wide\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "        #run the visualize function to export the heatmap\n",
    "        visualize_gdp_figure(df_tranformed, country, city, subdir)\n",
    "        visualize_gdp_graph(df_graph, country, city, subdir)\n",
    "        merge_JsonFiles_=merge_JsonFiles(dir=shapefiles, section=subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_gdp_change(input_ssp, start_year, end_year):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        extension = '.tif'\n",
    "        two_raster_list=[]\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name[3:7].upper() \n",
    "                    ssp = file_name[8:14].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==f\"{input_ssp}\" and Year==f\"{start_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = out_rst #f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        \n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "                    elif (ssp==f\"{input_ssp}\" ) and (  Year==f\"{end_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}_1.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 =out_rst # f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        # map_title= f\"Population in {Year} under {ssp}\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}_1.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "        # for delta in len(two_raster_list):\n",
    "        diff_out_path = f'{rasters}/projected_clipped_difference_{subdir}_{file_name}.tif'\n",
    "        base_year_raster=two_raster_list[0]\n",
    "        end_year_raster=two_raster_list[1]\n",
    "        print(f\"base_year_raster:: {base_year_raster}-----end_year_raster::{end_year_raster}\")\n",
    "        diff_out_path=difference_rasters(base_year_raster, end_year_raster, diff_out_path)\n",
    "        \n",
    "        map_title= f\"GDP(Millions USD) change during {start_year}-{end_year} ({input_ssp})\"\n",
    "        legend_title=\"GDP(Millions USD)\"\n",
    "        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}_diff.shp'\n",
    "        output_geojson= f'{shapefiles}/geojson_{subdir}_diff.geojson'\n",
    "        output_csv= f'{tables}/csv_{subdir}_{file_name}_diff.csv'\n",
    "        driver= 'ESRI Shapefile'\n",
    "        mask_value=None\n",
    "        raster_val= subdir[0:9]\n",
    "        name = polygonize_with_negatives(raster_file=diff_out_path ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                            driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_difference_{subdir}_{file_name}.shp'\n",
    "        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                    output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(1000000).round(2)\n",
    "\n",
    "        # polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(1000).round(0)\n",
    "        \n",
    "        # Function for getting quantile ranges. \n",
    "        polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 ,duplicates='drop')\n",
    "        a = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.right\n",
    "        b = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.left\n",
    "        binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "        \n",
    "        map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles_diff_{input_ssp}_{start_year}_{end_year}.png\"\n",
    "        quantile_range= \"quantile_range\"\n",
    "        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                title=map_title, map_output=map_output)\n",
    "       \n",
    "        return polygonized_shapefile \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urban Land Extent\n",
    "The dataset includes gridded global maps of urban land fractions for the period of 20002100 at the 1-km resolution and updates at decadal time intervals, under five urban land expansion scenarios consistent with the SSPs (SSP1  sustainability, SSP2  middle of the road, SSP3  regional rivalry, SSP4  inequality, SSP5  fossil-fuelled development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_urbanland():\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    Year = file_name[5:9].upper() \n",
    "                    ssp = file_name[0:4].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "                    combined_results.append(df)\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==\"SSP2\" or ssp==\"SSP5\") and (Year==\"2050\" or Year==\"2100\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"%Urban Land in {Year} under {ssp}\"\n",
    "                        legend_title=\"%age Urban Land\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].multiply(100).round(0)\n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "        # Wide\n",
    "        # df_tranformed= df.pivot(index=['Scenario'], columns='Year', values='mean')\n",
    "        [stats_to_get] = stats_to_get\n",
    "        df_tranformed= df.pivot(index=['Scenario'], columns='Year', values=stats_to_get)\n",
    "        # Long\n",
    "        df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "        # export csv of wide\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "        # print(df_tranformed)\n",
    "        #run the visualize function to export the heatmap\n",
    "        visualize_urbanland_figure(df_tranformed, country, city, subdir)\n",
    "        visualize_urbanland_graph(df_graph, country, city, subdir)\n",
    "        merge_JsonFiles_=merge_JsonFiles(dir=shapefiles, section=subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_urbanland_change(input_ssp, start_year, end_year):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        extension = '.tif'\n",
    "        two_raster_list=[]\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name.split('_')[1].upper() \n",
    "                    ssp = file_name.split('_')[0].upper() \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==f\"{input_ssp}\" and Year==f\"{start_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = out_rst #f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Difference in Population in {Year} under {ssp} from 2050\"\n",
    "                        legend_title=\"Population (x1000)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        \n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "                    elif (ssp==f\"{input_ssp}\" ) and (  Year==f\"{end_year}\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}_1.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 =out_rst # f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        # map_title= f\"Population in {Year} under {ssp}\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}_1.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "        # for delta in len(two_raster_list):\n",
    "        diff_out_path = f'{rasters}/projected_clipped_difference_{subdir}_{file_name}.tif'\n",
    "        base_year_raster=two_raster_list[0]\n",
    "        end_year_raster=two_raster_list[1]\n",
    "        print(f\"base_year_raster:: {base_year_raster}-----end_year_raster::{end_year_raster}\")\n",
    "        diff_out_path=difference_rasters(base_year_raster, end_year_raster, diff_out_path)\n",
    "                                \n",
    "        map_title= f\"%Urban Land change during {start_year}-{end_year}({input_ssp})\"\n",
    "        legend_title=\"%age Urban Land\"\n",
    "\n",
    "        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}_diff.shp'\n",
    "        output_geojson= f'{shapefiles}/geojson_{subdir}_diff.geojson'\n",
    "        output_csv= f'{tables}/csv_{subdir}_{file_name}_diff.csv'\n",
    "        driver= 'ESRI Shapefile'\n",
    "        mask_value=None\n",
    "        raster_val= subdir[0:9]\n",
    "        name = polygonize_with_negatives(raster_file=diff_out_path ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                            driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_difference_{subdir}_{file_name}.shp'\n",
    "        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                    output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "        \n",
    "        # Function for getting quantile ranges. \n",
    "        polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 ,duplicates='drop')\n",
    "        a = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.right\n",
    "        b = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.left\n",
    "        binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "        \n",
    "        map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles_diff_{input_ssp}_{start_year}_{end_year}.png\"\n",
    "        quantile_range= \"quantile_range\"\n",
    "        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                title=map_title, map_output=map_output)\n",
    "       \n",
    "        return polygonized_shapefile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                Hourly anthropogenic heat flux\n",
    "\n",
    "Under this assumption, annual-average anthropogenic heat of the country (Qfy) is assumed to be equal to the sum of the primary energy consumption and anthropogenic heat from metabolic processes (QM). Primary energy consumption of the country can be further partitioned to components such as heat loss (QL), and AHE of industrial and agricultural (QIA), and commercial, residential, and transport sectors (QM) (Eq. 1).\n",
    "\n",
    "Qfy=QL+QCRT+QIA+QM\n",
    "\n",
    "Data available: temporal:2050 only, Scenarios: SSP 3 only. Both Hourly, monthly and yearly averages are available here: https://urbanclimate.tse.ens.titech.ac.jp/database/AHE/AH4GUC/future/\n",
    "Hourly data is avaiable for 24 hours of each month. Total hourly tiffs: 24*12*1 (data is available for 2010 and 2050)\n",
    "\n",
    "We show maps for the 12 months for year 2050.  We can produce these for each hour of a given month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatflux\n",
    "def export_heatflux():\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # AHE_2010_01_average\n",
    "                    Year = file_name.split('_')[1].upper() \n",
    "                    Month = file_name.split('_')[2].upper() \n",
    "                    ssp = \"SSP3\" #Only fo this SSP we have AHE  \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    df['Month'] = Month\n",
    "                    # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "                    combined_results.append(df)\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if  (Year==\"2050\" and Month==\"YEAR\" ):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Annualized average Urban Heat Flux \\n in Year:{Year} under {ssp}\"\n",
    "                        legend_title=\"UHF(W/sq.m)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs,  unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(100).round(1)\n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                    if  (Year==\"2050\" and Month!=\"YEAR\" ):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Urban Heat Flux during \\n Month-Year:{Month}-{Year} under {ssp}\"\n",
    "                        legend_title=\"UHF(W/sq.m)\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "                        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(100).round(1)\n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                                title=map_title, map_output=map_output)\n",
    "\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "        # Wide\n",
    "        # print(df)\n",
    "        [stats_to_get] = stats_to_get\n",
    "        # df_tranformed= df.pivot(index=['Scenario'], columns='Year', values=stats_to_get)\n",
    "        df_tranformed= df.pivot_table(index=['Scenario'] , columns='Year', values=stats_to_get, aggfunc=stats_to_get)        \n",
    "        # Long\n",
    "        df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "        # export csv of wide\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "        merge_JsonFiles_=merge_JsonFiles(dir=shapefiles, section=subdir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_heatflux_change(input_ssp, start_year, end_year):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        extension = '.tif'\n",
    "        two_raster_list=[]\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name.split('_')[1].upper() \n",
    "                    Month = file_name.split('_')[2].upper() \n",
    "                    ssp = \"SSP3\" #Only fo this SSP we have AHE  \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    df['Month'] = Month\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "\n",
    "                    if (ssp==f\"{input_ssp}\" and Year==f\"{start_year}\" and Month==\"YEAR\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = out_rst #f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        \n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "                    elif (ssp==f\"{input_ssp}\" ) and (  Year==f\"{end_year}\" and Month==\"YEAR\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}_1.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 =out_rst # f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        # map_title= f\"Population in {Year} under {ssp}\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}_1.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        two_raster_list.append(projected_raster)\n",
    "\n",
    "        # for delta in len(two_raster_list):\n",
    "        diff_out_path = f'{rasters}/projected_clipped_difference_{subdir}_{file_name}.tif'\n",
    "        base_year_raster=two_raster_list[0]\n",
    "        end_year_raster=two_raster_list[1]\n",
    "        print(f\"base_year_raster:: {base_year_raster}-----end_year_raster::{end_year_raster}\")\n",
    "        diff_out_path=difference_rasters_float_32(base_year_raster, end_year_raster, diff_out_path)\n",
    "                                \n",
    "        map_title= f\"Change in average Urban Heat Flux(annual) \\n  during {start_year}-{end_year}({input_ssp})\"\n",
    "        legend_title=\"UHF(W/sq.m)\"\n",
    "        \n",
    "        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}_diff.shp'\n",
    "        output_geojson= f'{shapefiles}/geojson_{subdir}_diff.geojson'\n",
    "        output_csv= f'{tables}/csv_{subdir}_{file_name}_diff.csv'\n",
    "        driver= 'ESRI Shapefile'\n",
    "        mask_value=None\n",
    "        raster_val= subdir[0:9]\n",
    "        name = polygonize_with_negatives(raster_file=diff_out_path ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                            driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_difference_{subdir}_{file_name}.shp'\n",
    "        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                    output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "        polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].div(100).round(1)\n",
    "\n",
    "        # Function for getting quantile ranges. \n",
    "        polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 ,duplicates='drop')\n",
    "        a = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.right\n",
    "        b = pd.qcut(polygonized_shapefile[raster_val], 5,duplicates='drop').cat.categories.left\n",
    "        binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "        \n",
    "        map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles_diff_{input_ssp}_{start_year}_{end_year}.png\"\n",
    "        quantile_range= \"quantile_range\"\n",
    "        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                title=map_title, map_output=map_output)\n",
    "       \n",
    "        return polygonized_shapefile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UHI intensification\n",
    "Snippet from the paper: Our results show that urban land expansion through 2050 will lead to warming of surface air temperature (hereafter referred to as 'urban expansion-warming'), which is about 40%70% as strong as that caused by GHG emissions (hereafter referred to as 'GHG emissions-warming'; figure 2 and table 1). During summer, urban expansion will increase daily minimum and maximum temperatures by 0.5C0.7 C on average (up to 3 C in some locations), which is about 39%64% (up to >200%) as strong as the GHG emissions-induced warming (multi-model ensemble average in the IPCC RCP 4.5 scenario). During winter, the urban expansion-warming will be weaker: on average 0.4 C0.6 C (up to 2 C), about 43%65% (up to 170%) as strong as the GHG emissions-warming. Despite varying magnitudes of urban expansion, intensities of the subsequent warming are relatively similar across scenarios (table 1). \n",
    "\n",
    "Data available: temporal:2050 only, Scenarios: All SSPs and Day and Night Degrees Centigrade changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_urbanheatisland():\n",
    "    hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    # print(file_name,  file_name_path, subdir)\n",
    "                    Year = file_name.split('_')[1].title() \n",
    "                    Year=Year.replace(\"Nig\", \"Night\") \n",
    "                    ssp = file_name.split('_')[0].upper()\n",
    "                    ssp = ssp.split('-')[1].upper() \n",
    "                    season = file_name.split('_')[2].title() \n",
    "                    season=season.replace(\"Win\", \"Winter\") \n",
    "                    season=season.replace(\"Sum\", \"Summer\") \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    raster = rasterio.open(rst)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    # print(f\"season:{season} ssp:{ssp} Year or day and night:{Year}\")\n",
    "                    combined_results.append(df)\n",
    "                    #clip and export rasters for the pertinent ssp and year combo\n",
    "                    if (ssp==\"SSP2\" or ssp==\"SSP5\") and (Year==\"Day\" or Year==\"Night\"):\n",
    "                        out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                        clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                        out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "                        map_title= f\"Heat Island Intensification \\n during {season} {Year} under {ssp}\"\n",
    "                        legend_title=\"HIU\"\n",
    "                        [statistic]= stats_to_get\n",
    "                        hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "                            hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "                                raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "                        crs = 4326\n",
    "                        projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                        unprojected_raster = out_rst_2\n",
    "                        reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                           projected_raster=projected_raster)\n",
    "                        # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "                        # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "                        map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                        document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                        ###export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                        output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                        output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                        output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                        driver= 'ESRI Shapefile'\n",
    "                        mask_value=None\n",
    "                        raster_val= subdir[0:9]\n",
    "                        name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                           driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                        clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                        clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                   output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                    clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                        polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "                        export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, \\\n",
    "                                    crs=crs, cmap=cmap , visualize_column=raster_val, \\\n",
    "                                        title=map_title, map_output=map_output)\n",
    "                        export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "                                                           hexagons=polygonized_shapefile,legend_title=legend_title, \\\n",
    "                                                            legends_format=legends_format, crs=crs, cmap=cmap , \\\n",
    "                                                                visualize_column=raster_val,  \\\n",
    "                                                                    title=map_title, map_output=map_output)\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "        # Wide\n",
    "        [stats_to_get] = stats_to_get\n",
    "        df_tranformed= df.pivot_table(index=['Scenario'] , columns='Year', values=stats_to_get, aggfunc=stats_to_get)        \n",
    "        # Long\n",
    "        df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "        # export csv of wide\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fathom Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fast_scandir(dir):    # dir: str, ext: list\n",
    "    subfolders=[]\n",
    "    for f in os.scandir(dir):\n",
    "        if f.is_dir():\n",
    "            subfolders.append(f.path)\n",
    "    return subfolders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fathom_mosaic(section):\n",
    "    subdir=section[0]\n",
    "    dir=Path(os.path.join(data, f\"{subdir}\"))\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        rasters_for_mosaic = defaultdict(list)\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    file_name_clean = file_name.split(\"_\")[:-1]\n",
    "                    file_name_clean = \"_\".join(file_name_clean)\n",
    "                    rasters_for_mosaic[file_name_clean].append(file_name_path)\n",
    "\n",
    "    output_dir=Path(os.path.join(data, f\"{subdir}/clean\"))\n",
    "    if os.path.exists(output_dir):\n",
    "        try:\n",
    "            shutil.rmtree(output_dir, ignore_errors=True)\n",
    "            print(f\"deleted {output_dir}\")\n",
    "        except:\n",
    "            print(f\"Couldnt delete {output_dir}\")\n",
    "            pass\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    for key, value in rasters_for_mosaic.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        raster_to_mosiac=[]\n",
    "        for p in value:\n",
    "            raster = rio.open(p)\n",
    "            raster_to_mosiac.append(raster)\n",
    "            # print(f\"input_dir {key} has raster>>>>{p}\")\n",
    "        \n",
    "        # output_path= Path(os.path.join(output_dir)\n",
    "        # if not os.path.exists(output_path):\n",
    "        #     os.mkdir(output_path)\n",
    "        output_path= os.path.join(output_dir, f\"{key}.tif\")\n",
    "        print(f\" file_name-->> {file_name}-raster_to_mosiac : {raster_to_mosiac}----output_path {output_path}\")\n",
    "        mosaic, output = merge(raster_to_mosiac)\n",
    "\n",
    "        # copy raster's metadata and update it to match the height and width of the mosaic.\n",
    "        output_meta = raster.meta.copy()\n",
    "        output_meta.update(\n",
    "            {\"driver\": \"GTiff\",\n",
    "                \"height\": mosaic.shape[1],\n",
    "                \"width\": mosaic.shape[2],\n",
    "                \"transform\": output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # write the mosaiced file in the folder.\n",
    "        with rio.open(output_path, 'w', **output_meta) as m:\n",
    "            m.write(mosaic)\n",
    "\n",
    "    return rasters_for_mosaic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_fathom(input_dir, section,shp, stat):\n",
    "    vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "    for subdir in section:\n",
    "        combined_results = []\n",
    "        extension = '.tif'\n",
    "        for root, dirs_list, files_list in os.walk(input_dir):\n",
    "            for file_name in files_list:\n",
    "                if os.path.splitext(file_name)[-1] == extension:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    print(file_name,  file_name_path, subdir)\n",
    "                    file_name_clean=file_name.replace(\"-\", \" \")\n",
    "                    file_name_clean=file_name_clean.split(\" \") #[0:3]\n",
    "\n",
    "                    return_period=file_name_clean[0]\n",
    "                    return_period=return_period.split(\"in\")[1]\n",
    "                    # print(file_name_clean, return_period)\n",
    "                    flood_type= file_name_clean[1].title()\n",
    "                    print(file_name_clean, return_period, flood_type)\n",
    "\n",
    "                    var= file_name_clean[1]\n",
    "                    Year= file_name_clean[3]\n",
    "                    # scenario=file_name_clean[4]\n",
    "                    # list_length=len(file_name_clean)\n",
    "                    # if \"SSP\" in scenario:\n",
    "                    if len(file_name_clean)>4:\n",
    "                        # scenario=file_name_clean[4]\n",
    "                        ssp=file_name_clean[4]\n",
    "                        # ssp=ssp.replace(\"_\", \"-\")\n",
    "                    else:\n",
    "                        ssp=\"Present\"\n",
    "\n",
    "                    title= f\"{flood_type} flood in {Year} with a \\n return period of {return_period} years ({ssp})\"\n",
    "                    # print(f\"file_name  for legends----------------{file_name}---clean{file_name_clean}\")\n",
    "                    print(f\"title->>>>>> {title}\")\n",
    "\n",
    "                            \n",
    "                    file_name_projected= f'{file_name}_wsg84.tif'\n",
    "                    file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "                    output_raster=os.path.join(root, file_name_projected) \n",
    "                    rst = file_name_path \n",
    "                    vector = load_shapefile_buffer(shp)\n",
    "                    out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "                    clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "                    \n",
    "                    raster = rasterio.open(rst)\n",
    "                    p_vector = reproject_gpdf(vector, raster)\n",
    "                    # vector = load_shapefile(shp)\n",
    "                    stats_to_get = list_statistics(stat)\n",
    "                    df = calculate_zonal_stats(p_vector, out_rst, stats_to_get)\n",
    "                    df['Scenario'] = ssp\n",
    "                    df['Year'] = Year\n",
    "                    df['Flood type'] = flood_type\n",
    "                    # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "                    combined_results.append(df)\n",
    "                    print(df.head())\n",
    "        \n",
    "                    map_title= f\"{title}\"\n",
    "                    legend_title= f\"Flood depth(cm)\"\n",
    "                    print(f\"map_title {map_title}----legend_title-->>>{legend_title}\")\n",
    "                    crs = 4326\n",
    "                    projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "                    unprojected_raster = out_rst\n",
    "                    reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "                                                        projected_raster=projected_raster)\n",
    "                    \n",
    "                    map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "                    document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "                    #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "                    output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "                    output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "                    output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "                    driver= 'ESRI Shapefile'\n",
    "                    mask_value=None\n",
    "                    raster_val= subdir[0:9]\n",
    "                    name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "                                        driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "                    clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "                    clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "                                                                output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "                                                                clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "                    polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84)\n",
    "                    # Function for getting quantile ranges. \n",
    "                    polygonized_shapefile[\"quantile_range\"] = pd.qcut(polygonized_shapefile[raster_val],5 )\n",
    "                    a = pd.qcut(polygonized_shapefile[raster_val], 5).cat.categories.right\n",
    "                    b = pd.qcut(polygonized_shapefile[raster_val], 5).cat.categories.left\n",
    "                    binned = pd.cut(polygonized_shapefile[raster_val], bins=b)\n",
    "                    \n",
    "                    # binned = pd.cut(polygonized_shapefile[raster_val], bins=bins, labels=labels)\n",
    "                    # print(f\"a:{a}---df1:{df1} df2:{df2} , bins: {bins}, binned: {binned}, binned_df {binned_df}\" )\n",
    "                    legends_format= '{:,.0f}'\n",
    "                    cmap = \"OrRd\"\n",
    "                    map_output = f\"{maps}/{city}_{subdir}_{file_name}_quantiles.png\"\n",
    "                    quantile_range= \"quantile_range\"\n",
    "                    export_the_map= export_map_categorical(vector_file=vector_file , \\\n",
    "                            hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                            crs=crs, cmap=cmap , visualize_column=quantile_range,  \\\n",
    "                            title=map_title, map_output=map_output)\n",
    "\n",
    "                    # ------------------------\n",
    "                    export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "                            hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "                            crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "                            title=map_title, map_output=map_output)\n",
    "\n",
    "\n",
    "        # Append vertically and then pivot horizontally.\n",
    "        df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "        # Wide\n",
    "        df_tranformed= df # df[['Flood type', 'Year' ,'Scenario', stats_to_get]] #df.pivot(index=['Flood type', 'Year' ], columns=['Scenario'], values=stats_to_get)\n",
    "        print(f\"df_tranformed NOT: {df_tranformed}\")\n",
    "\n",
    "        df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "        merge_JsonFiles_=merge_JsonFiles(dir=shapefiles, section=subdir)\n",
    "        return clipped_output_shapefile , df_tranformed \n",
    "\n",
    "def fathom_barchart(df,subtitle, y_var):\n",
    "    df['scenario_year']=df['Scenario']+ \" (\"+df['Year']+\")\"\n",
    "    # Draw a nested barplot by flood-type and ssp\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x='Flood type', y=y_var, hue='scenario_year',\n",
    "        palette=\"dark\", alpha=.6, height=6\n",
    "    )\n",
    "    g.fig.suptitle(f\"Flood Projections {subtitle}\")\n",
    "    # title\n",
    "    new_title = 'Scenario'\n",
    "    g._legend.set_title(new_title)\n",
    "    # sns.move_legend(g, \"upper right\", bbox_to_anchor=(.8, 1.02), frameon=False)\n",
    "    g.set_axis_labels(\"Flood type\", \"Flood depth(cm)\")\n",
    "    g.figure.savefig(f\"{maps}/{y_var}_fathom_flood_barchart.jpeg\", dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1in10-COASTAL-DEFENDED-2030-SSP1 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in10-COASTAL-DEFENDED-2030-SSP1.tif fathom\n",
      "['1in10', 'COASTAL', 'DEFENDED', '2030', 'SSP1'] 10 Coastal\n",
      "title->>>>>> Coastal flood in 2030 with a \n",
      " return period of 10 years (SSP1)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  0.65            0.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP1  2030    Coastal  \n",
      "map_title Coastal flood in 2030 with a \n",
      " return period of 10 years (SSP1)----legend_title-->>>Flood depth(cm)\n",
      "1in10-COASTAL-DEFENDED-2030-SSP2 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in10-COASTAL-DEFENDED-2030-SSP2.tif fathom\n",
      "['1in10', 'COASTAL', 'DEFENDED', '2030', 'SSP2'] 10 Coastal\n",
      "title->>>>>> Coastal flood in 2030 with a \n",
      " return period of 10 years (SSP2)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  0.65            0.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP2  2030    Coastal  \n",
      "map_title Coastal flood in 2030 with a \n",
      " return period of 10 years (SSP2)----legend_title-->>>Flood depth(cm)\n",
      "1in10-FLUVIAL-DEFENDED-2020 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in10-FLUVIAL-DEFENDED-2020.tif fathom\n",
      "['1in10', 'FLUVIAL', 'DEFENDED', '2020'] 10 Fluvial\n",
      "title->>>>>> Fluvial flood in 2020 with a \n",
      " return period of 10 years (Present)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  0.31            0.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0  Present  2020    Fluvial  \n",
      "map_title Fluvial flood in 2020 with a \n",
      " return period of 10 years (Present)----legend_title-->>>Flood depth(cm)\n",
      "1in10-FLUVIAL-DEFENDED-2030-SSP1 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in10-FLUVIAL-DEFENDED-2030-SSP1.tif fathom\n",
      "['1in10', 'FLUVIAL', 'DEFENDED', '2030', 'SSP1'] 10 Fluvial\n",
      "title->>>>>> Fluvial flood in 2030 with a \n",
      " return period of 10 years (SSP1)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  0.33            0.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP1  2030    Fluvial  \n",
      "map_title Fluvial flood in 2030 with a \n",
      " return period of 10 years (SSP1)----legend_title-->>>Flood depth(cm)\n",
      "1in10-FLUVIAL-DEFENDED-2030-SSP2 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in10-FLUVIAL-DEFENDED-2030-SSP2.tif fathom\n",
      "['1in10', 'FLUVIAL', 'DEFENDED', '2030', 'SSP2'] 10 Fluvial\n",
      "title->>>>>> Fluvial flood in 2030 with a \n",
      " return period of 10 years (SSP2)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  0.34            0.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP2  2030    Fluvial  \n",
      "map_title Fluvial flood in 2030 with a \n",
      " return period of 10 years (SSP2)----legend_title-->>>Flood depth(cm)\n",
      "1in100-COASTAL-DEFENDED-2020 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in100-COASTAL-DEFENDED-2020.tif fathom\n",
      "['1in100', 'COASTAL', 'DEFENDED', '2020'] 100 Coastal\n",
      "title->>>>>> Coastal flood in 2020 with a \n",
      " return period of 100 years (Present)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  7.12           52.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0  Present  2020    Coastal  \n",
      "map_title Coastal flood in 2020 with a \n",
      " return period of 100 years (Present)----legend_title-->>>Flood depth(cm)\n",
      "1in100-COASTAL-DEFENDED-2030-SSP1 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in100-COASTAL-DEFENDED-2030-SSP1.tif fathom\n",
      "['1in100', 'COASTAL', 'DEFENDED', '2030', 'SSP1'] 100 Coastal\n",
      "title->>>>>> Coastal flood in 2030 with a \n",
      " return period of 100 years (SSP1)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  7.58           56.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP1  2030    Coastal  \n",
      "map_title Coastal flood in 2030 with a \n",
      " return period of 100 years (SSP1)----legend_title-->>>Flood depth(cm)\n",
      "1in100-COASTAL-DEFENDED-2030-SSP2 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in100-COASTAL-DEFENDED-2030-SSP2.tif fathom\n",
      "['1in100', 'COASTAL', 'DEFENDED', '2030', 'SSP2'] 100 Coastal\n",
      "title->>>>>> Coastal flood in 2030 with a \n",
      " return period of 100 years (SSP2)\n",
      "                                            geometry  mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  7.63           56.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0     SSP2  2030    Coastal  \n",
      "map_title Coastal flood in 2030 with a \n",
      " return period of 100 years (SSP2)----legend_title-->>>Flood depth(cm)\n",
      "1in100-FLUVIAL-DEFENDED-2020 C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\clean\\1in100-FLUVIAL-DEFENDED-2020.tif fathom\n",
      "['1in100', 'FLUVIAL', 'DEFENDED', '2020'] 100 Fluvial\n",
      "title->>>>>> Fluvial flood in 2020 with a \n",
      " return period of 100 years (Present)\n",
      "                                            geometry   mean  percentile_95  \\\n",
      "0  POLYGON ((91.73066 22.35957, 91.73024 22.36080...  12.72           90.0   \n",
      "\n",
      "  Scenario  Year Flood type  \n",
      "0  Present  2020    Fluvial  \n",
      "map_title Fluvial flood in 2020 with a \n",
      " return period of 100 years (Present)----legend_title-->>>Flood depth(cm)\n"
     ]
    }
   ],
   "source": [
    "stat= [ 'mean','percentile_95']\n",
    "section= ['fathom']\n",
    "\n",
    "# rasters_for_mosaic=create_fathom_mosaic(section=section)\n",
    "input_dir=PureWindowsPath(os.path.join(data, f\"{section[0]}/clean\"))\n",
    "clipped_output_shapefile , df_tranformed =export_fathom(input_dir, section,shp, stat)\n",
    "\n",
    "# titles= [\"(Average)\", \"(95 Percentile)\"]\n",
    "# for s, t in zip(stat,titles):  \n",
    "#     fathom_barchart(df=df_tranformed, subtitle= t, y_var=s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_map_fathom_overlays(vector_file,  layer_1, layer,legend_title,legends_format,legends_format_1,legend_label,legend_label_1, crs ,cmap,cmap_1,  visualize_column,visualize_column_1, title, map_output):\n",
    "   #  fig = plt.figure(figsize=(width, height))\n",
    "   height=7\n",
    "   width=8\n",
    "   fig, ax = plt.subplots(figsize=(width, height))\n",
    "   #  ax = fig.add_subplot(111)\n",
    "   ax = layer_1.to_crs(crs).plot(ax=ax, \n",
    "                                 column= visualize_column_1, \\\n",
    "                                 linewidth=0 , \n",
    "                                 alpha=0.6, \n",
    "                              #    scheme=\"quantiles\", \n",
    "                                 cmap=cmap_1, \n",
    "                                 label=f\"{legend_label_1}\" ,\n",
    "                                       )\n",
    "\n",
    "   ax = layer.to_crs(crs).plot(ax=ax, \n",
    "                                 column= visualize_column, \n",
    "                                 alpha=0, \n",
    "                                 facecolor='none',\n",
    "                                 scheme=\"quantiles\", \n",
    "                                 cmap=cmap,\n",
    "                                 label=f\"{legend_label}\" ,\n",
    "                                 legend=False \n",
    "                                       )\n",
    "   layer_filtered=layer[layer[visualize_column]>0]\n",
    "   ax = layer_filtered.to_crs(crs).plot(ax=ax, \n",
    "                                 column= visualize_column, \n",
    "                                 alpha=.6, \n",
    "                              #    hatch=\"/\",\n",
    "                                 scheme=\"quantiles\", \n",
    "                                 cmap=cmap,\n",
    "                                 label=f\"{legend_label}\" ,\n",
    "                                 legend=False \n",
    "                                       )\n",
    "\n",
    "   vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "   \n",
    "   cx.add_basemap(ax, \n",
    "               source=cx.providers.Esri.WorldImagery,  \n",
    "               crs=crs,\n",
    "               attribution=False, #No citation\n",
    "               ) \n",
    "\n",
    "   visualize_column_1=visualize_column_1 #\"popdynami\"\n",
    "   layer_1[\"quantile_range\"] = pd.qcut(layer_1[visualize_column_1],5 )\n",
    "   a = pd.qcut(layer_1[visualize_column_1], 5).cat.categories.right\n",
    "   b = pd.qcut(layer_1[visualize_column_1], 5).cat.categories.left\n",
    "   c=[str(int(m))+ \"-\" +str(int(n)) for m,n in zip(b ,a)]\n",
    "\n",
    "   cmap_1 = cm.get_cmap(cmap_1)\n",
    "   # title_patch = mpatches.Patch(color='none',label=f'{legend_label_1}')\n",
    "   # title_patch = mpatches.Patch(label=f'{legend_label_1}')\n",
    "   title_patch = Line2D([0], [0], marker='o', color='none', label=f'{legend_label_1}', markerfacecolor='none', markersize=1)\n",
    "   white_patch = mpatches.Patch(color=cmap_1(0.0), label=c[0])\n",
    "   lowblue_patch = mpatches.Patch(color=cmap_1(0.25), label=c[1])\n",
    "   midblue_patch = mpatches.Patch(color=cmap_1(0.5), label=c[2])\n",
    "   highblue_patch = mpatches.Patch(color=cmap_1(0.75), label=c[3])\n",
    "   veryblue_patch = mpatches.Patch(color=cmap_1(1.0), label=c[4])\n",
    "   space_patch = mpatches.Patch(color='none',label='')\n",
    "   \n",
    "   layer[\"quantile_range\"] = pd.qcut(layer[visualize_column],5 )\n",
    "   a = pd.qcut(layer[visualize_column], 5).cat.categories.right\n",
    "   b = pd.qcut(layer[visualize_column], 5).cat.categories.left\n",
    "   c=[str(int(m))+ \"-\" +str(int(n)) for m,n in zip(b ,a)]\n",
    "\n",
    "   cmap = cm.get_cmap(cmap)\n",
    "   title_patch_1 = mpatches.Patch(color='none',label=f'{legend_label}')\n",
    "   white_patch_1 = mpatches.Patch(color=cmap(0.0), label=c[0])\n",
    "   lowblue_patch_1 = mpatches.Patch(color=cmap(0.25), label=c[1])\n",
    "   midblue_patch_1 = mpatches.Patch(color=cmap(0.5), label=c[2])\n",
    "   highblue_patch_1 = mpatches.Patch(color=cmap(0.75), label=c[3])\n",
    "   veryblue_patch_1 = mpatches.Patch(color=cmap(1.0), label=c[4])\n",
    "\n",
    "   ax2 = fig.add_axes(rect=[.73, 0.02,0.22, .88]) #frame_on=False #dimensions [left, bottom, width, height] of the new Axes\n",
    "   ax2.legend(handles=[ title_patch, white_patch, lowblue_patch, midblue_patch, highblue_patch, veryblue_patch,\n",
    "                     space_patch, title_patch_1,white_patch_1,lowblue_patch_1, midblue_patch_1, highblue_patch_1, veryblue_patch_1\n",
    "                     ], loc='upper right', bbox_to_anchor=(1, 1), fontsize='small',title=f'{legend_title}',labelspacing=0.5, handleheight=2, handlelength=3)\n",
    "\n",
    "   plt.xticks([])\n",
    "   plt.yticks([])\n",
    "   # ax.title.set_text(f'{title}')\n",
    "   ax.set_title(title, fontsize=10, fontweight='bold', wrap=True)\n",
    "   # ax.axis('tight') #zooms in the to the largest features bounds\n",
    "   ax.axis('off')\n",
    "   ax2.axis('off')\n",
    "   plt.tight_layout()\n",
    "   ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_fathom_on_fathom_plus_infrastructure(vector_file, legend_title, crs ,  visualize_column ):\n",
    "   #  fig = plt.figure(figsize=(width, height))\n",
    "   height=7\n",
    "   width=8\n",
    "   fig, ax = plt.subplots(figsize=(width, height))\n",
    "   # create flood list\n",
    "   flood_layers_dict = defaultdict(list)\n",
    "   file_list = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_fathom_1*.shp\"))\n",
    "   for file_path in file_list:\n",
    "      file_name= os.path.basename(file_path)\n",
    "      file_name = os.path.splitext(file_name)[0]\n",
    "      # print(file_name)\n",
    "      file_name_clean=file_name.replace(\"-\", \" \")\n",
    "      file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "      return_period=file_name_clean[0]\n",
    "      return_period=return_period.split(\"in\")[-1]\n",
    "      flood_type= file_name_clean[1].title()\n",
    "      var= file_name_clean[2]\n",
    "      Year= file_name_clean[4]\n",
    "      scenario=file_name_clean[5]\n",
    "      if \"SSP\" in scenario:\n",
    "         ssp=scenario\n",
    "         ssp=ssp.replace(\"_\", \"-\")\n",
    "      else:\n",
    "         ssp=\"Present\"\n",
    "      # flood_layers_dict[Year].append(file_name)\n",
    "      flood_layers_dict[Year].append(file_path)\n",
    "      \n",
    "   res = [[i for i in flood_layers_dict[x]] for x in flood_layers_dict.keys()] \n",
    "   # cmap_list=list(colormaps)\n",
    "   cmap_list=['RdPu','afmhot_r','Blues']\n",
    "   for i in res:\n",
    "      print( i)\n",
    "      handles_list=[]\n",
    "      for cmap, j in enumerate(i):\n",
    "         # for file_path in file_list:\n",
    "         file_name= os.path.basename(j)\n",
    "         file_name = os.path.splitext(file_name)[0]\n",
    "         file_name_clean=file_name.replace(\"-\", \" \")\n",
    "         file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "         return_period=file_name_clean[0]\n",
    "         return_period=return_period.split(\"in\")[-1]\n",
    "         flood_type= file_name_clean[1].title()\n",
    "         var= file_name_clean[2]\n",
    "         Year= file_name_clean[4]\n",
    "         scenario=file_name_clean[5]\n",
    "         if \"SSP\" in scenario:\n",
    "               ssp=scenario\n",
    "               ssp=ssp.replace(\"_\", \"-\")\n",
    "         else:\n",
    "               ssp=\"Present\"\n",
    "\n",
    "         print( j, cmap_list[cmap], Year,ssp , flood_type)\n",
    "         cmap=cmap_list[cmap]\n",
    "         layer= gpd.read_file(Path(j)).to_crs(WGS84)\n",
    "         ax = layer.to_crs(crs).plot(ax=ax, \n",
    "                                       column= visualize_column, \n",
    "                                       alpha=0, \n",
    "                                       facecolor='none',\n",
    "                                       scheme=\"quantiles\", \n",
    "                                       cmap=cmap,\n",
    "                                       label=f\"{flood_type}\" ,\n",
    "                                       legend=False \n",
    "                                             )\n",
    "         \n",
    "         layer_filtered=layer[layer[visualize_column]>0]\n",
    "         ax = layer_filtered.to_crs(crs).plot(ax=ax, \n",
    "                                       column= visualize_column, \n",
    "                                       alpha=.6, \n",
    "                                    #    hatch=\"/\",\n",
    "                                       scheme=\"quantiles\", \n",
    "                                       cmap=cmap,\n",
    "                                       label=f\"{flood_type}\" ,\n",
    "                                       legend=False \n",
    "                                             )\n",
    "      \n",
    "         layer[\"quantile_range\"] = pd.qcut(layer[visualize_column],5 )\n",
    "         a = pd.qcut(layer[visualize_column], 5).cat.categories.right\n",
    "         b = pd.qcut(layer[visualize_column], 5).cat.categories.left\n",
    "         c=[str(int(m))+ \"-\" +str(int(n)) for m,n in zip(b ,a)]\n",
    "         \n",
    "         \n",
    "\n",
    "         legend_label=f\"{flood_type} flood(cm)\"  #f\"Flood\"\n",
    "         legend_label_patch = Line2D([0], [0], marker='o', color='none', label=f'{legend_label}', markerfacecolor='none', markersize=1)\n",
    "         handles_list.append(legend_label_patch)\n",
    "         cmap = cm.get_cmap(cmap)\n",
    "         for num, increment in enumerate(np.arange(0, 1.25, 0.25)):\n",
    "            print(num, increment)\n",
    "            patch = mpatches.Patch(color=cmap(increment), label=c[num])\n",
    "            handles_list.append(patch)\n",
    "      \n",
    "      ax=vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "\n",
    "      cx.add_basemap(ax, \n",
    "                  source=cx.providers.Esri.WorldImagery,  \n",
    "                  crs=crs,\n",
    "                  attribution=False, #No citation\n",
    "                  ) \n",
    "\n",
    "      # -----\n",
    "      index=0\n",
    "      polygon = vector_file.reset_index().iloc[index]['geometry']\n",
    "      tags={'amenity':['college', 'dancing_school', 'driving_school', 'kindergarten', \\\n",
    "                     'language_school', 'library', 'surf_school', 'toy_library', 'research_institute', \\\n",
    "                     'training', 'music_school', 'school', 'university'],\n",
    "                     'landuse': 'education'\n",
    "                     }\n",
    "      education=ox.features.features_from_polygon(polygon, tags)\n",
    "      # tags={'leisure':['park', 'nature_reserve', 'protected_area', 'garden']}\n",
    "      tags={'leisure':['park',  'garden']}\n",
    "      park=ox.features.features_from_polygon(polygon, tags)\n",
    "      park.geometry = park['geometry'].centroid\n",
    "      tags={'landuse':['commercial', 'industrial', 'construction', 'retail']                   }\n",
    "      industry=ox.features.features_from_polygon(polygon, tags)\n",
    "      industry.geometry = industry['geometry'].centroid\n",
    "      tags={'building': 'government'}\n",
    "      # government_building=ox.features.features_from_polygon(polygon, tags)\n",
    "      # government_building.geometry = government_building['geometry'].centroid\n",
    "      tags = {\"building\": True}\n",
    "      building_footprint=ox.features.features_from_polygon(polygon, tags)\n",
    "      building_footprint.geometry = building_footprint['geometry'].centroid\n",
    "      tags = {'amenity':['clinic', 'doctors', 'dentist', 'health_post', 'hospital', 'nursing_home', 'pharmacy', 'veterinary']}\n",
    "      health_care=ox.features.features_from_polygon(polygon, tags)\n",
    "      health_care.geometry = health_care['geometry'].centroid\n",
    "      G = ox.graph.graph_from_polygon(polygon, network_type='drive')\n",
    "      # add travel time based on maximum speed\n",
    "      G = ox.add_edge_speeds(G) \n",
    "      G = ox.add_edge_travel_times(G) \n",
    "      G = ox.projection.project_graph(G, to_crs=3857)\n",
    "      # get edges as Geo data frame\n",
    "      _, gdf_edges = ox.graph_to_gdfs(G)\n",
    "\n",
    "      ax = education.to_crs(crs).plot(ax=ax,\n",
    "                                       alpha=0.6,  \n",
    "                                       facecolor='olive',\n",
    "                                       edgecolor='olive' , \n",
    "                                       label=\"Education\",\n",
    "                                       linewidth=3,\n",
    "                                       markersize=10,\n",
    "                                       marker=\"P\"  \n",
    "                                       )\n",
    "      \n",
    "      ax = industry.to_crs(crs).plot(ax=ax,\n",
    "                                       alpha=0.6,  \n",
    "                                       facecolor='cyan', \n",
    "                                       edgecolor='cyan' , \n",
    "                                       label=\"Industries\",\n",
    "                                       linewidth=3,\n",
    "                                       markersize=10,\n",
    "                                       marker=\"p\"  \n",
    "                                       )\n",
    "      \n",
    "      ax = park.to_crs(crs).plot(ax=ax,\n",
    "                                       alpha=0.6,  \n",
    "                                       facecolor='peru',\n",
    "                                       edgecolor='peru' , \n",
    "                                       label=\"Parks\",\n",
    "                                       linewidth=3,\n",
    "                                       markersize=10,\n",
    "                                       marker=\"*\"   \n",
    "                                       )\n",
    "      ax = health_care.to_crs(crs).plot(ax=ax,\n",
    "                                       alpha=0.6,  \n",
    "                                       facecolor='red',\n",
    "                                       edgecolor='red' , \n",
    "                                       label=\"Healthcare\",\n",
    "                                       linewidth=3,\n",
    "                                       markersize=10,\n",
    "                                       marker=\"o\"   \n",
    "                                       )\n",
    "\n",
    "      ax = gdf_edges.to_crs(crs).plot(ax=ax,\n",
    "                                       alpha=0.4,  \n",
    "                                       facecolor='none',\n",
    "                                       edgecolor='black' , \n",
    "                                       label=\"Roads\",\n",
    "                                       linewidth=.3,\n",
    "                                       linestyle='dashed',\n",
    "                                       # zorder=1\n",
    "                                       )\n",
    "      lines = [Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"P\", markersize=10, color='olive' , label='Education'),\n",
    "               Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"p\", markersize=10, color='cyan' , label='Industries'),\n",
    "               Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"*\", markersize=10, color='peru' , label='Parks'),\n",
    "               Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"o\", markersize=10, color='red' , label='Healthcare'),\n",
    "               Line2D([0], [0], linestyle=\"dashed\",  fillstyle=None, marker=\"_\", markersize=10, color='black' , label='Roads')]\n",
    "      \n",
    "      space_patch = mpatches.Patch(color='none',label='')\n",
    "      handles_list.append(space_patch)\n",
    "      legend_label_infra=f\"Infrastucture\"  #f\"Flood\"\n",
    "      legend_label_patch = Line2D([0], [0], marker='o', color='none', label=f'{legend_label_infra}', markerfacecolor='none', markersize=1)\n",
    "      handles_list.append(legend_label_patch)\n",
    "      \n",
    "      for l in lines:\n",
    "         handles_list.append(l)\n",
    "      ax2 = fig.add_axes(rect=[.72, 0.025,0.22, .88]) #frame_on=False #dimensions [left, bottom, width, height] of the new Axes\n",
    "      ax2.legend(handles=handles_list, loc='upper right', bbox_to_anchor=(1, 1), fontsize='small',title=f'{legend_title}',labelspacing=0.5, handleheight=2, handlelength=3)\n",
    "\n",
    "\n",
    "      #------\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      # ax.title.set_text(f'{title}')\n",
    "      title= f\"Overlay of flood in {Year} with a \\n return period of {return_period} years ({ssp})\"\n",
    "      map_output = f\"{maps}/{city}_{Year}_{ssp}_{flood_type}_fathom_on_fathom_plus_infrastructure.png\"\n",
    "\n",
    "      ax.set_title(title, fontsize=10, fontweight='bold', wrap=True)\n",
    "      # ax.axis('tight') #zooms in the to the largest features bounds\n",
    "      ax.axis('off')\n",
    "      ax2.axis('off')\n",
    "      plt.tight_layout()\n",
    "      ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_fathom_on_fathom_overlays(vector_file, legend_title, crs ,  visualize_column ):\n",
    "   #  fig = plt.figure(figsize=(width, height))\n",
    "   height=7\n",
    "   width=8\n",
    "   fig, ax = plt.subplots(figsize=(width, height))\n",
    "   # create flood list\n",
    "   flood_layers_dict = defaultdict(list)\n",
    "   file_list = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_fathom_1*.shp\"))\n",
    "   for file_path in file_list:\n",
    "      file_name= os.path.basename(file_path)\n",
    "      file_name = os.path.splitext(file_name)[0]\n",
    "      # print(file_name)\n",
    "      file_name_clean=file_name.replace(\"-\", \" \")\n",
    "      file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "      return_period=file_name_clean[0]\n",
    "      return_period=return_period.split(\"in\")[-1]\n",
    "      flood_type= file_name_clean[1].title()\n",
    "      var= file_name_clean[2]\n",
    "      Year= file_name_clean[4]\n",
    "      scenario=file_name_clean[5]\n",
    "      if \"SSP\" in scenario:\n",
    "         ssp=scenario\n",
    "         ssp=ssp.replace(\"_\", \"-\")\n",
    "      else:\n",
    "         ssp=\"Present\"\n",
    "      # flood_layers_dict[Year].append(file_name)\n",
    "      flood_layers_dict[Year].append(file_path)\n",
    "      \n",
    "   res = [[i for i in flood_layers_dict[x]] for x in flood_layers_dict.keys()] \n",
    "   # cmap_list=list(colormaps)\n",
    "   cmap_list=['RdPu','afmhot_r','Blues']\n",
    "   for i in res:\n",
    "      print( i)\n",
    "      handles_list=[]\n",
    "      for cmap, j in enumerate(i):\n",
    "         # for file_path in file_list:\n",
    "         file_name= os.path.basename(j)\n",
    "         file_name = os.path.splitext(file_name)[0]\n",
    "         file_name_clean=file_name.replace(\"-\", \" \")\n",
    "         file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "         return_period=file_name_clean[0]\n",
    "         return_period=return_period.split(\"in\")[-1]\n",
    "         flood_type= file_name_clean[1].title()\n",
    "         var= file_name_clean[2]\n",
    "         Year= file_name_clean[4]\n",
    "         scenario=file_name_clean[5]\n",
    "         if \"SSP\" in scenario:\n",
    "               ssp=scenario\n",
    "               ssp=ssp.replace(\"_\", \"-\")\n",
    "         else:\n",
    "               ssp=\"Present\"\n",
    "\n",
    "         print( j, cmap_list[cmap], Year,ssp , flood_type)\n",
    "         cmap=cmap_list[cmap]\n",
    "         layer= gpd.read_file(Path(j)).to_crs(WGS84)\n",
    "         ax = layer.to_crs(crs).plot(ax=ax, \n",
    "                                       column= visualize_column, \n",
    "                                       alpha=0, \n",
    "                                       facecolor='none',\n",
    "                                       scheme=\"quantiles\", \n",
    "                                       cmap=cmap,\n",
    "                                       label=f\"{flood_type}\" ,\n",
    "                                       legend=False \n",
    "                                             )\n",
    "         \n",
    "         layer_filtered=layer[layer[visualize_column]>0]\n",
    "         ax = layer_filtered.to_crs(crs).plot(ax=ax, \n",
    "                                       column= visualize_column, \n",
    "                                       alpha=.6, \n",
    "                                    #    hatch=\"/\",\n",
    "                                       scheme=\"quantiles\", \n",
    "                                       cmap=cmap,\n",
    "                                       label=f\"{flood_type}\" ,\n",
    "                                       legend=False \n",
    "                                             )\n",
    "      \n",
    "         layer[\"quantile_range\"] = pd.qcut(layer[visualize_column],5 )\n",
    "         a = pd.qcut(layer[visualize_column], 5).cat.categories.right\n",
    "         b = pd.qcut(layer[visualize_column], 5).cat.categories.left\n",
    "         c=[str(int(m))+ \"-\" +str(int(n)) for m,n in zip(b ,a)]\n",
    "         \n",
    "         \n",
    "\n",
    "         legend_label=f\"{flood_type} flood(cm)\"  #f\"Flood\"\n",
    "         legend_label_patch = Line2D([0], [0], marker='o', color='none', label=f'{legend_label}', markerfacecolor='none', markersize=1)\n",
    "         handles_list.append(legend_label_patch)\n",
    "         cmap = cm.get_cmap(cmap)\n",
    "         for num, increment in enumerate(np.arange(0, 1.25, 0.25)):\n",
    "            print(num, increment)\n",
    "            patch = mpatches.Patch(color=cmap(increment), label=c[num])\n",
    "            handles_list.append(patch)\n",
    "       \n",
    "         ax=vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "\n",
    "         cx.add_basemap(ax, \n",
    "                     source=cx.providers.Esri.WorldImagery,  \n",
    "                     crs=crs,\n",
    "                     attribution=False, #No citation\n",
    "                     ) \n",
    "\n",
    "         \n",
    "         ax2 = fig.add_axes(rect=[.72, 0.025,0.22, .88]) #frame_on=False #dimensions [left, bottom, width, height] of the new Axes\n",
    "         ax2.legend(handles=handles_list, loc='upper right', bbox_to_anchor=(1, 1), fontsize='small',title=f'{legend_title}',labelspacing=0.5, handleheight=2, handlelength=3)\n",
    "\n",
    "\n",
    "         #------\n",
    "         plt.xticks([])\n",
    "         plt.yticks([])\n",
    "         # ax.title.set_text(f'{title}')\n",
    "         title= f\"Overlay of flood in {Year} with a \\n return period of {return_period} years ({ssp})\"\n",
    "         map_output = f\"{maps}/{city}_{Year}_{ssp}_{flood_type}_fathom_on_fathom_overlays.png\"\n",
    "\n",
    "         ax.set_title(title, fontsize=10, fontweight='bold', wrap=True)\n",
    "         # ax.axis('tight') #zooms in the to the largest features bounds\n",
    "         ax.axis('off')\n",
    "         ax2.axis('off')\n",
    "         plt.tight_layout()\n",
    "         ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\Dropbox\\CRP\\FCS\\data\\fathom\\COASTAL_DEFENDED\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "C:/Users/Aziz/Dropbox/CRP/FCS/data/fathom/COASTAL_DEFENDED/2020: Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mrasterio\\_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_base.pyx:221\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: C:/Users/Aziz/Dropbox/CRP/FCS/data/fathom/COASTAL_DEFENDED/2020: Permission denied",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m stat\u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentile_95\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m section\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfathom\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m create_mosaic\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_fathom_mosaic\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m input_dir\u001b[38;5;241m=\u001b[39mPureWindowsPath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/clean\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m clipped_output_shapefile , df_tranformed \u001b[38;5;241m=\u001b[39mexport_fathom(input_dir, section,shp, stat)\n",
      "Cell \u001b[1;32mIn[21], line 35\u001b[0m, in \u001b[0;36mcreate_fathom_mosaic\u001b[1;34m(section)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m raster_files:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(p):\n\u001b[1;32m---> 35\u001b[0m         raster \u001b[38;5;241m=\u001b[39m \u001b[43mrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         raster_to_mosiac\u001b[38;5;241m.\u001b[39mappend(raster)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraster_to_mosiac : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraster_to_mosiac\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m----output_path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aziz\\anaconda3\\envs\\uh\\lib\\site-packages\\rasterio\\env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aziz\\anaconda3\\envs\\uh\\lib\\site-packages\\rasterio\\__init__.py:333\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 333\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    335\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[0;32m    336\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    337\u001b[0m     )\n",
      "File \u001b[1;32mrasterio\\_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: C:/Users/Aziz/Dropbox/CRP/FCS/data/fathom/COASTAL_DEFENDED/2020: Permission denied"
     ]
    }
   ],
   "source": [
    "stat= [ 'mean','percentile_95']\n",
    "section= ['fathom']\n",
    "\n",
    "create_mosaic=create_fathom_mosaic(section=section)\n",
    "input_dir=PureWindowsPath(os.path.join(data, f\"{section[0]}/clean\"))\n",
    "clipped_output_shapefile , df_tranformed =export_fathom(input_dir, section,shp, stat)\n",
    "\n",
    "titles= [\"(Average)\", \"(95 Percentile)\"]\n",
    "for s, t in zip(stat,titles):  \n",
    "    fathom_barchart(df=df_tranformed, subtitle= t, y_var=s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "legends_format= '{:,.0f}'\n",
    "legends_format_1= '{:,.0f}'\n",
    "legend_title=\"Legend\"\n",
    "visualize_column=\"fathom\"\n",
    "crs = 4326\n",
    "export_fathom_on_fathom_overlays(vector_file, legend_title, crs ,  visualize_column )\n",
    "export_fathom_on_fathom_plus_infrastructure(vector_file, legend_title, crs ,  visualize_column )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def upsample_rasters_to_fathoms_resolution(rasters):\n",
    "    for root, dirs_list, files_list in os.walk( rasters):\n",
    "        for file_name in files_list:\n",
    "            extension = '.tif'\n",
    "            if os.path.splitext(file_name)[-1] == extension and \"clipped_fathom_GLOBAL-1ARCSEC-NW_OFFSET-1in10-PLUVIAL-DEFENDED-DEPTH-2050-SSP2_4.5-PERCENTILE50-v3.0\" in file_name:\n",
    "                # if \"SSP1_2020\" in file_name:\n",
    "                file_name_path = os.path.join(root, file_name)\n",
    "                file_name = os.path.splitext(file_name)[0]\n",
    "                Year = file_name.split('_')[4].upper() \n",
    "                ssp = file_name.split('_')[3].upper() \n",
    "                print(file_name, ssp, Year, file_name_path)\n",
    "                # Get the projection and resolution of fathom and apply it to popdynamic \n",
    "                ds=gdal.Open(file_name_path)\n",
    "                prj=ds.GetProjection()\n",
    "                print(prj)\n",
    "                ulx, xres, xskew, uly, yskew, yres  = ds.GetGeoTransform()\n",
    "                print(\"ulx, xres, xskew, uly, yskew, yres\", ulx, xres, xskew, uly, yskew, yres)\n",
    "\n",
    "            if os.path.splitext(file_name)[-1] == extension and \"projected_clipped_popdynamics\" in file_name:\n",
    "                if \"SSP1_2020\" in file_name:\n",
    "                    file_name_path = os.path.join(root, file_name)\n",
    "                    file_name = os.path.splitext(file_name)[0]\n",
    "                    Year = file_name.split('_')[4].upper() \n",
    "                    ssp = file_name.split('_')[3].upper() \n",
    "                    print(file_name, ssp, Year, file_name_path)\n",
    "                    \n",
    "                    # prj=\"EPSG:4326\"\n",
    "                    upsampled=f'{rasters}/upsample_popdynamics_{ssp}_{Year}.tif'\n",
    "                    reproj = gdal.Warp(upsampled, file_name_path, dstSRS=prj , xRes = xres, yRes =yres, resampleAlg='near')\n",
    "                    reproj = None\n",
    "\n",
    "                    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,10))\n",
    "                    b1 = rasterio.open(file_name_path)\n",
    "                    b2 = rasterio.open(upsampled)\n",
    "                    p1  = rasterio.plot.show(b1, ax=ax[0], cmap='nipy_spectral', title='file_name_path 1')\n",
    "                    p2  = rasterio.plot.show(b2, ax=ax[1], cmap='nipy_spectral', title='upsampled 2')\n",
    "                    plt.show()\n",
    "\n",
    "    return upsampled\n",
    "\n",
    "upsampled=upsample_rasters_to_fathoms_resolution(rasters)\n",
    "upsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from shapely.geometry import box\n",
    "# import rasterio as rio\n",
    "# from rasterio.enums import Resampling\n",
    "# # def upsample_rasters_to_fathoms_resolution(rasters):\n",
    "# for root, dirs_list, files_list in os.walk( rasters):\n",
    "#     for file_name in files_list:\n",
    "#         extension = '.tif'\n",
    "#         if os.path.splitext(file_name)[-1] == extension and \"clipped_fathom_GLOBAL-1ARCSEC-NW_OFFSET-1in10-PLUVIAL-DEFENDED-DEPTH-2050-SSP2_4.5-PERCENTILE50-v3.0\" in file_name:\n",
    "#             # if \"SSP1_2020\" in file_name:\n",
    "#             file_name_path = os.path.join(root, file_name)\n",
    "#             ds=gdal.Open(file_name_path)\n",
    "#             prj=ds.GetProjection()\n",
    "#             print(prj)\n",
    "\n",
    "#         if os.path.splitext(file_name)[-1] == extension and \"projected_clipped_popdynamics\" in file_name:\n",
    "#             if \"SSP1_2020\" in file_name:\n",
    "#                 file_name_path_1 = os.path.join(root, file_name)\n",
    "                \n",
    "\n",
    "# print(file_name_path, file_name_path_1)           \n",
    "\n",
    "# with rio.open(file_name_path) as ras1, rio.open(file_name_path_1) as ras2:\n",
    "#     x_scale = ras2.transform.a / ras1.transform.a\n",
    "#     y_scale = ras2.transform.e / ras1.transform.e\n",
    "\n",
    "#     # scale image transform\n",
    "#     transform = ras2.transform * ras2.transform.scale(\n",
    "#         (ras2.width / ras2.shape[-1]),\n",
    "#         (ras2.height / ras2.shape[-2])\n",
    "#     )\n",
    "\n",
    "#     ext1 = box(*ras1.bounds)\n",
    "#     ext2 = box(*ras2.bounds)\n",
    "#     intersection = ext1.intersection(ext2)\n",
    "#     win1 = rio.windows.from_bounds(*intersection.bounds, ras1.transform)\n",
    "#     win2 = rio.windows.from_bounds(*intersection.bounds, transform)\n",
    "\n",
    "#     overlap_1 = ras1.read(window=win1)\n",
    "#     overlap_2 = ras2.read(\n",
    "#         window=win2,\n",
    "#         out_shape=(\n",
    "#             ras2.count,\n",
    "#             int(win2.height * y_scale),\n",
    "#             int(win2.width * x_scale)\n",
    "#         ),\n",
    "#         resampling=Resampling.bilinear\n",
    "#     )\n",
    "\n",
    "#     print(overlap_1)\n",
    "#     print(overlap_2)\n",
    "\n",
    "# new_dataset = rasterio.open(f'{rasters}/test1_.tif', 'w', driver='GTiff',\n",
    "#                             height = overlap_2.shape[0], width = overlap_2.shape[1],\n",
    "#                             count=1, dtype=str(overlap_2.dtype),\n",
    "#                             crs=prj,\n",
    "#                             transform=transform)\n",
    "\n",
    "# # new_dataset.write(arr, 1)\n",
    "# new_dataset.write(overlap_2)\n",
    "# new_dataset.close()\n",
    "\n",
    "\n",
    "# from osgeo import gdal\n",
    "\n",
    "# driver = gdal.GetDriverByName('GTiff')\n",
    "# file = gdal.Open('/home/user/workspace/raster.tif')\n",
    "# band = file.GetRasterBand(1)\n",
    "# lista = band.ReadAsArray()\n",
    "\n",
    "# # reclassification\n",
    "# for j in  range(file.RasterXSize):\n",
    "#     for i in  range(file.RasterYSize):\n",
    "#         if lista[i,j] < 200:\n",
    "#             lista[i,j] = 1\n",
    "#         elif 200 < lista[i,j] < 400:\n",
    "#             lista[i,j] = 2\n",
    "#         elif 400 < lista[i,j] < 600:\n",
    "#             lista[i,j] = 3\n",
    "#         elif 600 < lista[i,j] < 800:\n",
    "#             lista[i,j] = 4\n",
    "#         else:\n",
    "#             lista[i,j] = 5\n",
    "\n",
    "# # create new file\n",
    "# file2 = driver.Create( 'raster2.tif', file.RasterXSize , file.RasterYSize , 1)\n",
    "# file2.GetRasterBand(1).WriteArray(lista)\n",
    "\n",
    "# # spatial ref system\n",
    "# proj = file.GetProjection()\n",
    "# georef = file.GetGeoTransform()\n",
    "# file2.SetProjection(proj)\n",
    "# file2.SetGeoTransform(georef)\n",
    "# file2.FlushCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get fathom shps list \n",
    "file_list = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_fathom_GLOBAL*v3.0.shp\"))\n",
    "files = []\n",
    "for file_path in file_list:\n",
    "    file_name= os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    print(file_name)\n",
    "    file_name_clean=file_name.replace(\"-\", \" \")\n",
    "    file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "    return_period=file_name_clean[0]\n",
    "    return_period=return_period.split(\"in\")[-1]\n",
    "    flood_type= file_name_clean[1].title()\n",
    "    var= file_name_clean[2]\n",
    "    Year= file_name_clean[4]\n",
    "    scenario=file_name_clean[5]\n",
    "    if \"SSP\" in scenario:\n",
    "        ssp=scenario\n",
    "        ssp=ssp.replace(\"_\", \"-\")\n",
    "    else:\n",
    "        ssp=\"Present\"\n",
    "\n",
    "    #get pop shps list\n",
    "    file_list_1 = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_popdynamics_*.shp\"))\n",
    "    for file_path_1 in file_list_1:\n",
    "        file_name_1= os.path.basename(file_path_1)\n",
    "        file_name_1 = os.path.splitext(file_name_1)[0]\n",
    "        year_1=file_name_1.split(\"_\")[-1]\n",
    "        ssp_1=file_name_1.split(\"_\")[-2]\n",
    "        print(file_name_1, ssp_1, year_1)\n",
    "\n",
    "        layer= gpd.read_file(Path(file_path)).to_crs(WGS84)\n",
    "        layer_1= gpd.read_file(Path(file_path_1)).to_crs(WGS84)\n",
    "\n",
    "        legends_format= '{:,.0f}'\n",
    "        legends_format_1= '{:,.0f}'\n",
    "        legend_title=\"Legend\"\n",
    "        legend_label=f\"{flood_type} flood(cm)\"  #f\"Flood\"\n",
    "        legend_label_1=f\"Population(x1000)\"\n",
    "        cmap ='afmhot_r'\n",
    "        cmap_1 =\"OrRd\"\n",
    "        visualize_column=\"fathom\"\n",
    "        visualize_column_1=\"popdynami\"\n",
    "        crs = 4326\n",
    "        title= f\"Population in {year_1} ({ssp_1}) overlayed on \\n {flood_type} flood in {Year} with a \\n return period of {return_period} years ({ssp})\"\n",
    "        layer_1[visualize_column_1]=layer_1[visualize_column_1].div(1000).round(0)\n",
    "        map_output = f\"{maps}/{city}_{year_1}_{ssp_1}_pop_{flood_type}_{Year}_fathom_overlays.png\"\n",
    "        export_map_fath=export_map_fathom_overlays(vector_file, \n",
    "                                                layer_1,\n",
    "                                                layer, \n",
    "                                                legend_title,\n",
    "                                                legends_format,\n",
    "                                                legends_format_1,\n",
    "                                                legend_label,\n",
    "                                                legend_label_1, \n",
    "                                                crs ,\n",
    "                                                cmap,\n",
    "                                                cmap_1,  \n",
    "                                                visualize_column,\n",
    "                                                visualize_column_1, \n",
    "                                                title, map_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Urban land cover overlayed on floods. infras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get fathom shps list \n",
    "file_list = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_fathom_GLOBAL*v3.0.shp\"))\n",
    "for file_path in file_list:\n",
    "    file_name= os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    print(file_name)\n",
    "    file_name_clean=file_name.replace(\"-\", \" \")\n",
    "    file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "    return_period=file_name_clean[0]\n",
    "    return_period=return_period.split(\"in\")[-1]\n",
    "    flood_type= file_name_clean[1].title()\n",
    "    var= file_name_clean[2]\n",
    "    Year= file_name_clean[4]\n",
    "    scenario=file_name_clean[5]\n",
    "    if \"SSP\" in scenario:\n",
    "        ssp=scenario\n",
    "        ssp=ssp.replace(\"_\", \"-\")\n",
    "    else:\n",
    "        ssp=\"Present\"\n",
    "\n",
    "    #get pop shps list\n",
    "    file_list_1 = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_urbanland_*.shp\"))\n",
    "    for file_path_1 in file_list_1:\n",
    "        file_name_1= os.path.basename(file_path_1)\n",
    "        file_name_1 = os.path.splitext(file_name_1)[0]\n",
    "        year_1=file_name_1.split(\"_\")[-2].upper()\n",
    "        year_1=year_1.split(\"GDP\")[-1].upper()\n",
    "        ssp_1=file_name_1.split(\"_\")[-1].upper() \n",
    "        print(file_name_1, ssp_1, year_1)\n",
    "        layer= gpd.read_file(Path(file_path)).to_crs(WGS84)\n",
    "        layer_1= gpd.read_file(Path(file_path_1)).to_crs(WGS84)\n",
    "        legends_format= '{:,.0f}'\n",
    "        legends_format_1= '{:,.0f}'\n",
    "        legend_title=\"Legend\"\n",
    "        legend_label=f\"{flood_type} flood(cm)\"  #f\"Flood\"\n",
    "        legend_label_1=\"% Urban Land\"\n",
    "        \n",
    "        cmap ='afmhot_r'\n",
    "        cmap_1 =\"RdPu\"\n",
    "        visualize_column=\"fathom\"\n",
    "        visualize_column_1=\"urbanland\"\n",
    "        crs = 4326\n",
    "        title= f\"% Urban in {year_1} ({ssp_1}) overlayed on \\n {flood_type} flood in {Year} with a \\n return period of {return_period} years ({ssp})\"\n",
    "        layer_1[visualize_column_1]=layer_1[visualize_column_1].multiply(100).round(0)\n",
    "\n",
    "        map_output = f\"{maps}/{city}_{year_1}_{ssp_1}_urbanland_{flood_type}_{Year}_fathom_overlays.png\"\n",
    "        export_map_fath=export_map_fathom_overlays(vector_file, \n",
    "                                                layer_1,\n",
    "                                                layer, \n",
    "                                                legend_title,\n",
    "                                                legends_format,\n",
    "                                                legends_format_1,\n",
    "                                                legend_label,\n",
    "                                                legend_label_1, \n",
    "                                                crs ,\n",
    "                                                cmap,cmap_1,  \n",
    "                                                visualize_column,\n",
    "                                                visualize_column_1, \n",
    "                                                title, map_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get fathom shps list \n",
    "# flood_layers=[]\n",
    "# years=[]\n",
    "\n",
    "flood_layers_dict = defaultdict(list)\n",
    "file_list = glob.glob(os.path.join(shapefiles, \"polygonized_clipped_fathom_GLOBAL*v3.0.shp\"))\n",
    "for file_path in file_list:\n",
    "    file_name= os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    # print(file_name)\n",
    "    file_name_clean=file_name.replace(\"-\", \" \")\n",
    "    file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "    return_period=file_name_clean[0]\n",
    "    return_period=return_period.split(\"in\")[-1]\n",
    "    flood_type= file_name_clean[1].title()\n",
    "    var= file_name_clean[2]\n",
    "    Year= file_name_clean[4]\n",
    "    scenario=file_name_clean[5]\n",
    "    if \"SSP\" in scenario:\n",
    "        ssp=scenario\n",
    "        ssp=ssp.replace(\"_\", \"-\")\n",
    "    else:\n",
    "        ssp=\"Present\"\n",
    "    # flood_layers_dict[Year].append(file_name)\n",
    "    flood_layers_dict[Year].append(file_path)\n",
    "\n",
    "res = [[i for i in flood_layers_dict[x]] for x in flood_layers_dict.keys()] \n",
    "# cmap_list=[\"blue\", \"red\", \"cyan\"]\n",
    "cmap_list=list(colormaps)\n",
    "for i in res:\n",
    "    print( i)\n",
    "    for cmap, j in enumerate(i):\n",
    "        # for file_path in file_list:\n",
    "        file_name= os.path.basename(j)\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        file_name_clean=file_name.replace(\"-\", \" \")\n",
    "        file_name_clean=file_name_clean.split(\" \")[3:-1]\n",
    "        return_period=file_name_clean[0]\n",
    "        return_period=return_period.split(\"in\")[-1]\n",
    "        flood_type= file_name_clean[1].title()\n",
    "        var= file_name_clean[2]\n",
    "        Year= file_name_clean[4]\n",
    "        scenario=file_name_clean[5]\n",
    "        if \"SSP\" in scenario:\n",
    "            ssp=scenario\n",
    "            ssp=ssp.replace(\"_\", \"-\")\n",
    "        else:\n",
    "            ssp=\"Present\"\n",
    "        print( j, cmap_list[cmap], Year,ssp , flood_type)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "There's another way to do this, also. You can load in your DEM as a numpy array using\n",
    "\n",
    "dem_obj = gdal.Open(path_to_DEM)\n",
    "I = dem_obj.ReadAsArray()\n",
    "Now set all the values equal to nan:\n",
    "\n",
    "I = I * np.nan\n",
    "Now you need to load your rivers raster:\n",
    "\n",
    "rr_obj = gdal.Open(path_to_riv_raster)\n",
    "Ir = rr_obj.ReadAsArray()\n",
    "I would resample the river raster next, using skimage.transform's resize function. You need to know the size of your resampled river raster, though, which you can obtain through the geotransforms of both:\n",
    "\n",
    "gt_dem = dem_obj.GetGeoTransform()\n",
    "new_rr_shape = (Ir.shape[0]/gt_dem[4], Ir.shape[1]/gt_dem[1])\n",
    "Now resize the river raster so that the pixels are the same size as the DEM:\n",
    "\n",
    "import skimage.transform as st \n",
    "Ir = st.resize(Ir, new_rr_shape, mode='constant')\n",
    "Now you have your river raster at the same resolution as your DEM. The next step is to place it in the correct position within the np.nan array we created from the DEM. To do that, you need to use the geotransforms of both images to align the top-left-most pixel.\n",
    "\n",
    "gt_rr = rr_obj.GetGeoTransform()\n",
    "ncols_offset = (gt_dem[0] - gt_rr[0]) / gt_dem[1]\n",
    "nrows_offset = (gt_dem[3] - gt_rr[3]) / gt_dem[4]\n",
    "You'll notice that your ncols and nrows _offsets are not integers, but you'll need them to be. You can use either round() or int() (or other choices) to make them integers; the choice you make will shift the river raster no more than a pixel in either direction.\n",
    "\n",
    "Now that you have your river raster resampled to the same resolution, and an empty (nan-filled, really) DEM template raster to put them into, and the row, col offsets, you can just put the river raster in with numpy:\n",
    "\n",
    "import numpy as np\n",
    "I[nrows_offset:nrows_offset+Ir.shape[1], ncols_offset:ncols_offset+Ir.shape[0]] = Ir\n",
    "Finally, you'll want to save your resampled, rescaled river raster as a geotiff. I will post code here you can use as a template:\n",
    "\n",
    "def write_geotiff(raster, gt, wkt, outputpath, dtype=gdal.GDT_UInt16, options=['COMPRESS=LZW'], color_table=0, nbands=1, nodata=False):\n",
    "\n",
    "    width = np.shape(raster)[1]\n",
    "    height = np.shape(raster)[0]\n",
    "\n",
    "    # Prepare destination file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    if options != 0:\n",
    "        dest = driver.Create(outputpath, width, height, nbands, dtype, options)\n",
    "    else:\n",
    "        dest = driver.Create(outputpath, width, height, nbands, dtype)\n",
    "\n",
    "    # Write output raster\n",
    "    if color_table != 0:\n",
    "        dest.GetRasterBand(1).SetColorTable(color_table)\n",
    "\n",
    "    dest.GetRasterBand(1).WriteArray(raster)\n",
    "\n",
    "    if nodata is not False:\n",
    "        dest.GetRasterBand(1).SetNoDataValue(nodata)\n",
    "\n",
    "    # Set transform and projection\n",
    "    dest.SetGeoTransform(gt)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dest.SetProjection(srs.ExportToWkt())\n",
    "\n",
    "    # Close output raster dataset \n",
    "    dest = None\n",
    "Your call should look something like:\n",
    "\n",
    "write_geotiff(I, gt_dem, dem_obj.GetProjection(), path_to_output_file, nodata=np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gdal.Warp('outputRaster.tif', 'inputRaster.tif', xRes=5, yRes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ample of upsampling by a factor of 2 using the bilinear resampling method.\n",
    "\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "upscale_factor = 2\n",
    "\n",
    "with rasterio.open(\"example.tif\") as dataset:\n",
    "\n",
    "    # resample data to target shape\n",
    "    data = dataset.read(\n",
    "        out_shape=(\n",
    "            dataset.count,\n",
    "            int(dataset.height * upscale_factor),\n",
    "            int(dataset.width * upscale_factor)\n",
    "        ),\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "    # scale image transform\n",
    "    transform = dataset.transform * dataset.transform.scale(\n",
    "        (dataset.width / data.shape[-1]),\n",
    "        (dataset.height / data.shape[-2])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay all floods ontop of each other. Present and 2050 on respective ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # africa.loc[africa['EAC'] == 1].plot(ax=ax, facecolor='none', edgecolor='red', linewidth=2)\n",
    "\n",
    "# # def export_fathom():\n",
    "\n",
    "# section= [\"fathom\"]\n",
    "# stat= ['mean']\n",
    "# # cmap= \"magma\" #\"Blues\"\n",
    "# cmap='Reds' \n",
    "# legends_format= '{:.2f}'\n",
    "\n",
    "# # export_fathom()\n",
    "# hexbins_projected= create_hexbins(shapefiles, shp=shp, hexbin_res=hexbin_res, WGS84=WGS84)\n",
    "# vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "# for subdir in section:\n",
    "#     combined_results = []\n",
    "#     extension = '.tif'\n",
    "#     for root, dirs_list, files_list in os.walk( os.path.join(data, subdir)):\n",
    "#         for file_name in files_list:\n",
    "#             if os.path.splitext(file_name)[-1] == extension:\n",
    "#                 file_name_path = os.path.join(root, file_name)\n",
    "#                 file_name = os.path.splitext(file_name)[0]\n",
    "#                 flood_type=file_name_path.split('\\\\')[-2].title() \n",
    "#                 Year = file_name.split('in')[1].title() \n",
    "#                 ssp= flood_type\n",
    "#                 print(f\"file_name: {file_name} ---  Year:{Year} --- flood_type:{flood_type}\")\n",
    "#                 file_name_projected= f'{file_name}_wsg84.tif'\n",
    "#                 file_name_projected= file_name_projected.replace(\"-\", \"_\")\n",
    "#                 output_raster=os.path.join(root, file_name_projected) \n",
    "#                 rst = file_name_path \n",
    "#                 raster = rasterio.open(rst)\n",
    "#                 # vector = load_shapefile(shp)\n",
    "#                 vector = load_shapefile_buffer(shp)\n",
    "#                 fathom_clean= os.path.join(data,  \"fathomclean\")\n",
    "#                 fathom_clean=create_dir(fathom_clean)\n",
    "#                 output_fathom=create_dir(os.path.join(fathom_clean, flood_type)) \n",
    "#                 clean_rst = f'{output_fathom}/{file_name}.tif'\n",
    "#                 clip_and_export_raster_fathom(input_gpdf=vector, raster=rst , out_raster=clean_rst)\n",
    "#                 rst=clean_rst\n",
    "#                 raster = rasterio.open(rst)\n",
    "#                 p_vector = reproject_gpdf(vector, raster)\n",
    "#                 stats_to_get = list_statistics(stat)\n",
    "#                 df = calculate_zonal_stats(p_vector, rst, stats_to_get)\n",
    "#                 df['Scenario'] = flood_type\n",
    "#                 df['Year'] = Year\n",
    "#                 # print(file_name,  file_name_path, subdir, ssp, Year)\n",
    "#                 combined_results.append(df)\n",
    "#                 #clip and export rasters for the pertinent ssp and year combo\n",
    "#                 if (flood_type!=\"\" ) and (Year==\"10\" ):\n",
    "#                     out_rst = f'{rasters}/clipped_{subdir}_{file_name}.tif'\n",
    "#                     # clip_and_export_raster(input_gpdf=vector, raster=rst , out_raster=out_rst)\n",
    "#                     out_rst_2 = f'{rasters}/clipped_2_{subdir}_{file_name}.tif'\n",
    "#                     map_title= f\"{flood_type} flood event in {Year} years\"\n",
    "#                     legend_title=f\"{flood_type} flood\"\n",
    "#                     [statistic]= stats_to_get\n",
    "#                     hexbins_projected , max , min= hexbin_zonal_stats(shp=shp, \\\n",
    "#                         hexbins_projected=hexbins_projected , crs=WGS84 , \\\n",
    "#                             raster=rst , out_raster=out_rst_2, statistic=statistic)\n",
    "#                     crs = 4326\n",
    "#                     projected_raster = f'{rasters}/projected_clipped_{subdir}_{file_name}.tif'\n",
    "#                     unprojected_raster = out_rst_2\n",
    "#                     reproject_rast= reproject_rasters(crs=crs, unprojected_raster=unprojected_raster, \\\n",
    "#                                                         projected_raster=projected_raster)\n",
    "#                     # cmap = plt.get_cmap('RdYlGn', 10)\n",
    "#                     # cmap = truncate_colormap(cmap, 0.1, 1)\n",
    "#                     map_output = f\"{maps}/{city}_{subdir}_{file_name}_cmap_max_div_4.png\"\n",
    "#                     document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "#                     #export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n",
    "#                     output_shapefile = f'{shapefiles}/polygonized_{subdir}_{file_name}.shp'\n",
    "#                     output_geojson= f'{shapefiles}/geojson_{subdir}_{file_name}.geojson'\n",
    "#                     output_csv= f'{tables}/csv_{subdir}_{file_name}.csv'\n",
    "#                     driver= 'ESRI Shapefile'\n",
    "#                     mask_value=None\n",
    "#                     raster_val= subdir[0:9]\n",
    "#                     name = polygonize(raster_file=projected_raster ,crs=EPSG_str, output_shapefile=output_shapefile, \\\n",
    "#                                         driver=driver, mask_value=mask_value, raster_val=raster_val, ssp=ssp, Year=Year)\n",
    "#                     clipped_output_shapefile = f'{shapefiles}/polygonized_clipped_{subdir}_{file_name}.shp'\n",
    "#                     clip_raster_shapes= clip_raster_shapefiles(shp=shp, output_shapefile=output_shapefile, crs=crs,\\\n",
    "#                                                                 output_geojson= output_geojson, output_csv=output_csv, \\\n",
    "#                                                                 clipped_output_shapefile=clipped_output_shapefile)                        \n",
    "#                     polygonized_shapefile = gpd.read_file(clipped_output_shapefile).to_crs(WGS84) \n",
    "#                     polygonized_shapefile[raster_val]=polygonized_shapefile[raster_val].round(2)\n",
    "#                     export_the_map= export_map(map_dir= output, vector_file=vector_file , \\\n",
    "#                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "#                             title=map_title, map_output=map_output)\n",
    "#                     export_the_osm_map= export_map_osm(map_dir= output, vector_file=vector_file , \\\n",
    "#                             hexagons=polygonized_shapefile,legend_title=legend_title, legends_format=legends_format, crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "#                             title=map_title, map_output=map_output)\n",
    "\n",
    "\n",
    "#     # Append vertically and then pivot horizontally.\n",
    "#     df = gpd.GeoDataFrame( pd.concat( combined_results, ignore_index=True) )\n",
    "#     # Wide\n",
    "#     [stats_to_get] = stats_to_get\n",
    "#     df_tranformed= df.pivot(index=['Scenario'], columns='Year', values=stats_to_get)        \n",
    "#     # Long\n",
    "#     df_graph = df_tranformed.melt(ignore_index=False).reset_index()\n",
    "#     # export csv of wide\n",
    "#     df_tranformed.to_csv(f\"{tables}/{country}_{city}_{subdir}.csv\")\n",
    "#     #run the visualize function to export the heatmap\n",
    "#     # visualize_gdp_figure(df_tranformed, country, city, subdir)\n",
    "#     # visualize_gdp_graph(df_graph, country, city, subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 100 return period. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic pyramids functions from Bramkas note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def demographic_projection(city, country_iso3, country, ssp_to_use, section):\n",
    "\n",
    "    gc_city_folder = os.path.join(data, f'{section[0]}/GC_countries/')\n",
    "    ssp_master_fn = os.path.join(data, f'{section[0]}/SspDb_country_data_2013-06-12.csv')\n",
    "    # Step 0.1: Get country-level projections\n",
    "    ssp_master = pd.read_csv(ssp_master_fn)\n",
    "    ssp_country = ssp_master.loc[ssp_master['REGION']==country_iso3]\n",
    "    ssp_country.reset_index(inplace=True, drop=True)\n",
    "    if len(ssp_country)==0:\n",
    "        print('Country not found in SSP database. Check if ISO3 name of the country is correct, and whether the country is actually available in the SSP database.')\n",
    "        return 0,0\n",
    "    else:\n",
    "        del ssp_master\n",
    "        \n",
    "    # Step 0.2: Get city's historical demographic change\n",
    "    global_cities = pd.read_csv(gc_city_folder+'GC_{}.csv'.format(country_iso3))\n",
    "    gc_city = global_cities.loc[global_cities['Location']==city]\n",
    "    if len(gc_city)==0:\n",
    "        print('City not found in the Global Cities database. Manually check if the city name spelling is correct, and whether the city is actually available in the Global Cities database.')\n",
    "        return 0,0\n",
    "    else:\n",
    "        del global_cities\n",
    "        \n",
    "    ######################################################################################################\n",
    "    ############## STEP 1: Get historical population change at country level\n",
    "    ######################################################################################################\n",
    "    all_variables = list(np.unique(ssp_country['VARIABLE']))\n",
    "    all_age_variables = [x for x in all_variables if 'Aged' in x]\n",
    "\n",
    "    children_ssp = ['Aged0-4', 'Aged5-9', 'Aged10-14']\n",
    "    young_adult_ssp = ['Aged15-19', 'Aged20-24', 'Aged25-29', 'Aged30-34']\n",
    "    adult_ssp = ['Aged35-39', 'Aged40-44', 'Aged45-49', 'Aged50-54', 'Aged55-59', 'Aged60-64']\n",
    "    elderly_ssp = ['Aged65-69', 'Aged70-74', 'Aged75-79', 'Aged80-84', 'Aged85-89', 'Aged90-94', 'Aged95-99', 'Aged100+']\n",
    "\n",
    "    buckets_ssp = {'children': children_ssp, 'young_adult': young_adult_ssp, 'adult': adult_ssp, 'elderly': elderly_ssp}\n",
    "    \n",
    "    def get_population_ssp_year(ssp_country, ssp, year, buckets_ssp, all_variables):\n",
    "    \n",
    "        age_groups = []\n",
    "        population = []\n",
    "\n",
    "        for age_group in buckets_ssp.keys():\n",
    "\n",
    "            # Filter relevant variables for the age group\n",
    "            filtered_vars = list(filter(lambda x: np.sum([y in x for y in buckets_ssp[age_group]])>0, all_variables))\n",
    "            filtered_vars = [x for x in filtered_vars if len(x.split(\"|\"))==3]\n",
    "\n",
    "            # Filter the SSP dataframe\n",
    "            filtered_dataframe = ssp_country.loc[ssp_country['VARIABLE'].isin(filtered_vars)]\n",
    "            filtered_dataframe = filtered_dataframe.loc[filtered_dataframe['SCENARIO'].str.contains(ssp)]\n",
    "\n",
    "            age_groups.append(age_group)\n",
    "            try:\n",
    "                population.append(filtered_dataframe[year].sum())\n",
    "            except:\n",
    "                population.append(filtered_dataframe[str(year)].sum())\n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        output['age_group'] = age_groups\n",
    "        output['population'] = population\n",
    "        output['ssp'] = ssp\n",
    "        output['year'] = year\n",
    "\n",
    "        return output\n",
    "    \n",
    "    pop_2010_sspA = get_population_ssp_year(ssp_country, ssp_to_use, 2010, buckets_ssp, all_variables)\n",
    "    pop_2020_sspA = get_population_ssp_year(ssp_country, ssp_to_use, 2020, buckets_ssp, all_variables)\n",
    "\n",
    "    # get historical country change\n",
    "    hstr_chg_country = {}\n",
    "    for age_group in buckets_ssp.keys():\n",
    "        pop_2020 = pop_2020_sspA.loc[pop_2020_sspA['age_group']==age_group, 'population'].values[0]\n",
    "        pop_2010 = pop_2010_sspA.loc[pop_2010_sspA['age_group']==age_group, 'population'].values[0]\n",
    "        hstr_chg_country[age_group] =  (pop_2020 - pop_2010) / pop_2010\n",
    "        \n",
    "    ######################################################################################################\n",
    "    ############## STEP 2: Get historical population change at city level\n",
    "    ######################################################################################################\n",
    "    gc_city_slc = gc_city.loc[gc_city['Year'].isin([2010, 2020])]\n",
    "\n",
    "    children_gc = ['POP0_14T']\n",
    "    young_adult_gc = ['POP15_19T', 'POP20_24T', 'POP25_29T', 'POP30_34T']\n",
    "    adult_gc = ['POP35_39T', 'POP40_44T', 'POP45_49T', 'POP50_54T', 'POP55_59T', 'POP60_64T']\n",
    "    elderly_gc = ['POP65_T']\n",
    "\n",
    "    buckets_gc = {'children': children_gc, 'young_adult': young_adult_gc, 'adult': adult_gc, 'elderly': elderly_gc}\n",
    "\n",
    "    # get historical city change\n",
    "    hstr_chg_city = {}\n",
    "    for age_group in buckets_gc.keys():\n",
    "        pop_2020 = gc_city_slc.loc[gc_city_slc['Year']==2020][buckets_gc[age_group]].sum().values[0]\n",
    "        pop_2010 = gc_city_slc.loc[gc_city_slc['Year']==2010][buckets_gc[age_group]].sum().values[0]\n",
    "        hstr_chg_city[age_group] =  (pop_2020 - pop_2010) / pop_2010\n",
    "        \n",
    "    ######################################################################################################\n",
    "    ############## STEP 3: Calculate scaling factors between country-level and city-level demographic change\n",
    "    ######################################################################################################\n",
    "    scaling_factor = {}\n",
    "    for age_group in buckets_gc.keys():\n",
    "        scaling_factor[age_group] = hstr_chg_city[age_group] / hstr_chg_country[age_group]\n",
    "        \n",
    "    ######################################################################################################\n",
    "    ############## STEP 4-6: Calculate future city population\n",
    "    ######################################################################################################\n",
    "    country_pop = pd.DataFrame(columns=['Year'] + list(buckets_gc.keys()))\n",
    "\n",
    "    city_pop = pd.DataFrame()\n",
    "    city_pop['Year'] = [2020]\n",
    "    for age_group in buckets_gc.keys():\n",
    "        city_pop[age_group] = [gc_city_slc.loc[gc_city_slc['Year']==2020][buckets_gc[age_group]].sum().values[0]]\n",
    "\n",
    "    for i in np.arange(2025,2105, 5):\n",
    "        \n",
    "        # Step 4: Get future projections at country level\n",
    "        pop_future1_sspA = get_population_ssp_year(ssp_country, ssp_to_use, i-5, buckets_ssp, all_variables)\n",
    "        pop_future2_sspA = get_population_ssp_year(ssp_country, ssp_to_use, i, buckets_ssp, all_variables)\n",
    "        future_chg_country = {}\n",
    "        for age_group in buckets_ssp.keys():\n",
    "            pop_future1 = pop_future1_sspA.loc[pop_future1_sspA['age_group']==age_group, 'population'].values[0]\n",
    "            pop_future2 = pop_future2_sspA.loc[pop_future2_sspA['age_group']==age_group, 'population'].values[0]\n",
    "            future_chg_country[age_group] =  (pop_future2 - pop_future1) / pop_future1\n",
    "\n",
    "        # Step 5: Adjust future demographic change at country level to the city level, based on the scaling factors \n",
    "        future_chg_city = {}\n",
    "        for age_group in buckets_ssp.keys():\n",
    "            future_chg_city[age_group] = future_chg_country[age_group] * scaling_factor[age_group]\n",
    "\n",
    "        # Step 6: Calculate future population for each demographic group at city level\n",
    "        to_input = []\n",
    "        to_input.append(i)\n",
    "        for age_group in buckets_gc.keys():\n",
    "            current_pop = city_pop.loc[city_pop['Year']==i-5, age_group].values[0]\n",
    "            future_pop = current_pop + current_pop * future_chg_city[age_group]\n",
    "            to_input.append(future_pop)\n",
    "\n",
    "        city_pop.loc[len(city_pop)] = to_input\n",
    "\n",
    "        # Save also country-level projection, just in case needed\n",
    "        if i == 2025:\n",
    "            country_pop.loc[len(country_pop)] = [i-5] + [pop_future1_sspA.loc[pop_future1_sspA['age_group']==list(buckets_gc.keys())[j]]['population'].values[0] for j in range(len(list(buckets_gc.keys())))]\n",
    "        country_pop.loc[len(country_pop)] = [i] + [pop_future2_sspA.loc[pop_future2_sspA['age_group']==list(buckets_gc.keys())[j]]['population'].values[0] for j in range(len(list(buckets_gc.keys())))]\n",
    "        \n",
    "    city_pop['SSP'] = ssp_to_use\n",
    "    country_pop['SSP'] = ssp_to_use\n",
    "        \n",
    "    return city_pop, country_pop\n",
    "\n",
    "# Add a plt.figure and then plot the pyramids on the same axis\n",
    "def pyramid(city_pop, year1, year2, ssp):\n",
    "    \n",
    "    # Slice the pyramid\n",
    "    city_pop_pyramid = city_pop.loc[city_pop['Year'].isin([year1, year2])]\n",
    "    city_pop_pyramid = city_pop_pyramid.loc[city_pop_pyramid['SSP']==ssp]\n",
    "    city_pop_pyramid = city_pop_pyramid[city_pop_pyramid.columns[1:-1]]\n",
    "    city_pop_pyramid = city_pop_pyramid.div(city_pop_pyramid.sum(axis=1), axis=0)\n",
    "    city_pop_pyramid *= 100\n",
    "    city_pop_pyramid['Year'] = [year1, year2]\n",
    "    \n",
    "    # Visualization\n",
    "    arr1 = city_pop_pyramid.loc[city_pop_pyramid['Year']==year1].values[0][:-1]\n",
    "    arr1 *= -1\n",
    "    arr2 = city_pop_pyramid.loc[city_pop_pyramid['Year']==year2].values[0][:-1]\n",
    "    \n",
    "    minval = np.floor(np.min(arr1) / 20)\n",
    "    maxval = np.ceil(np.max(arr2)  / 20)\n",
    "    topper = np.max([np.abs(minval), np.abs(maxval)])\n",
    "    fig_sns, ax_sns = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    ax_sns = sns.barplot(x=arr1, y=city_pop_pyramid.columns[:-1], color=\"blue\", ax=ax_sns)\n",
    "    ax_sns = sns.barplot(x=arr2, y=city_pop_pyramid.columns[:-1], color=\"green\", ax=ax_sns)\n",
    "\n",
    "\n",
    "    plt.xticks(ticks=[-topper*20, -topper*10, 0, topper*10, topper*20],\n",
    "    labels=['{}%'.format(topper*20), '{}%'.format(topper*10), '0', '{}%'.format(topper*10), '{}%'.format(topper*20)])\n",
    "\n",
    "    ax_sns.text(x=0.25, y=1.05, s=year1, transform = ax_sns.transAxes, horizontalalignment='center',\n",
    "             fontsize=12, color='blue')\n",
    "\n",
    "    ax_sns.text(x=0.75, y=1.05, s=year2, transform = ax_sns.transAxes, horizontalalignment='center',\n",
    "             fontsize=12, color='green')\n",
    "\n",
    "    ax_sns.text(x=0.5, y=1.08, s=ssp, transform = ax_sns.transAxes, horizontalalignment='center',\n",
    "             fontsize=15, color='black')\n",
    "\n",
    "    ax_sns.axvline(color='white')\n",
    "\n",
    "    ax_sns.figure.savefig(f\"{maps}/{country}_{city}_{section[0]}_{ssp}_{year1}_{year2}_pyramid.jpeg\",\n",
    "            dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "def pop_lineplot(city_pop, ssp):\n",
    "\n",
    "    city_pop_ = city_pop.loc[city_pop['SSP']==ssp]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.lineplot(x='Year', y='children', data=city_pop_, color='#1b9e77', ax=ax)\n",
    "    sns.lineplot(x='Year', y='young_adult', data=city_pop_, color='#d95f02', ax=ax)\n",
    "    sns.lineplot(x='Year', y='adult', data=city_pop_, color='#7570b3', ax=ax)\n",
    "    sns.lineplot(x='Year', y='elderly', data=city_pop_, color='#e7298a', ax=ax)\n",
    "\n",
    "    ax.legend(['Children', 'Young adult', 'Adult', 'Elderly'], loc='upper left', bbox_to_anchor=(0.98, 1))\n",
    "\n",
    "    ax.set_ylabel(\"Population ('000)\")\n",
    "\n",
    "    ax.set_title(\"{} - {}\".format(city,ssp), fontsize=14)\n",
    "\n",
    "    sns.despine()\n",
    "    \n",
    "    ax.figure.savefig(f\"{maps}/{country}_{city}_{section[0]}_{ssp}_lineplot.jpeg\",\n",
    "            dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ ==  '__main__':    \n",
    "    country_iso3 = 'BGD'\n",
    "    section= ['demographic']\n",
    "    city_pop_ssp2, country_pop_ssp2 = demographic_projection(city=city, \n",
    "                                                            country_iso3=country_iso3, \n",
    "                                                            country=country, \n",
    "                                                            ssp_to_use='SSP2', section=section)\n",
    "\n",
    "    city_pop_ssp5, country_pop_ssp5 = demographic_projection(city=city, \n",
    "                                                            country_iso3=country_iso3, \n",
    "                                                            country=country, \n",
    "                                                            ssp_to_use='SSP5', section=section)\n",
    "\n",
    "    city_pop = city_pop_ssp2._append(city_pop_ssp5)\n",
    "    city_pop = city_pop.loc[city_pop['Year']<=2070]\n",
    "    city_pop.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    country_pop = country_pop_ssp2._append(country_pop_ssp5)\n",
    "    country_pop = country_pop.loc[country_pop['Year']<=2070]\n",
    "    country_pop.reset_index(inplace=True, drop=True)\n",
    "    year1_list=[2020, 2050]\n",
    "    year2_list=[2050, 2070]\n",
    "    ssp_list=[\"SSP2\", \"SSP5\"]\n",
    "    for ssp in ssp_list:\n",
    "      pop_lineplot(city_pop, ssp)\n",
    "      for year1, year2 in zip(year1_list, year2_list):\n",
    "         print(year1, year2, ssp)\n",
    "         export_pyramid= pyramid(city_pop, year1, year2, ssp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporate the tropical cyclones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_wind_speed(gdf, country, city):\n",
    "    # Read data\n",
    "    # gdf = gpd.read_file(gdf_fn)\n",
    "    city_centroid = gdf.iloc[0]['geometry'].centroid\n",
    "    coords = [(city_centroid.x, city_centroid.y)] # get centroid of the city\n",
    "    coords += [x for x in gdf.iloc[0]['geometry'].exterior.coords] # get exterior of the city\n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    # List of return periods and regions\n",
    "    all_rps = [10, 20, 30, 40, 50, 60, 70, 80, 90, \n",
    "               100, 200, 300, 500, 600, 700, 800, 900,\n",
    "               1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    all_regions = ['EP', 'NA', 'NI', 'SI', 'SP', 'WP']\n",
    "\n",
    "    # Get wind speed in current climate\n",
    "    max_vals = []\n",
    "\n",
    "    for rp in all_rps:\n",
    "        vals = []\n",
    "        for region in all_regions:\n",
    "            # raster_file = r\"inputs/STORM_current/FIXED_RP/STORM_FIXED_RETURN_PERIODS_{}_{}_YR_RP.tif\".format(region, rp)\n",
    "            raster_file = os.path.join(data, f'{section[0]}/STORM_current/FIXED_RP/STORM_FIXED_RETURN_PERIODS_{region}_{rp}_YR_RP.tif')\n",
    "            src = rasterio.open(raster_file)\n",
    "            vals.append([x for x in src.sample(coords)])\n",
    "\n",
    "        max_vals.append(np.max(vals)) # take maximum value across all sampled points\n",
    "\n",
    "    output['RPs'] = all_rps\n",
    "    output['Current'] = max_vals\n",
    "\n",
    "    # List of all climate models used\n",
    "    all_gcms = ['CMCC-CM2-VHR4',\n",
    "                'CNRM-CM6-1-HR',\n",
    "                'EC-Earth3P-HR',\n",
    "                'HADGEM3-GC31-HM']\n",
    "    \n",
    "    # Get wind speed in future climate\n",
    "    for gcm in all_gcms:\n",
    "        max_vals = []\n",
    "\n",
    "        for rp in all_rps:\n",
    "\n",
    "            vals = []\n",
    "\n",
    "            for region in all_regions:\n",
    "                # raster_file = r\"inputs/STORM_climate_change/FIXED_RP/{}/STORM_FIXED_RETURN_PERIODS_{}_{}_{}_YR_RP.tif\".format(gcm, gcm, region, rp)\n",
    "                raster_file = os.path.join(data, f'{section[0]}/STORM_climate_change/FIXED_RP/{gcm}/STORM_FIXED_RETURN_PERIODS_{gcm}_{region}_{rp}_YR_RP.tif')\n",
    "                src = rasterio.open(raster_file)\n",
    "                vals.append([x for x in src.sample(coords)])\n",
    "            max_vals.append(np.max(vals)) # take maximum value across all sampled points\n",
    "        output[gcm] = max_vals\n",
    "    # Save the output\n",
    "    output.to_csv(f\"{tables}/{country}_{city}_{section[0]}_wind.csv\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def visualize_results(output, country, city):\n",
    "    \n",
    "    # List of all climate models used\n",
    "    all_gcms = ['CMCC-CM2-VHR4',\n",
    "                'CNRM-CM6-1-HR',\n",
    "                'EC-Earth3P-HR',\n",
    "                'HADGEM3-GC31-HM']\n",
    "    \n",
    "    # Transform the output into something seaborn-friendly\n",
    "    output_climate = output[[x for x in output.columns if x!='Current']] \n",
    "    output_climate = pd.melt(output_climate, id_vars='RPs', value_vars=all_gcms)\n",
    "    output_climate.columns = ['RPs', 'GCM', 'Wind speed']\n",
    "    \n",
    "    # Get minimum and maximum wind speed for visualization\n",
    "    minval = np.unique(output[output.columns[1:]])[1]\n",
    "    maxval = np.unique(output[output.columns[1:]])[-1]\n",
    "    \n",
    "    # Main visualization code\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.lineplot(data=output_climate, x=\"RPs\", y=\"Wind speed\", label='Climate change\\n(2015-2050)', color='indianred', ax=ax)\n",
    "    ax.plot(output['RPs'], output['Current'], label='Current\\n(1980-2017)')\n",
    "    ax.set_ylim(minval, maxval)\n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='upper left')\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.set_xlabel('Return period', fontsize=12)\n",
    "    ax.set_ylabel('Maximum wind speed (m/s)', fontsize=12)\n",
    "    plt.title('Maximum tropical cyclone\\nwind speed in {}'.format(city))\n",
    "    plt.grid()\n",
    "    \n",
    "    if maxval > 32:\n",
    "        ax.text(x= 14000, y=32, s='-- Cat 1')\n",
    "    if maxval > 42:\n",
    "        ax.text(x= 14000, y=42, s='-- Cat 2')\n",
    "    if maxval > 49:\n",
    "        ax.text(x= 14000, y=49, s='-- Cat 3')\n",
    "    if maxval > 58:\n",
    "        ax.text(x= 14000, y=58, s='-- Cat 4')\n",
    "    if maxval > 70:\n",
    "        ax.text(x= 14000, y=70, s='-- Cat 5')\n",
    "        \n",
    "    # Save and close\n",
    "    plt.savefig(f\"{maps}/{country}_{city}_{section[0]}_cyclone.jpeg\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ ==  '__main__':    \n",
    "    section= ['tropicalcyclones']\n",
    "    gdf =  gpd.read_file(shp).to_crs(WGS84) #r\"inputs/Ukhiya.shp\"\n",
    "    output = get_wind_speed(gdf, country, city)\n",
    "    visualize_results(output, country, city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ ==  '__main__': \n",
    "    if socio_econmic:   \n",
    "        section= ['popdynamics']\n",
    "        stat= ['sum']\n",
    "        cmap = \"OrRd\"\n",
    "        legends_format= '{:,.0f}'\n",
    "        export_popdynamics()\n",
    "        \n",
    "        section= ['gdp'] \n",
    "        stat= ['sum']\n",
    "        cmap= \"RdPu\"\n",
    "        legends_format= '{:,.0f}'\n",
    "        export_gdp()\n",
    "\n",
    "        # section= ['urbanland']  \n",
    "        # stat= ['mean']\n",
    "        # cmap=\"YlGnBu\"\n",
    "        # legends_format= '{:,.0f}'\n",
    "        # export_urbanland()\n",
    "\n",
    "        # section= ['heatflux']\n",
    "        # stat= ['mean']\n",
    "        # # cmap= \"magma\" #\"Blues\"\n",
    "        # cmap='YlOrRd' \n",
    "        # legends_format= '{:,.0f}'   \n",
    "        # export_heatflux()\n",
    "\n",
    "        # section= ['urbanheatisland']\n",
    "        # stat= ['mean']\n",
    "        # cmap='YlOrRd'\n",
    "        # legends_format= '{:.2f}'\n",
    "        # export_urbanheatisland()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ ==  '__main__': \n",
    "    if socio_econmic:  \n",
    "        section= ['popdynamics']\n",
    "        stat= ['sum']\n",
    "        cmap = \"OrRd\"\n",
    "        legends_format= '{:,.0f}'\n",
    "                \n",
    "        # ssp_list=[\"SSP1\", \"SSP2\",\"SSP3\",\"SSP4\",\"SSP5\"]\n",
    "        ssp_list=[\"SSP2\", \"SSP5\"]\n",
    "        start_year_list=[\"2030\",\"2030\"]\n",
    "        end_year_list=[\"2050\",\"2100\"]\n",
    "\n",
    "        # for input_ssp in ssp_list:\n",
    "        #     for start_year, end_year in zip(start_year_list, end_year_list):\n",
    "        #         print(input_ssp, start_year, end_year)\n",
    "        #         polygonized_shapefile=export_popdynamics_change(input_ssp, start_year, end_year)\n",
    "\n",
    "        # section= ['gdp'] \n",
    "        # stat= ['sum']\n",
    "        # cmap= \"RdPu\"\n",
    "        # legends_format= '{:,.0f}'\n",
    "\n",
    "        # for input_ssp in ssp_list:\n",
    "        #     for start_year, end_year in zip(start_year_list, end_year_list):\n",
    "        #         print(input_ssp, start_year, end_year)\n",
    "        #         polygonized_shapefile=export_gdp_change(input_ssp, start_year, end_year)\n",
    "\n",
    "\n",
    "        # section= ['urbanland']  \n",
    "        # stat= ['mean']\n",
    "        # cmap=\"YlGnBu\"\n",
    "        # legends_format= '{:,.2f}'\n",
    "\n",
    "        # for input_ssp in ssp_list:\n",
    "        #     for start_year, end_year in zip(start_year_list, end_year_list):\n",
    "        #         print(input_ssp, start_year, end_year)\n",
    "        #         polygonized_shapefile=export_urbanland_change(input_ssp, start_year, end_year)\n",
    "\n",
    " \n",
    "        # do this after fathom\n",
    "        # section= ['urbanheatisland']\n",
    "        # stat= ['mean']\n",
    "        # cmap='YlOrRd'\n",
    "        # legends_format= '{:.2f}'\n",
    "        # export_urbanheatisland()\n",
    "\n",
    "       # ssp_list=[\"SSP1\", \"SSP2\",\"SSP3\",\"SSP4\",\"SSP5\"]\n",
    "        ssp_list=[\"SSP3\"] #one only\n",
    "        start_year_list=[\"2010\"]\n",
    "        end_year_list=[\"2050\"]\n",
    "\n",
    "        section= ['heatflux']\n",
    "        stat= ['mean']\n",
    "        # cmap= \"magma\" #\"Blues\"\n",
    "        cmap='YlOrRd' \n",
    "        legends_format= '{:,.0f}'   \n",
    "        for input_ssp in ssp_list:\n",
    "            for start_year, end_year in zip(start_year_list, end_year_list):\n",
    "                print(input_ssp, start_year, end_year)\n",
    "                polygonized_shapefile=export_heatflux_change(input_ssp, start_year, end_year) #error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ ==  '__main__':  \n",
    "    if cckp:     \n",
    "        section= ['Precipitation']\n",
    "        appended_data= export_cckp_data(section=section)\n",
    "        export_graph= export_cckp_graph(df=appended_data , x_var_name='time', y_var_names='Variable', subdir='Precipitation', city=city)   \n",
    "        \n",
    "        section= ['Temperature']\n",
    "        df= export_cckp_data(section=section)\n",
    "        export_graph= export_cckp_graph(df=df , x_var_name='time', y_var_names='Variable', subdir='Temperature', city=city)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ ==  '__main__':    \n",
    "    section= ['globalerosion']\n",
    "    polygon=get_city_poolygon(shp)\n",
    "    df=process_erosion()\n",
    "    map_storms(appended_data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Process and Map Storms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "images_displayed=f'{base_dir}/data/images_displayed'  # Input data directory\n",
    "visualize_image(images_displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# vector = load_shapefile(shp).to_crs(WGS84)\n",
    "# vector=vector.reset_index()\n",
    "# # gdf_city = gpd.read_file(shapefile_path).to_crs(WGS84)\n",
    "\n",
    "# # OSM admin layers\n",
    "# tags = {\"boundary\":\"administrative\" }\n",
    "# minx, miny, maxx, maxy = vector.to_crs(WGS84).total_bounds\n",
    "# all_admin_layers =ox.geometries.geometries_from_bbox(miny,maxy,minx, maxx, tags)\n",
    "# # Get the lowest \n",
    "# lowest_admin_level=all_admin_layers[\"admin_level\"].mode()[0]\n",
    "# vector_gdf = all_admin_layers[all_admin_layers['admin_level'] == lowest_admin_level]\n",
    "# lowest_admin_level_name=\"Settlement\"\n",
    "# vector_gdf[\"pre_clip_area\"] = vector_gdf['geometry'].area\n",
    "# sub_city_gdf = gpd.clip(vector_gdf.to_crs(crs), vector.to_crs(crs))\n",
    "# sub_city_gdf[\"post_clip_area\"] = sub_city_gdf['geometry'].area\n",
    "# sub_city_gdf[\"pct_clip_area\"] = (sub_city_gdf[\"post_clip_area\"]/sub_city_gdf[\"pre_clip_area\"])*100\n",
    "# sub_city_gdf = sub_city_gdf[sub_city_gdf['pct_clip_area'] > 50]\n",
    "\n",
    "# # fill english names with native names\n",
    "# sub_city_gdf['name:en']=sub_city_gdf['name:en'].str.strip().replace('', np.nan).fillna(sub_city_gdf['name'])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_city_map(country, city, vector_file,sub_city_gdf,admin_name, lowest_admin_level,basemap, basemap_label, map_output):\n",
    "        plt.rc('font', weight='bold')\n",
    "        crs=3857\n",
    "        vector_file_degrees=vector_file.copy()\n",
    "        vector_file=vector_file.to_crs(crs)\n",
    "        width=12\n",
    "        height=8\n",
    "\n",
    "        admin_entities= len(sub_city_gdf[admin_name])  \n",
    "        if admin_entities<10:\n",
    "                labelfontsize=11\n",
    "                boundarylinewidth=2\n",
    "                print(f\"labelfontsize--->> {labelfontsize}\")  \n",
    "        else:\n",
    "                labelfontsize=7\n",
    "                boundarylinewidth=1.5\n",
    "                print(f\"labelfontsize--->>>> {labelfontsize}\")  \n",
    "\n",
    "\n",
    "        # fig = plt.figure(figsize=(width+5, height+5))\n",
    "        fig = plt.figure(figsize=(width, height))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax = vector_file.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.8,  \n",
    "                                        facecolor='none',\n",
    "                                        edgecolor='blue' , \n",
    "                                        label=f\"{city}\",\n",
    "                                        linewidth=3,\n",
    "                                        zorder=100\n",
    "                                        )\n",
    "\n",
    "\n",
    "        sub_city_gdf=sub_city_gdf.to_crs(crs)\n",
    "        ax = sub_city_gdf.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.8,  \n",
    "                                        facecolor='none',\n",
    "                                        edgecolor='cyan' ,\n",
    "                                        label=f\"{city}\",\n",
    "                                        linewidth=boundarylinewidth,\n",
    "                                        missing_kwds={\"color\": \"white\", \"edgecolor\": \"cyan\", \"label\": \"none\"},\n",
    "                                        zorder=5 \n",
    "\n",
    "                                        )\n",
    "        # \n",
    "        # gdf=gdf.rename(columns = {'name:en':'name_en'}, inplace = True)\n",
    "        sub_city_gdf['name_en'] =sub_city_gdf[admin_name]               \n",
    "        sub_city_gdf= sub_city_gdf.replace('nan', np.nan) #.dropna(how='all', axis=1, inplace=True)\n",
    "        label = sub_city_gdf.dropna(subset='name_en')\n",
    "        texts = []\n",
    "        for i, row in label.iterrows(): \n",
    "                # print(f\"Subcity name: {str(row['name_en'])}\")\n",
    "                texts.append(ax.annotate(text=str(row['name_en']), \n",
    "                                         xy=row.geometry.centroid.coords[0], color=\"black\", \n",
    "                                         ha='center', fontsize=labelfontsize,  path_effects=[pe.withStroke(linewidth=1, \n",
    "                                                                                               foreground=\"white\")]))\n",
    "                \n",
    "        # adjust_text(texts)\n",
    "        vector_file_buffer = vector_file.to_crs(crs).buffer(3000) #for zoom out\n",
    "        minx, miny, maxx, maxy = vector_file_buffer.total_bounds\n",
    "        ax.set_xlim(minx, maxx)\n",
    "        ax.set_ylim(miny, maxy)\n",
    "    \n",
    "        #Identifying how many tiles\n",
    "        latlon_outline = vector_file.to_crs(WGS84).total_bounds\n",
    "        def_zoom = cx.tile._calculate_zoom(*latlon_outline)\n",
    "        howmany= cx.howmany(*latlon_outline, def_zoom+2, ll=True)\n",
    "        print(f'Default Zoom level {def_zoom} and +2 is {howmany}')\n",
    "\n",
    "        try:\n",
    "                cx.add_basemap(ax,\n",
    "                        zoom=\"auto\",\n",
    "                        # zoom=abs(def_zoom-1),\n",
    "                        # zoom=def_zoom,\n",
    "                        crs=vector_file.crs.to_string(),\n",
    "                        source= basemap,\n",
    "                        # source=cx.providers.CartoDB.PositronOnlyLabels,\n",
    "                        # source=cx.providers.CartoDB.DarkMatter,\n",
    "                        attribution=False, #No citation\n",
    "                        # zorder = 2\n",
    "                        )\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "                cx.add_basemap(ax,\n",
    "                        # zoom=\"auto\",\n",
    "                        zoom=abs(def_zoom),\n",
    "                        crs=vector_file.crs.to_string(),\n",
    "                        # source=cx.providers.OpenStreetMap.Mapnik,\n",
    "                        source=basemap_label, #cx.providers.CartoDB.PositronOnlyLabels,\n",
    "                        # source=cx.providers.CartoDB.DarkMatter,\n",
    "                        attribution=False, #No citation\n",
    "                        # zorder = 3\n",
    "                        )\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #         cx.add_basemap(ax,\n",
    "        #                 # zoom=\"auto\",\n",
    "        #                 zoom=abs(def_zoom-1),\n",
    "        #                 crs=vector_file.crs.to_string(),\n",
    "        #                 # source=cx.providers.OpenStreetMap.Mapnik,\n",
    "        #                 source=basemap_label, #cx.providers.CartoDB.PositronOnlyLabels,\n",
    "        #                 # source=cx.providers.CartoDB.DarkMatter,\n",
    "        #                 attribution=False, #No citation\n",
    "        #                 # zorder = 2\n",
    "        #                 )\n",
    "        # except:\n",
    "        #         pass\n",
    "\n",
    "        ax2 = inset_axes(ax, \"40%\", \"39%\", loc=\"upper right\", bbox_to_anchor=(.078,0.01, 1,1), bbox_transform=ax.transAxes\n",
    "                     ) # [left, bottom, width, height], or a tuple of [left, bottom].\n",
    "        ax2 = vector_file.to_crs(crs).plot(ax=ax2, \n",
    "                                        #    alpha=0.6,\n",
    "                                        #    column= visualize_column, \n",
    "                                        facecolor='none',\n",
    "                                        edgecolor='blue' ,\n",
    "                                        label=f\"{city}\", \n",
    "                                        linewidth=2,\n",
    "                                        #    zorder = 2\n",
    "\n",
    "                                        )\n",
    "        ax2 = gdf_country.to_crs(crs).plot(ax=ax2, \n",
    "                                        #  alpha=0.6,\n",
    "                                        facecolor='none',\n",
    "                                        edgecolor='black' , \n",
    "                                        label=f\"{country}\",\n",
    "                                        linewidth=2,\n",
    "                                                # zorder = 3\n",
    "\n",
    "                                        )\n",
    "        try:\n",
    "                cx.add_basemap(ax2,\n",
    "                        zoom=\"auto\",\n",
    "                        # zoom=10,\n",
    "                        crs=vector_file.crs.to_string(),\n",
    "                        source=basemap, #cx.providers.OpenStreetMap.Mapnik,\n",
    "                        attribution=False, #No citation\n",
    "                        # zorder = 4\n",
    "                        )\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "                cx.add_basemap(ax2,\n",
    "                        # zoom=\"auto\",\n",
    "                        zoom=abs(def_zoom-6),\n",
    "                        crs=vector_file.crs.to_string(),\n",
    "                        # source=cx.providers.OpenStreetMap.Mapnik,\n",
    "                        source=basemap_label, #cx.providers.CartoDB.PositronOnlyLabels,\n",
    "                        # source=cx.providers.CartoDB.DarkMatter,\n",
    "                        attribution=False, #No citation\n",
    "                        # zorder = 4\n",
    "                        )\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        box, c1, c2 = mark_inset(ax2, ax, loc1=2, loc2=4, fc=\"none\", lw=2, ec='r')\n",
    "        box.set_linewidth(2)\n",
    "        plt.setp([c1,c2], linewidth=0)\n",
    "\n",
    "\n",
    "        LegendElement = [\n",
    "                        Line2D([0],[0],\n",
    "                               color='black', \n",
    "                               label=f'{country}', \n",
    "                               fillstyle=None,\n",
    "                               linestyle=\"none\", \n",
    "                               markerfacecolor=None,\n",
    "                               marker=\"s\", \n",
    "                               markersize=5.5),\n",
    "                        Line2D([0],[0], \n",
    "                               color='blue', \n",
    "                               label=f'{city}',\n",
    "                               linestyle=\"none\", \n",
    "                               markerfacecolor=None,\n",
    "                               fillstyle=None, \n",
    "                               marker=\"s\", \n",
    "                               markersize=5.5),\n",
    "                      Line2D([0],[0], \n",
    "                               color='cyan', \n",
    "                               label=f'{lowest_admin_level}',\n",
    "                               linestyle='none', \n",
    "                               markerfacecolor=None,\n",
    "                               fillstyle=None, \n",
    "                               marker=\"s\", \n",
    "                               markersize=5.5)\n",
    "                               ]\n",
    "        ax2.legend(handles=LegendElement,loc='upper right', prop = { \"size\": 8.5, 'weight':'bold'},\n",
    "                     bbox_transform=ax2.transAxes) #bbox_to_anchor=(.065,0.017, 1,1),\n",
    "        # get axis position and instert the N arrow \n",
    "        # x, y , x+dx, y+dy\n",
    "\n",
    "        gdf_country_buffer = gdf_country.to_crs(crs).buffer(20000) #for zoom out\n",
    "        minx, miny, maxx, maxy = gdf_country_buffer.total_bounds\n",
    "        ax2.set_xlim(minx, maxx)\n",
    "        ax2.set_ylim(miny, maxy)\n",
    "\n",
    "\n",
    "        ax2.axis('off')\n",
    "        index=0\n",
    "        polygon = vector_file_degrees.reset_index().iloc[index]['geometry']\n",
    "        tags={'amenity':['college', 'dancing_school', 'driving_school', 'kindergarten', \\\n",
    "                        'language_school', 'library', 'surf_school', 'toy_library', 'research_institute', \\\n",
    "                        'training', 'music_school', 'school', 'university'],\n",
    "                        'landuse': 'education'\n",
    "                        }\n",
    "        education=ox.features.features_from_polygon(polygon, tags)\n",
    "        # tags={'leisure':['park', 'nature_reserve', 'protected_area', 'garden']}\n",
    "        tags={'leisure':['park',  'garden']}\n",
    "        park=ox.features.features_from_polygon(polygon, tags)\n",
    "        park.geometry = park['geometry'].centroid\n",
    "        tags={'landuse':['commercial', 'industrial', 'construction', 'retail']                   }\n",
    "        industry=ox.features.features_from_polygon(polygon, tags)\n",
    "        industry.geometry = industry['geometry'].centroid\n",
    "        tags={'building': 'government'}\n",
    "        government_building=ox.features.features_from_polygon(polygon, tags)\n",
    "        government_building.geometry = government_building['geometry'].centroid\n",
    "        tags = {\"building\": True}\n",
    "        building_footprint=ox.features.features_from_polygon(polygon, tags)\n",
    "        building_footprint.geometry = building_footprint['geometry'].centroid\n",
    "        tags = {'amenity':['clinic', 'doctors', 'dentist', 'health_post', 'hospital', 'nursing_home', 'pharmacy', 'veterinary']}\n",
    "        health_care=ox.features.features_from_polygon(polygon, tags)\n",
    "        health_care.geometry = health_care['geometry'].centroid\n",
    "        G = ox.graph.graph_from_polygon(polygon, network_type='drive')\n",
    "        # add travel time based on maximum speed\n",
    "        G = ox.add_edge_speeds(G) \n",
    "        G = ox.add_edge_travel_times(G) \n",
    "        G = ox.projection.project_graph(G, to_crs=3857)\n",
    "        # get edges as Geo data frame\n",
    "        _, gdf_edges = ox.graph_to_gdfs(G)\n",
    "\n",
    "        ax = education.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.6,  \n",
    "                                        facecolor='yellow',\n",
    "                                        edgecolor='yellow' , \n",
    "                                        label=\"Education\",\n",
    "                                        linewidth=3,\n",
    "                                        markersize=10,\n",
    "                                        marker=\"P\"  \n",
    "                                        )\n",
    "        \n",
    "        ax = industry.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.6,  \n",
    "                                        facecolor='red', \n",
    "                                        edgecolor='red' , \n",
    "                                        label=\"Industries\",\n",
    "                                        linewidth=3,\n",
    "                                        markersize=10,\n",
    "                                        marker=\"p\"  \n",
    "                                        )\n",
    "        \n",
    "        ax = park.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.6,  \n",
    "                                        facecolor='green',\n",
    "                                        edgecolor='green' , \n",
    "                                        label=\"Parks\",\n",
    "                                        linewidth=3,\n",
    "                                        markersize=10,\n",
    "                                        marker=\"*\"   \n",
    "                                        )\n",
    "        ax = health_care.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.6,  \n",
    "                                        facecolor='magenta',\n",
    "                                        edgecolor='magenta' , \n",
    "                                        label=\"Healthcare\",\n",
    "                                        linewidth=3,\n",
    "                                        markersize=10,\n",
    "                                        marker=\"o\"   \n",
    "                                        )\n",
    "\n",
    "        ax = gdf_edges.to_crs(crs).plot(ax=ax,\n",
    "                                        alpha=0.4,  \n",
    "                                        facecolor='none',\n",
    "                                        edgecolor='black' , \n",
    "                                        label=\"Roads\",\n",
    "                                        linewidth=.3,\n",
    "                                        linestyle='dashed',\n",
    "                                        # zorder=1\n",
    "                                        )\n",
    "\n",
    "\n",
    "        lines = [Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"P\", markersize=10, color='yellow' , label='Education'),\n",
    "                 Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"p\", markersize=10, color='red' , label='Industries'),\n",
    "                 Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"*\", markersize=10, color='green' , label='Parks'),\n",
    "                 Line2D([0], [0], linestyle=\"none\",  fillstyle=None, marker=\"o\", markersize=10, color='magenta' , label='Healthcare'),\n",
    "                 Line2D([0], [0], linestyle=\"dashed\",  fillstyle=None, marker=\"_\", markersize=10, color='black' , label='Roads')]\n",
    "        ax.legend(handles=lines,loc='lower right', prop = { \"size\": 8 },)\n",
    "\n",
    "        size_in_meters=5000\n",
    "        asb = AnchoredSizeBar(ax.transData,\n",
    "                                size=size_in_meters,\n",
    "                                label=f\"{int(size_in_meters/1000)} KM\",\n",
    "                                loc='lower left',\n",
    "                                pad=0.7, borderpad=0.9, sep=5,\n",
    "                                frameon=False,\n",
    "                                size_vertical=size_in_meters*.03\n",
    "                                # bbox_to_anchor=(0,0.001, .1,.1)\n",
    "                                )\n",
    "        ax.add_artist(asb)\n",
    "\n",
    "        miny, maxy = ax.get_ylim()\n",
    "        minx, maxx = ax.get_xlim()\n",
    "        bounds=[minx, miny, maxx, maxy]\n",
    "        print(f\"City bounds:{bounds}\")\n",
    "        dx=maxx-minx\n",
    "        dy=maxy-miny\n",
    "        # N_x= minx+abs(dx/14.95) \n",
    "        N_x= minx+abs(dx/17) \n",
    "        N_y= miny+abs(dy/7)\n",
    "        arrow_sign_x=minx+abs(dx/15)\n",
    "        arrow_sign_y=miny+abs(dy/11)\n",
    "\n",
    "        head_width=abs(dy/28)\n",
    "        overhang=.1\n",
    "        head_length=1.5*head_width\n",
    "        ax.text(x=N_x, y=N_y, s='N', fontsize=15)\n",
    "        ax.arrow(arrow_sign_x, arrow_sign_y, dx=dx*0, dy=head_width , length_includes_head=True,\n",
    "                head_width=head_width, head_length=head_length, overhang=overhang, facecolor='k')\n",
    "\n",
    "        ax.axis('off')\n",
    "        # ax.set_title(f'{city}', fontdict={'fontsize': '16', 'fontweight' : '6'})\n",
    "        # ax.annotate('Source: OSM basemaps',xy=(0.1, .1),  xycoords='figure fraction', horizontalalignment='left', verticalalignment='top', fontsize=6, color='#555555')\n",
    "        plt.xticks(visible=False)\n",
    "        plt.yticks(visible=False)\n",
    "        plt.tight_layout()\n",
    "        ax.figure.savefig(map_output,  bbox_inches='tight',   dpi = 400, pad_inches=0)\n",
    "        return education, park, industry, government_building, building_footprint, gdf_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Section In Progress: generate text etc\n",
    "Union councils or upazils subdistricts level analysis. Hum + OSM. If no subcity then go for clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_profiles(polygonized_shapefile,shp,WGS84,subdir,visualize_column, title, legend_title,cmap):\n",
    "     # adjust_text(texts)\n",
    "     vector_file = gpd.read_file(shp).to_crs(WGS84)\n",
    "     vector_file_buffer = vector_file.to_crs(WGS84).buffer(-0.02) #-2km buffer\n",
    "     # vector_file_buffer = vector_file.to_crs(WGS84)\n",
    "     minx, miny, maxx, maxy = vector_file_buffer.total_bounds\n",
    "     minx, miny, maxx, maxy\n",
    "     midy= miny+ (maxy-miny)/2\n",
    "     midx= minx+ (maxx-minx)/2\n",
    "     minx, miny, maxx, maxy, midx,midy\n",
    "     left_start=Point(minx, midy)\n",
    "     right_end=Point(maxx, midy)\n",
    "     top_start=Point(midx, maxy)\n",
    "     bottom_end=Point(midx, miny)\n",
    "\n",
    "     length = 0.013\n",
    "     angle = -math.pi/8\n",
    "     line = LineString([left_start, right_end])\n",
    "     # for restircting line within the boundary\n",
    "     gdf_line = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[line])\n",
    "     line_spat_join_city_horizontal=gdf_line\n",
    "     # line_spat_join_city_horizontal =overlay(gdf_line.to_crs(crs), vector_file.to_crs(crs), how=\"intersection\")\n",
    "     # line_spat_join_city_horizontal = gpd.sjoin(vector_file.to_crs(crs), gdf_line.to_crs(crs), how=\"right\", predicate='intersects')\n",
    "\n",
    "     line = LineString([top_start, bottom_end])\n",
    "     # for restircting line within the boundary\n",
    "     gdf_line = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[line])\n",
    "     # line_spat_join_city_vertical =overlay(gdf_line.to_crs(crs), vector_file.to_crs(crs), how=\"intersection\")\n",
    "     # line_spat_join_city_vertical =overlay(gdf_line.to_crs(crs), vector_file.to_crs(crs), how=\"identity\")\n",
    "     line_spat_join_city_vertical = gpd.sjoin(vector_file.to_crs(crs), gdf_line.to_crs(crs), how=\"right\", predicate='contains')\n",
    "     fig = plt.figure(figsize=(width, height))\n",
    "     ax = fig.add_subplot(111)\n",
    "     ax = polygonized_shapefile.to_crs(crs).plot(ax=ax, column= visualize_column, \n",
    "                                   linewidth=0 , alpha=0.6, scheme=\"quantiles\", cmap=cmap, \n",
    "                                   legend=True,  # Add legend \n",
    "                                   legend_kwds={'loc':'upper right', \n",
    "                                                'bbox_to_anchor':(1.01, 1.01), \n",
    "                                                'fmt':legends_format,\n",
    "                                                'markerscale':1.01, \n",
    "                                                'title_fontsize':'small', \n",
    "                                                'fontsize':'small',\n",
    "                                                # 'fontweight': 'bold'\n",
    "                                                }  \n",
    "                                        )\n",
    "     vector_file.to_crs(crs).plot(ax=ax,facecolor='none',edgecolor='k',alpha=0.2)\n",
    "     cx.add_basemap(ax, \n",
    "                   source=cx.providers.Esri.WorldImagery,  \n",
    "                   crs=crs,\n",
    "                   attribution=False, #No citation\n",
    "                   ) \n",
    "     # plt.title(f'{title}',fontsize=16)\n",
    "     plt.xticks([])\n",
    "     plt.yticks([])\n",
    "          \n",
    "     leg1 = ax.get_legend()\n",
    "     # Set markers to square shape\n",
    "     for ea in leg1.legendHandles:\n",
    "          ea.set_marker('s')\n",
    "     leg1.set_title(f'{legend_title}')\n",
    "     # ax.set_title(title, fontsize=12,  fontweight='bold', wrap=True)\n",
    "     ax.set_title(title,  fontweight='bold', wrap=True)\n",
    "     # plt.subplots_adjust(top = 0.00001, bottom = 0.00001, right = 0, left = 0, hspace = 0, wspace = 0)\n",
    "     \n",
    "\n",
    "     ax = line_spat_join_city_horizontal.to_crs(crs).plot(ax=ax,\n",
    "                                   alpha=0.6,  \n",
    "                                   facecolor='yellow',\n",
    "                                   edgecolor='yellow' , \n",
    "                                   label=\"Horizontal\",\n",
    "                                   linewidth=3,\n",
    "                                   markersize=10,\n",
    "                                   marker=\"o\"   \n",
    "                                   )\n",
    "\n",
    "     ax = line_spat_join_city_vertical.to_crs(crs).plot(ax=ax,\n",
    "                                   alpha=0.6,  \n",
    "                                   facecolor='cyan',\n",
    "                                   edgecolor='cyan' , \n",
    "                                   label=\"Top down\",\n",
    "                                   linewidth=3,\n",
    "                                   markersize=10,\n",
    "                                   marker=\"o\"   \n",
    "                                   )\n",
    "\n",
    "     \n",
    "     #  horizontal line profile\n",
    "     lines_read= line_spat_join_city_horizontal #gpd.read_file(f\"{shapefiles}/Poly_1.shp\").to_crs(crs)\n",
    "     # newdf = gpd.sjoin(hexagons.to_crs(crs), lines_read)\n",
    "     for ind, row in lines_read.iterrows():\n",
    "          # XS_ID = row['id']\n",
    "          start_coords =  list([row.geometry][0].coords)[0]\n",
    "          end_coords = list([row.geometry][0].coords)[1]\n",
    "          lon = [start_coords[0]]\n",
    "          lat = [start_coords[1]]\n",
    "          n_points = 10\n",
    "          for i in np.arange(1, n_points+1):\n",
    "               x_dist = end_coords[0] - start_coords[0]\n",
    "               y_dist = end_coords[1] - start_coords[1]\n",
    "               point  = [(start_coords[0] + (x_dist/(n_points+1))*i), (start_coords[1] + (y_dist/(n_points+1))*i)]\n",
    "               lon.append(point[0])\n",
    "               lat.append(point[1])\n",
    "               \n",
    "          lon.append(end_coords[0])\n",
    "          lat.append(end_coords[1])\n",
    "          df = pd.DataFrame({'Latitude': lat, \n",
    "                              'Longitude': lon})\n",
    "          gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "          gdf.crs = {'init': 'epsg:4326'}\n",
    "          gdf_pcs = gdf.to_crs(epsg = 3857)\n",
    "          print(gdf_pcs.head())\n",
    "          gdf_pcs['h_distance'] = 0\n",
    "          for index, row in gdf_pcs.iterrows():\n",
    "               gdf_pcs['h_distance'].loc[index] = gdf_pcs.geometry[0].distance(gdf_pcs.geometry[index])\n",
    "\n",
    "          profile_x_var='h_distance'\n",
    "          visualize_column= subdir[0:9]  \n",
    "\n",
    "          for x, y in zip(lon[::1], lat[::1]):\n",
    "               value=\"----------\"\n",
    "               ax.text(x, y, f\"{value}\", ha=\"center\", va=\"center\", rotation=angle, size=1.5,\n",
    "                         bbox=dict(boxstyle=\"rarrow,pad=0.4\", fc=\"yellow\", ec=\"yellow\", lw=2))\n",
    "\n",
    "\n",
    "     fig, axh = plt.subplots(figsize = (4.5, 3.5))\n",
    "     line_spat_join = gpd.sjoin(polygonized_shapefile.to_crs(crs), gdf_pcs.to_crs(crs), how=\"right\", predicate='contains')\n",
    "     axh.plot(line_spat_join[profile_x_var], line_spat_join[f'{visualize_column}'])\n",
    "     axh.set_title(label=f\"{legend_title} Profile (Horizonal)\" , fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "     axh.set_xlabel(\"Distance (KM)\",  fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "     axh.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1f}'.format(x/1000)))\n",
    "     axh.set_ylabel(f\"{legend_title}\", fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "\n",
    "     #----------------------\n",
    "     max_value= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()][f'{visualize_column}'].round()\n",
    "     Latitude= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()]['Latitude']\n",
    "     Longitude= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()]['Longitude']\n",
    "     max_distance= line_spat_join.loc[(line_spat_join['Latitude'] == Latitude) & (line_spat_join['Longitude'] == Longitude)]['h_distance']\n",
    "     thisr, thistheta =  max_value, max_distance\n",
    "     axh.plot([thistheta], [thisr], 'o')\n",
    "     axh.annotate(\n",
    "     f'Max:{int(max_value)}',\n",
    "     xy=(thistheta, thisr), xycoords='data',\n",
    "     xytext=(30, -2), textcoords='offset points',\n",
    "     arrowprops=dict(arrowstyle=\"->\")\n",
    "     )\n",
    "\n",
    "     plt.xticks(fontsize=11) # rotation=90\n",
    "     plt.yticks(fontsize=11) \n",
    "     # Hide the right and top spines\n",
    "     axh.spines[['right', 'top']].set_visible(False)\n",
    "     plt.tight_layout()\n",
    "     axh.figure.savefig(f\"{maps}/{country}_{city}_{subdir}_profile_horizontal_{visualize_column}.png\",  bbox_inches='tight', dpi=60) #, transparent=True  \n",
    "     line_spat_join.to_csv(f\"{tables}/{country}_{city}_{subdir}_profile_horizontal_{visualize_column}.csv\")\n",
    "\n",
    "\n",
    "     #  vertical line profile\n",
    "     # angle = 0\n",
    "     angle = -90\n",
    "     lines_read= line_spat_join_city_vertical #gpd.read_file(f\"{shapefiles}/Poly_1.shp\").to_crs(crs)\n",
    "     # newdf = gpd.sjoin(hexagons.to_crs(crs), lines_read)\n",
    "     for ind, row in lines_read.iterrows():\n",
    "          # XS_ID = row['id']\n",
    "          start_coords =  list([row.geometry][0].coords)[0]\n",
    "          end_coords = list([row.geometry][0].coords)[1]\n",
    "          lon = [start_coords[0]]\n",
    "          lat = [start_coords[1]]\n",
    "          n_points = 10\n",
    "          for i in np.arange(1, n_points+1):\n",
    "               x_dist = end_coords[0] - start_coords[0]\n",
    "               y_dist = end_coords[1] - start_coords[1]\n",
    "               point  = [(start_coords[0] + (x_dist/(n_points+1))*i), (start_coords[1] + (y_dist/(n_points+1))*i)]\n",
    "               lon.append(point[0])\n",
    "               lat.append(point[1])\n",
    "               \n",
    "     lon.append(end_coords[0])\n",
    "     lat.append(end_coords[1])\n",
    "     df = pd.DataFrame({'Latitude': lat, \n",
    "                         'Longitude': lon})\n",
    "     gdf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "     gdf.crs = {'init': 'epsg:4326'}\n",
    "     gdf_pcs = gdf.to_crs(epsg = 3857)\n",
    "     print(gdf_pcs.head())\n",
    "     gdf_pcs['h_distance'] = 0\n",
    "     for index, row in gdf_pcs.iterrows():\n",
    "          gdf_pcs['h_distance'].loc[index] = gdf_pcs.geometry[0].distance(gdf_pcs.geometry[index])\n",
    "\n",
    "     profile_x_var='h_distance'\n",
    "     visualize_column= subdir[0:9]  \n",
    "     for x, y in zip(lon[::1], lat[::1]):\n",
    "          value=\"----------\"\n",
    "          ax.text(x, y, f\"{value}\", ha=\"center\", va=\"center\", rotation=angle, size=1.5,\n",
    "                    bbox=dict(boxstyle=\"rarrow,pad=0.4\", fc=\"cyan\", ec=\"cyan\", lw=2))\n",
    "\n",
    "     fig, axv = plt.subplots(figsize = (4.5, 3.5))\n",
    "     line_spat_join = gpd.sjoin(polygonized_shapefile.to_crs(crs), gdf_pcs.to_crs(crs), how=\"right\", predicate='contains')\n",
    "     axv.plot(line_spat_join[profile_x_var], line_spat_join[f'{visualize_column}'])\n",
    "     axv.set_title(label=f\"{legend_title} Profile (Vertical)\" , fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "     axv.set_xlabel(\"Distance (KM)\",  fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "     axv.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1f}'.format(x/1000)))\n",
    "     axv.set_ylabel(f\"{legend_title}\", fontdict={'fontsize': 11, 'fontweight': 'bold'})\n",
    "\n",
    "     #----------------------\n",
    "     max_value= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()][f'{visualize_column}'].round()\n",
    "     Latitude= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()]['Latitude']\n",
    "     Longitude= line_spat_join.loc[line_spat_join[f'{visualize_column}'].idxmax()]['Longitude']\n",
    "     max_distance= line_spat_join.loc[(line_spat_join['Latitude'] == Latitude) & (line_spat_join['Longitude'] == Longitude)]['h_distance']\n",
    "     thisr, thistheta =  max_value, max_distance\n",
    "     axv.plot([thistheta], [thisr], 'o')\n",
    "     axv.annotate(\n",
    "     f'Max:{int(max_value)}',\n",
    "     xy=(thistheta, thisr), xycoords='data',\n",
    "     xytext=(30, -2), textcoords='offset points',\n",
    "     arrowprops=dict(arrowstyle=\"->\")\n",
    "     )\n",
    "\n",
    "     plt.xticks(fontsize=11) # rotation=90\n",
    "     plt.yticks(fontsize=11) \n",
    "     # Hide the right and top spines\n",
    "     axv.spines[['right', 'top']].set_visible(False)\n",
    "     plt.tight_layout()\n",
    "     axv.figure.savefig(f\"{maps}/{country}_{city}_{subdir}_profile_vertical_{visualize_column}.png\",  bbox_inches='tight', dpi=60) #, transparent=True  \n",
    "     line_spat_join.to_csv(f\"{tables}/{country}_{city}_{subdir}_profile_vertical_{visualize_column}.csv\")\n",
    "\n",
    "     ax.axis('off')\n",
    "     plt.tight_layout()\n",
    "     ax.figure.savefig(f\"{maps}/{country}_{city}_{subdir}_profiles_{visualize_column}.png\",  bbox_inches='tight', dpi=400) #, transparent=True  \n",
    "     \n",
    "def get_services_infrastructure_inside_poly(polygon):\n",
    "     try:\n",
    "          tags={'leisure':['park', 'nature_reserve', 'protected_area', 'garden']}\n",
    "          park=ox.features.features_from_polygon(polygon, tags)\n",
    "     except:\n",
    "          park=0\n",
    "          pass\n",
    "     try:\n",
    "          tags={'amenity':['college', 'dancing_school', 'driving_school', 'kindergarten', \\\n",
    "                         'language_school', 'library', 'surf_school', 'toy_library', 'research_institute', \\\n",
    "                              'training', 'music_school', 'school', 'university'],\n",
    "                              'landuse': 'education'\n",
    "                              }\n",
    "          education=ox.features.features_from_polygon(polygon, tags)\n",
    "     except:\n",
    "          education=0\n",
    "          pass\n",
    "     try:\n",
    "          tags={'landuse':['commercial', 'industrial', 'construction', 'retail']                   }\n",
    "          industry=ox.features.features_from_polygon(polygon, tags)\n",
    "     except:\n",
    "          industry=0\n",
    "          pass\n",
    "     try:\n",
    "          tags={'building': 'government'}\n",
    "          building=ox.features.features_from_polygon(polygon, tags)\n",
    "     except:\n",
    "          building=0\n",
    "          pass\n",
    "     # print(\"park , education, industry, building:\", park , education, industry, building)\n",
    "     return park , education, industry, building\n",
    "\n",
    "def create_ahc_knn_clusters(db  , raster_val , WGS84_meters):\n",
    "    raster_val=raster_val[0:9]\n",
    "    db =db.to_crs(WGS84_meters)\n",
    "    db['area_sqm'] = db.geometry.area\n",
    "    cluster_variables = [raster_val]\n",
    "    db_scaled = robust_scale(db[cluster_variables])\n",
    "    w = KNN.from_dataframe(db, k=3) #k for four nearest neighbors\n",
    "    # Specify cluster model with spatial constraint\n",
    "    model = AgglomerativeClustering(\n",
    "        linkage=\"ward\", connectivity=w.sparse, n_clusters=5\n",
    "    )\n",
    "    # Fit algorithm to the data\n",
    "    model.fit(db_scaled)\n",
    "    db[\"ward5wknn\"] = model.labels_\n",
    "    db['ward5wknn_sum'] = db[cluster_variables].groupby(db['ward5wknn']).transform('sum')\n",
    "    db['area_sum'] = db['area_sqm'].groupby(db['ward5wknn']).transform('sum')\n",
    "    db['area_sum'] = db['area_sum'].apply(lambda x: x/10000) #Hectares\n",
    "    visualize_var= 'ward5wknn_sum_per_meter'\n",
    "    db[visualize_var] = db['ward5wknn_sum'].div(db['area_sum']).round(2)\n",
    "    # select the columns that you with to use for the dissolve and that will be retained\n",
    "    cluster_boundaries = db[[raster_val , visualize_var,\"ward5wknn\", \"geometry\", \"area_sqm\", \"area_sum\", \"ward5wknn_sum\"]]\n",
    "    # dissolve the state boundary by region \n",
    "    cluster_boundaries = cluster_boundaries.dissolve(by=\"ward5wknn\" , aggfunc={raster_val: \"sum\", visualize_var: \"sum\" , \"area_sqm\":\"sum\" , \"area_sum\": \"sum\", \"ward5wknn_sum\": \"sum\",} , )\n",
    "    cluster_boundaries=cluster_boundaries.explode().reset_index() #convert multipolygons to single part polygons. Multis cause errors\n",
    "    cluster_boundaries[\"area_for_dupes\"]=cluster_boundaries.geometry.area\n",
    "#     cluster_boundaries=cluster_boundaries.sort_values('area_for_dupes', ascending=False).drop_duplicates([\"ward5wknn\"])\n",
    "    cluster_boundaries=multi2single(cluster_boundaries)\n",
    "    cluster_boundaries = cluster_boundaries.to_crs(crs)\n",
    "    cluster_boundaries[\"centroid\"] = cluster_boundaries[\"geometry\"].centroid\n",
    "    cluster_boundaries['lon'] = cluster_boundaries['centroid'].x\n",
    "    cluster_boundaries['lat'] = cluster_boundaries['centroid'].y\n",
    "    cluster_boundaries['cluster_area_deg'] = cluster_boundaries.geometry.area\n",
    "    return db , cluster_boundaries, visualize_var\n",
    "\n",
    "\n",
    "\n",
    "def describe_cluster_trees_pop(cluster_boundaries, category, maps):\n",
    "     category=category[0:9]\n",
    "     df = pd.DataFrame(cluster_boundaries.drop(columns='geometry'))\n",
    "     df = df.sort_values('cluster_area_deg', ascending = False).groupby('ward5wknn').head(2)\n",
    "     df = df.reset_index() \n",
    "     df['Rank'] = df['ward5wknn_sum_per_meter'].rank(method='dense', ascending=False)\n",
    "     df= df.sort_values(['Rank'], ascending=[True])\n",
    "     cent= cluster_boundaries.dissolve().centroid \n",
    "     # White house 38.8977 N, 77.0365 W. centroid of the city.\n",
    "     lat1 = float(cent.centroid.y)\n",
    "     long1= float(cent.centroid.x)\n",
    "     list_statements= []\n",
    "     stats_dict= defaultdict(list)\n",
    "     for index, row in df.iterrows():\n",
    "          lat2 = row['lat'] # cent.centroid.y  #38.8893\n",
    "          long2 =row['lon'] # cent.centroid.x  # -77.0506\n",
    "          # print(f\"lat1 {lat1} , long1 {long1}, lat2 {lat2}, long2 {long2}\")\n",
    "          points = calcNSEW(lat1, long1, lat2, long2)\n",
    "          rank= int(row['Rank'])\n",
    "          print(f\"rank------>>> {rank}\")\n",
    "          # rank= ordinal(rank)\n",
    "          value= row['ward5wknn_sum_per_meter']\n",
    "          statement= f\"A cluster that is located in the {points} of the city, is ranked {rank} and has the value of {value}\"\n",
    "          list_statements.append(statement)\n",
    "          print(f\"statement:::: {statement}\")\n",
    "          try:\n",
    "               polygon = cluster_boundaries.iloc[index]['geometry']\n",
    "               cluster_area_deg= cluster_boundaries.iloc[index]['cluster_area_deg']\n",
    "               ward5wknn= cluster_boundaries.iloc[index]['ward5wknn']\n",
    "               park , education, industry, building=get_services_infrastructure_inside_poly(polygon)\n",
    "               stats_dict[\"cluster\"].append(index)\n",
    "               stats_dict[\"ward5wknn\"].append(ward5wknn)\n",
    "               stats_dict[\"points\"].append(points)\n",
    "               stats_dict[\"cluster_area_deg\"].append(cluster_area_deg)\n",
    "               stats_dict[\"rank\"].append(rank)\n",
    "               stats_dict[\"value\"].append(value)\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               stats_dict[\"park\"].append(len(park))\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               stats_dict[\"education\"].append(len(education))\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               stats_dict[\"industry\"].append(len(industry))\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               stats_dict[\"building\"].append(len(building))\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               \n",
    "               park[\"ward5wknn\"]=ward5wknn\n",
    "               park[\"index\"]=index\n",
    "               park[\"points\"]=points\n",
    "               park[\"rank\"]=rank\n",
    "               park[\"value\"]=value\n",
    "               park.to_csv(f\"{tables}/{ward5wknn}_{category}_park_df.csv\")\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               education[\"ward5wknn\"]=ward5wknn\n",
    "               education[\"index\"]=index\n",
    "               education[\"points\"]=points\n",
    "               education[\"rank\"]=rank\n",
    "               education[\"value\"]=value\n",
    "               education.to_csv(f\"{tables}/{ward5wknn}_{category}_education_df.csv\")\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               industry[\"ward5wknn\"]=ward5wknn\n",
    "               industry[\"index\"]=index\n",
    "               industry[\"points\"]=points\n",
    "               industry[\"rank\"]=rank\n",
    "               industry[\"value\"]=value\n",
    "               industry.to_csv(f\"{tables}/{ward5wknn}_{category}_industry_df.csv\")\n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               building[\"ward5wknn\"]=ward5wknn\n",
    "               building[\"index\"]=index\n",
    "               building[\"points\"]=points\n",
    "               building[\"rank\"]=rank\n",
    "               building[\"value\"]=value\n",
    "               building.to_csv(f\"{tables}/{ward5wknn}_{category}_building_df.csv\")               \n",
    "          except:\n",
    "               pass\n",
    "\n",
    "          try:\n",
    "               stats=get_network_stats(polygon)\n",
    "               stats = stats.to_frame(0).T\n",
    "               # category=category[0:9]\n",
    "               stats.to_csv(f\"{tables}/{ward5wknn}_{category}_infra_network_stats.csv\")\n",
    "          except:\n",
    "               print(f\"no network for {ward5wknn}\")\n",
    "               pass\n",
    "     \n",
    "     try:\n",
    "          statement_df=pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in stats_dict.items() ]))\n",
    "          # category=category[0:9]\n",
    "          statement_df.to_csv(f\"{tables}/{category}_amenities_stats_all.csv\")\n",
    "     except:\n",
    "          print(\"No amenities\")\n",
    "          pass\n",
    "\n",
    "     return list_statements\n",
    "\n",
    "\n",
    "def describe_db( db_var, category, tables):\n",
    "     try:\n",
    "          category=category[0:9]\n",
    "          cluster_variables= category\n",
    "          k5desc = db_var.groupby('ward5wknn')[cluster_variables].describe()\n",
    "          df1=k5desc.reset_index(0).reset_index(drop=True)\n",
    "          # category=category[0:9]\n",
    "          df1.to_csv(f\"{tables}/{category}_summary_stats_df.csv\")\n",
    "     except:\n",
    "          pass\n",
    "\n",
    "def multi2single(gpdf):\n",
    "    gpdf_singlepoly = gpdf[gpdf.geometry.type == 'Polygon']\n",
    "    gpdf_multipoly = gpdf[gpdf.geometry.type == 'MultiPolygon']\n",
    "\n",
    "    for i, row in gpdf_multipoly.iterrows():\n",
    "        Series_geometries = pd.Series(row.geometry)\n",
    "        df = pd.concat([gp.GeoDataFrame(row, crs=gpdf_multipoly.crs).T]*len(Series_geometries), ignore_index=True)\n",
    "        df['geometry']  = Series_geometries\n",
    "        gpdf_singlepoly = pd.concat([gpdf_singlepoly, df])\n",
    "\n",
    "    gpdf_singlepoly.reset_index(inplace=True, drop=True)\n",
    "    return gpdf_singlepoly\n",
    "\n",
    "\n",
    "def calcBearing (lat1, long1, lat2, long2):\n",
    "    dLon = (long2 - long1)\n",
    "    x = math.cos(math.radians(lat2)) * math.sin(math.radians(dLon))\n",
    "    y = math.cos(math.radians(lat1)) * math.sin(math.radians(lat2)) - math.sin(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.cos(math.radians(dLon))\n",
    "    bearing = math.atan2(x,y)   # use atan2 to determine the quadrant\n",
    "    bearing = math.degrees(bearing)\n",
    "    return bearing\n",
    "\n",
    "def calcNSEW(lat1, long1, lat2, long2):\n",
    "    points = [\"north\", \"north east\", \"east\", \"south east\", \"south\", \"south west\", \"west\", \"north west\"]\n",
    "    bearing = calcBearing(lat1, long1, lat2, long2)\n",
    "    bearing += 22.5\n",
    "    bearing = bearing % 360\n",
    "    bearing = int(bearing / 45) # values 0 to 7\n",
    "    NSEW = points [bearing]\n",
    "    return NSEW\n",
    "\n",
    "def ordinal(n: int):\n",
    "    if 11 <= (n % 100) <= 13:\n",
    "        suffix = 'th'\n",
    "    else:\n",
    "        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n",
    "    return str(n) + suffix\n",
    "\n",
    "\n",
    "def get_network_stats(polygon):\n",
    "     try:\n",
    "          # polygon = cluster_boundaries.iloc[index]['geometry'] \n",
    "          graph = ox.graph.graph_from_polygon(polygon, network_type='drive')\n",
    "          # Retrieve only edges from the graph\n",
    "          edges = ox.graph_to_gdfs(graph, nodes=False, edges=True)\n",
    "          graph_proj = ox.project_graph(graph)\n",
    "          # fig, ax = ox.plot_graph(graph_proj) \n",
    "          # Get Edges and Nodes\n",
    "          nodes_proj, edges_proj = ox.graph_to_gdfs(graph_proj, nodes=True, edges=True)\n",
    "          # print(\"Coordinate system:\", edges_proj.crs)\n",
    "          # edges_proj.head()\n",
    "          # Get Edges and Nodes\n",
    "          nodes_proj, edges_proj = ox.graph_to_gdfs(graph_proj, nodes=True, edges=True)\n",
    "          # print(\"Network Coordinate system: projected\", edges_proj.crs)\n",
    "          # edges_proj.head()\n",
    "          # Get the Convex Hull of the network\n",
    "          convex_hull = edges_proj.unary_union.convex_hull\n",
    "          # Calculate the area\n",
    "          area = convex_hull.area\n",
    "          # Calculate statistics with density information\n",
    "          stats = ox.stats.basic_stats(graph_proj, area=area) #replace area by city polygon\n",
    "          stats =pd.Series(stats) #edge_density_km(road density) - edge_length_total per sq km\n",
    "     except:\n",
    "          stats=0\n",
    "          stats =pd.Series(stats) \n",
    "          pass     \n",
    "     return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "db , cluster_boundaries, visualize_var =create_ahc_knn_clusters(db = polygonized_shapefile , raster_val=raster_val, \n",
    "                                                              WGS84_meters=WGS84_meters)\n",
    "category=raster_val\n",
    "describe_cluster_trees= describe_cluster_trees_pop(cluster_boundaries, category, maps)\n",
    "map_title= f\"Population across {city}\"\n",
    "legends_format= '{:,.0f}'\n",
    "categorical=True\n",
    "scheme=\"quantiles\"\n",
    "\n",
    "map_output= f'{maps}/{city}_{subdir}_{raster_val}_2.png'\n",
    "export_the_map= export_map(map_dir=maps, vector_file=vector_file , \\\n",
    "        hexagons=db,legend_title=legend_title, legends_format=legends_format,  \\\n",
    "        crs=crs, cmap=cmap , visualize_column=raster_val,  \\\n",
    "        title=map_title, map_output=map_output)\n",
    "\n",
    "cluster_boundaries[visualize_var]=cluster_boundaries[visualize_var].round(0)\n",
    "\n",
    "map_title= f\"Population across clusters(Population/Hectares)\"\n",
    "legends_format= '{:,.f}'\n",
    "categorical=True\n",
    "map_output= f'{maps}/{city}_{subdir}_{raster_val}_{visualize_var}_map_clusters.png'\n",
    "export_map_categorical(vector_file=vector_file , hexagons=cluster_boundaries,legend_title=legend_title, \n",
    "                       legends_format=legends_format, crs=WGS84 \n",
    "                       ,cmap=cmap,  visualize_column=visualize_var, title=map_title, map_output=map_output)\n",
    "\n",
    "visualize_column=\"ward5wknn_sum\"\n",
    "map_title= f\"Population across clusters(Total Population)\"\n",
    "\n",
    "map_output= f'{maps}/{city}_{subdir}_{raster_val}_{visualize_column}_map_clusters.png'\n",
    "export_map_categorical(vector_file=vector_file , hexagons=cluster_boundaries,legend_title=legend_title, \n",
    "                       legends_format=legends_format, crs=WGS84 \n",
    "                       ,cmap=cmap,  visualize_column=visualize_column, title=map_title, map_output=map_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "p = document.add_paragraph()\n",
    "r = p.add_run()\n",
    "r.add_text('Good Morning every body,This is my ')\n",
    "r.add_picture(map_output)\n",
    "r.add_text(' do you like it?')\n",
    "\n",
    "document_output = f\"{tables}/{country}_{city}_fcs_report.docx\"\n",
    "document.save(document_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "                        \n",
    "def export_figure_with_heading( document_output, mappath, title):  \n",
    "    document = Document(document_output)\n",
    "    p = document.add_paragraph()\n",
    "    r = p.add_run()\n",
    "    r.add_text(title)\n",
    "    r.add_picture(mappath)\n",
    "    r.add_text('Notes here')\n",
    "    document.save(document_output)\n",
    "    # return print('doc saved')\n",
    "\n",
    "\n",
    "# document_output = f\"{tables}/{country}_{city}_Future_city_scan_document.docx\"\n",
    "# export_dox= export_figure_with_heading( document_output = document_output, mappath=map_output, title=map_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create modules and package then import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from section_1_population import export_popdynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "if __name__ ==  '__main__':    \n",
    "    from utils import load_shapefile\n",
    "    load_shapefile(shp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "I want all three ax objects to share the same bins (preferable the central_pop scenario quantiles) so that the legend is consistent for the whole figure.\n",
    "This way I should see darker colors (more red) in the far right ax showing the high_pop scenario.\n",
    "\n",
    "How can I set the colorscheme bins for the whole figure / each of the ax objects?\n",
    "\n",
    "The simplest way I can see this working is either\n",
    "a) Provide a set of bins to the geopandas.plot() function\n",
    "b) extract the colorscheme / bins from one ax and apply it to another.\n",
    "\n",
    "@knaaptime\n",
    "knaaptime commented on Jun 21, 2019  \n",
    "Under the hood, geopandas uses mapclassify, and the easiest way to achieve what you want would be to just use it directly:\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mapclassify import Quantiles, User_Defined\n",
    "\n",
    "# Note you can read directly from the URL\n",
    "gdf = gpd.read_file('https://opendata.arcgis.com/datasets/8edafbe3276d4b56aec60991cbddda50_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D&session=850489311.1553456889'\n",
    ")\n",
    "\n",
    "# 380 values\n",
    "df = pd.DataFrame([])\n",
    "df['AREA_CODE'] = gdf.lad15cd.values\n",
    "df['central_pop'] = np.random.normal(30, 15, size=(len(gdf.lad15cd.values)))\n",
    "df['low_pop'] = np.random.normal(10, 15, size=(len(gdf.lad15cd.values)))\n",
    "df['high_pop'] = np.random.normal(50, 15, size=(len(gdf.lad15cd.values)))\n",
    "\n",
    "def join_df_to_shp(pd_df, gpd_gdf):\n",
    "    \"\"\"\"\"\"\n",
    "    df_ = pd.merge(pd_df, gpd_gdf[['lad15cd','geometry']], left_on='AREA_CODE', right_on='lad15cd', how='left')\n",
    "\n",
    "    # DROP the NI counties\n",
    "    df_ = df_.dropna(subset=['geometry'])\n",
    "\n",
    "    # convert back to a geopandas object (for ease of plotting etc.)\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    gdf_ = gpd.GeoDataFrame(df_, crs=crs, geometry='geometry')\n",
    "    # remove the extra area_code column joined from gdf\n",
    "    gdf_.drop('lad15cd',axis=1, inplace=True)\n",
    "\n",
    "    return gdf_\n",
    "\n",
    "pop_gdf = join_df_to_shp(df, gdf)\n",
    "\n",
    "fig,(ax1,ax2,ax3,) = plt.subplots(1,3,figsize=(15,6))\n",
    "\n",
    "# define your bins\n",
    "bins = Quantiles(pop_gdf['central_pop'], 5).bins\n",
    "\n",
    "# create a new column with the discretized values and plot that col\n",
    "# repeat for each view\n",
    "pop_gdf.assign(cl=User_Defined(df['low_pop'].dropna(), bins).yb).plot(\n",
    "    column='cl', ax=ax1, cmap='OrRd'\n",
    ")\n",
    "pop_gdf.assign(cl=User_Defined(df['central_pop'].dropna(), bins).yb).plot(\n",
    "    column='cl', ax=ax2, cmap='OrRd',\n",
    ")\n",
    "pop_gdf.assign(cl=User_Defined(df['high_pop'].dropna(), list(bins)).yb).plot(\n",
    "    column='cl', ax=ax3, cmap='OrRd',\n",
    ")\n",
    "for ax in (ax1,ax2,ax3,):\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "plt.xkcd()\n",
    "seoul = gpd.read_file('./data/seoul_municipalities_geo.json')\n",
    "# You can download the same file from the Github below\n",
    "# https://github.com/southkorea/seoul-maps/tree/master/juso/2015/json\n",
    "\n",
    "seoul.plot(figsize=(14,10),linewidth=0.25, edgecolor='black', column='ESRI_PK', cmap='Blues', scheme='quantiles')\n",
    "\n",
    "for index, row in seoul.iterrows():\n",
    "    xy = row['geometry'].centroid.coords[:]\n",
    "    xytext = row['geometry'].centroid.coords[:]\n",
    "    plt.annotate(row['SIG_ENG_NM'], xy=xy[0], xytext=xytext[0], horizontalalignment='center', verticalalignment='center')\n",
    "    plt.axis('off')\n",
    "\n",
    "cmap = cm.get_cmap('Blues')\n",
    "white_patch = mpatches.Patch(color=cmap(0.0), label='0 - 5')\n",
    "lowblue_patch = mpatches.Patch(color=cmap(0.25), label='5 - 10')\n",
    "midblue_patch = mpatches.Patch(color=cmap(0.5), label='10 - 14')\n",
    "highblue_patch = mpatches.Patch(color=cmap(0.75), label='14 - 19')\n",
    "veryblue_patch = mpatches.Patch(color=cmap(1.0), label='19 - 24')\n",
    "plt.legend(handles=[white_patch, lowblue_patch, midblue_patch, highblue_patch, veryblue_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Stop\n",
    "# Visulizie %\n",
    "# Fathom vars\n",
    "# set up one_drive folder for maps\n",
    "# Ask Nick for heat layer\n",
    "# Fathom + TextDisplayObject\n",
    "# Minor cosmetic changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'uh (Python 3.8.17)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
